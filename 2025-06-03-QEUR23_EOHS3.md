---
title: QEUR23_EOHS3 - FAISSベクトルストアを構築し、推論する
date: 2025-06-03
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_EOHS3 - FAISSベクトルストアを構築し、推論する

## ～ 多様化せよ！アウフヘーベンせよ！！ ～

QEU:FOUNDER（設定年齢65歳）  ： “それでは、プログラムの後半を始めます。前半のアウトプットは、ユーザーの検索コンセプトを満足するURLリストを含むEXCELファイルを作成することでした。”

![imageEHS1-4-1](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-1.jpg) 

D先生（設定年齢65歳） ： “ここでのポイントは、URLは多言語であることです。我々のプロジェクトでは、ここで議論しているの多言語の処理は、最終的に**「多様化の配慮」に発展**していきます。”

QEU:FOUNDER  ： “このEXCELファイルの中のWeb情報をFAISSのベクトルストアに入れます。結構苦労したわ、これ・・・。”

```python
# --- モジュールインポート ---
import os
import time
import requests
import pandas as pd
import tempfile
import gc
import logging
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from langchain.document_loaders import WebBaseLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS  # FAISSに変更
from langchain.embeddings.base import Embeddings
from openai import OpenAI
import certifi
import langchain

# --- 環境設定 ---
os.environ["USER_AGENT"] = "RAG-Application/1.0"
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
# USER_AGENT警告抑制（新しい形式）
langchain.requests.Requests = lambda: requests.Session()

# --- ロギング設定 ---
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# --- 定数定義 ---
VECTOR_STORE_PATH = "drive/MyDrive/faiss_db"  # パス変更
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 100
BATCH_SIZE = 3
EMBEDDING_BATCH_SIZE = 10

# --- PDFダウンロード関数 ---
def download_pdf(url):
    headers = {"User-Agent": os.environ["USER_AGENT"]}
    try:
        response = requests.get(
            url,
            headers=headers,
            timeout=30,
            verify=os.environ["REQUESTS_CA_BUNDLE"]
        )
        response.raise_for_status()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmpfile:
            tmpfile.write(response.content)
            return tmpfile.name
    except Exception as e:
        logger.error(f"PDFダウンロード失敗: {url} - {str(e)}")
        return None

# --- 埋め込みモデルクラス ---
class AlibabaCloudEmbeddings(Embeddings):
    def __init__(self, model_name="text-embedding-v3", dimensions=1024, api_key=None, base_url=None):
        self.model_name = model_name
        self.dimensions = dimensions
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_fixed(10),
        retry=retry_if_exception_type((requests.exceptions.RequestException,)),
        before_sleep=lambda _: logger.warning("埋め込みリトライ中...")
    )
    def embed_documents(self, texts):
        embeddings = []
        for i in range(0, len(texts), EMBEDDING_BATCH_SIZE):
            batch = texts[i:i+EMBEDDING_BATCH_SIZE]
            response = self.client.embeddings.create(
                model=self.model_name,
                input=batch,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            embeddings.extend([data.embedding for data in response.data])
        return embeddings
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_fixed(10),
        retry=retry_if_exception_type((requests.exceptions.RequestException,)),
        before_sleep=lambda _: logger.warning("埋め込みリトライ中...")
    )
    def embed_query(self, text):
        response = self.client.embeddings.create(
            model=self.model_name,
            input=text,
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return response.data[0].embedding

# --- 埋め込み生成 ---
def create_embeddings():
    return AlibabaCloudEmbeddings(
        model_name="text-embedding-v3",
        dimensions=1024,
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"  
    )

# --- メモリ解放 ---
def free_memory():
    gc.collect()
    logger.info("メモリ解放を実行")

# --- メイン処理 ---
# 埋め込みモデル初期化
embeddings = create_embeddings()

# FAISSベクトルストア初期化
vectorstore = None
if os.path.exists(VECTOR_STORE_PATH):
    try:
        vectorstore = FAISS.load_local(
            VECTOR_STORE_PATH, 
            embeddings,
            allow_dangerous_deserialization=True
        )
        logger.info("既存のFAISSデータベースを読み込み")
    except:
        logger.warning("FAISSデータベースの読み込みに失敗、新規作成します")
        vectorstore = None

if vectorstore is None:
    from langchain.docstore.in_memory import InMemoryDocstore
    import faiss
    index = faiss.IndexFlatL2(1024)  # 1024次元
    vectorstore = FAISS(
        embeddings.embed_query, 
        index, 
        InMemoryDocstore({}), 
        {}
    )

# データ読み込み
df = pd.read_excel('drive/MyDrive/ai_trend.xlsx')
splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP
)

# ドキュメント処理
for i in range(0, len(df), BATCH_SIZE):
    batch = df.iloc[i:i+BATCH_SIZE]
    batch_docs = []
    for _, row in batch.iterrows():
        url = row['url']
        title = row['title']
        logger.info(f"処理中: {title} [{i+1}/{len(df)}]")
        try:
            parsed = urlparse(url)
            if parsed.path.endswith('.pdf'):
                tmp_path = download_pdf(url)
                if tmp_path:
                    loader = PyPDFLoader(tmp_path)
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata['title'] = title
                        doc.metadata['source'] = url  # ソース情報追加
                    batch_docs.extend(docs)
                    os.unlink(tmp_path)
            else:
                loader = WebBaseLoader(
                    web_paths=[url],
                    requests_kwargs={
                        "headers": {"User-Agent": os.environ["USER_AGENT"]},
                        "verify": os.environ["REQUESTS_CA_BUNDLE"]
                    },
                    requests_per_second=1
                )
                docs = loader.load()
                for doc in docs:
                    doc.metadata['title'] = title
                    doc.metadata['source'] = url  # ソース情報追加
                batch_docs.extend(docs)
        except Exception as e:
            logger.error(f"ドキュメント処理失敗: {url} - {str(e)}")
        time.sleep(1)
    
    if batch_docs:
        splits = splitter.split_documents(batch_docs)
        if splits:
            # FAISSにバッチ追加
            new_vectorstore = FAISS.from_documents(splits, embeddings)
            if vectorstore:
                vectorstore.merge_from(new_vectorstore)
            else:
                vectorstore = new_vectorstore
            logger.info(f"バッチ追加: {len(splits)}チャンク")
            # 定期的に保存
            if i % (BATCH_SIZE * 10) == 0:
                vectorstore.save_local(VECTOR_STORE_PATH)
        del batch_docs, splits, new_vectorstore
        free_memory()

# 最終保存
vectorstore.save_local(VECTOR_STORE_PATH)

# コンテキスト生成
topics = [
    "最近のLLMの進歩に関する重要なトピック",
    "日本のLLM開発における固有の特徴",
    "米国のLLM開発における固有の特徴",
    "中国のLLM開発における固有の特徴",
    "米国と中国のLLM開発のレベル差",
    "シンギュラリティの定義と、その実現時期",
    "AGIの実現時期と、現在の課題",
    "人工知能で人々は豊かになるか",
    "人工知能で失業は増えるか",
    "人工知能の金融業界への応用",
    "人工知能の教育業界への応用",
    "人工知能の工業デザインへの応用",
    "人工知能の農業への応用",
    "人工知能の文化への影響",
    "人工知能の社会への影響"
]

context = ""
reference_titles = []
for topic in topics:
    try:
        context_docs = vectorstore.similarity_search(topic, k=2)
        part_context = "\n".join([doc.page_content for doc in context_docs])
        context += f"## {topic}\n{part_context}\n\n"
        
        title_docs = vectorstore.similarity_search(topic, k=1)
        if title_docs:
            title = title_docs[0].metadata.get('title', 'Unknown')
            reference_titles.append(title)
        else:
            reference_titles.append("Unknown")
    except Exception as e:
        logger.error(f"トピック処理失敗: {topic} - {str(e)}")
        reference_titles.append("Error")
    finally:
        del context_docs, title_docs
        free_memory()

logger.info("コンテキスト生成完了")
# ----
# 結果を出力する(1)
print("=== context[:1000] ===")
print(context[:1000])

# ---
# 結果を出力する(2)
print("=== reference_titles ===")
print(reference_titles)

```

QEU:FOUNDER  ： “・・・というわけで、無事にFAISSデータベースができました。”

![imageEHS1-4-2](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-2.jpg) 

D先生： “おや？**QWEN製のEmbeddingモデル**を使ったんですね。これに対応しているツールがLangChainの中にありましたっけ？”

QEU:FOUNDER  ： “QWENの推論機能に対しては、Langchainでもサポートしていますが、Embeddingモデルに対してはサポートされていません。”

D先生： “だから、わざわざ「埋め込みモデルクラス」を作ったんですね。これは大変だ・・・。次のプログラムに行きましょう。ベクトルストアを使って、何かをやるんでしょ？”

```python
# --- モジュールインポート ---
import os
import time
import requests
import tempfile
import gc
import logging
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from langchain.document_loaders import WebBaseLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from openai import OpenAI
import certifi
import langchain
from langchain_community.utilities import Requests

# --- 環境設定 ---
os.environ["USER_AGENT"] = "RAG-Application/1.0"
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
Requests._session = requests.Session()

# --- ロギング設定 ---
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# --- 定数定義 ---
VECTOR_STORE_PATH = "drive/MyDrive/faiss_db"

# --- 埋め込みモデルクラス ---
class AlibabaCloudEmbeddings(Embeddings):
    def __init__(self, model_name="text-embedding-v3", dimensions=1024, api_key=None, base_url=None):
        self.model_name = model_name
        self.dimensions = dimensions
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_fixed(10),
        retry=retry_if_exception_type((requests.exceptions.RequestException,)),
        before_sleep=lambda _: logger.warning("埋め込みリトライ中...")
    )
    def embed_documents(self, texts):
        embeddings = []
        for i in range(0, len(texts), 10):
            batch = texts[i:i+10]
            response = self.client.embeddings.create(
                model=self.model_name,
                input=batch,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            embeddings.extend([data.embedding for data in response.data])
        return embeddings
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_fixed(10),
        retry=retry_if_exception_type((requests.exceptions.RequestException,)),
        before_sleep=lambda _: logger.warning("埋め込みリトライ中...")
    )
    def embed_query(self, text):
        response = self.client.embeddings.create(
            model=self.model_name,
            input=text,
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return response.data[0].embedding

# --- メモリ解放 ---
def free_memory():
    gc.collect()
    logger.info("メモリ解放を実行")

# --- メイン処理 ---
# 埋め込みモデル初期化
embeddings = AlibabaCloudEmbeddings(
    model_name="text-embedding-v3",
    dimensions=1024,
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
)

# FAISSベクトルストア読み込み
vectorstore = FAISS.load_local(
    VECTOR_STORE_PATH,
    embeddings,
    allow_dangerous_deserialization=True
)
logger.info("既存のFAISSデータベースを読み込み")

# トピックリスト
topics = [
    "最近のLLMの進歩に関する重要なトピック",
    "日本のLLM開発における固有の特徴",
    "米国のLLM開発における固有の特徴",
    "中国のLLM開発における固有の特徴",
    "米国と中国のLLM開発のレベル差",
    "シンギュラリティの定義と、その実現時期",
    "AGIの実現時期と、現在の課題",
    "人工知能で人々は豊かになるか",
    "人工知能で失業は増えるか",
    "人工知能の金融業界への応用",
    "人工知能の教育業界への応用",
    "人工知能の工業デザインへの応用",
    "人工知能の農業への応用",
    "人工知能の文化への影響",
    "人工知能の社会への影響"
]

# コンテキスト生成
context = ""
reference_titles = []
for topic in topics:
    try:
        context_docs = vectorstore.similarity_search(topic, k=2)
        part_context = "\n".join([doc.page_content for doc in context_docs])
        context += f"## {topic}\n{part_context}\n\n"
        
        title_docs = vectorstore.similarity_search(topic, k=1)
        if title_docs:
            title = title_docs[0].metadata.get('title', 'Unknown')
            reference_titles.append(title)
        else:
            reference_titles.append("Unknown")
    except Exception as e:
        logger.error(f"トピック処理失敗: {topic} - {str(e)}")
        reference_titles.append("Error")
    finally:
        del context_docs, title_docs
        free_memory()

logger.info("コンテキスト生成完了")

# 結果出力
print("=== context[:1000] ===")
print(context[:1000])

print("=== reference_titles ===")
print(reference_titles)
```

QEU:FOUNDER ： “まあ、「何かをやる」と言っても、FAISSを読み込んで前回と同じくcontextを生成するだけなんですがね。”

![imageEHS1-4-3](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-3.jpg) 

D先生： “このようにするのは、次のLLMによる推論のためのステップです。そんなことは、私もわかってますがな・・・（笑）。”

```python
# --- モジュールインポート ---
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# --- 固定プロンプト ---
USER_PROMPT = """2025年以降、AI（人工知能）の進歩がさらに速くなって、人々の仕事と生活が変化しています。
AIの最近の技術進歩と、AIが各種の産業、仕事と生活に与える影響について知りたい。以下のフォーマットに示す形で
出力してください。

分析用リスト化フォーマット:
1. **最近のLLMの進歩に関する重要なトピック** - [AIの分析結果](注１)
2. **日本のLLM開発における固有の特徴** - [AIの分析結果](注１)
3. **米国のLLM開発における固有の特徴** - [AIの分析結果](注１)
4. **中国のLLM開発における固有の特徴** - [AIの分析結果](注１)
5. **米国と中国のLLM開発のレベル差** - [AIの分析結果](注１)
6. **シンギュラリティの定義と、その実現時期** - [AIの分析結果](注１)
7. **AGIの実現時期と、現在の課題** - [AIの分析結果](注１)
8. **人工知能で人々は豊かになるか** - [AIの分析結果](注１)
9. **人工知能で失業は増えるか** - [AIの分析結果](注１)
10. **人工知能の金融業界への応用** - [AIの分析結果](注１)
11. **人工知能の教育業界への応用** - [AIの分析結果](注１)
12. **人工知能の工業デザインへの応用** - [AIの分析結果](注１)
13. **人工知能の農業への応用** - [AIの分析結果](注１)
14. **人工知能の文化への影響** - [AIの分析結果](注１)
15. **人工知能の社会への影響** - [AIの分析結果](注１)

注１:[AIの分析結果]は、400文字以上600文字以内とし、リスト化しないでください。

日本語で回答してください。"""

# --- プロンプト生成関数 ---
full_prompt = f"""以下は関連する検索結果です：
{context}

この情報を基に、以下の質問に答えてください。
{USER_PROMPT}

また、以下のフォーマットに基づいて、分析に当たって、AIが最も参考にしたコンテンツのタイトルのリストを生成してください。
参考にしたコンテンツ用リスト化フォーマット:
1. **最近のLLMの進歩に関する重要なトピック** - {reference_titles[0]}
2. **日本のLLM開発における固有の特徴** - {reference_titles[1]}
3. **米国のLLM開発における固有の特徴** - {reference_titles[2]}
4. **中国のLLM開発における固有の特徴** - {reference_titles[3]}
5. **米国と中国のLLM開発のレベル差** - {reference_titles[4]}
6. **シンギュラリティの定義と、その実現時期** - {reference_titles[5]}
7. **AGIの実現時期と、現在の課題** - {reference_titles[6]}
8. **人工知能で人々は豊かになるか** - {reference_titles[7]}
9. **人工知能で失業は増えるか** - {reference_titles[8]}
10. **人工知能の金融業界への応用** - {reference_titles[9]}
11. **人工知能の教育業界への応用** - {reference_titles[10]}
12. **人工知能の工業デザインへの応用** - {reference_titles[11]}
13. **人工知能の農業への応用** - {reference_titles[12]}
14. **人工知能の文化への影響** - {reference_titles[13]}
15. **人工知能の社会への影響** - {reference_titles[14]}
"""

# LLMの初期化
llm = ChatOpenAI(
    model="qwen-max",
    temperature=0.3,
    openai_api_key=os.getenv("DASHSCOPE_API_KEY"),
    openai_api_base="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
)

# LLM呼び出し
print("LLM推論を実行中...")
response = llm.invoke([HumanMessage(content=full_prompt)])
llm_output = response.content
#print("==== 推論の実行 ====")
#print(llm_output)

# ---
from IPython.display import Markdown

# ---
print("\n======== AIレポート(QWEN-MAX) ========\n")
Markdown(llm_output)

```

QEU:FOUNDER ： “ご要望に応えて推論をやってみましょう。ドン！！”

![imageEHS1-4-4](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-4.jpg) 

QEU:FOUNDER  ： “どうせ、「詳細を見たい」を言うんでしょ？詳細をドン！”

### 分析用リスト化フォーマット:

1. **最近のLLMの進歩に関する重要なトピック**  
大規模言語モデル（LLM）は、特に多言語対応や多モーダル処理能力において急速に進化しています。2025年には、DeepSeek-R1のような高コストパフォーマンスモデルが登場し、中国市場で優れた性能を発揮しましたが、他の言語環境での改善余地も残っています。また、GPT-4oなどの多モーダルモデルは、テキストだけでなく音声や映像も同時に処理できるようになり、ARや自動運転システムなどへの応用が広がっています。さらに、LLMは特定分野を超えてクロスディシプリンな活用が進み、MicrosoftやBaiduなどがオフィスソフトや産業用途に統合しています。

2. **日本のLLM開発における固有の特徴**  
日本では、LLMの開発が主に現地の言語環境や文化に適応することに重点を置いています。例えば、日本語の複雑な文法構造や敬語体系に対応するための微調整が行われています。また、ロボティクスとの連携が強調され、介護や農業といった社会問題解決に向けた実用的なアプリケーションが注目されています。ただし、国際的な競争力においては、米中と比較して基礎技術の革新性で劣る傾向があります。

3. **米国のLLM開発における固有の特徴**  
米国は、LLMの技術的最先端をリードしており、特に新しい基盤技術や革新的なアプリケーションの開発に力を入れています。OpenAIのGPTシリーズやGoogleのPaLMなどは、自然言語理解や生成能力で世界トップクラスの成果を上げています。また、倫理的責任やセキュリティ面にも配慮しながら、学術界と産業界の協力体制を強化することで、持続可能な成長を目指しています。

4. **中国のLLM開発における固有の特徴**  
中国のLLM開発は、現地言語環境での最適化と実用的な適用に焦点を当てています。例えば、ERNIEやChatGLMといったモデルは、中国語圏での高いパフォーマンスを誇ります。また、政府の支援を受けた大規模プロジェクトを通じて、教育や医療、製造業など幅広い分野での導入が進められています。一方で、海外市場での汎用性向上が今後の課題です。

5. **米国と中国のLLM開発のレベル差**  
米国と中国のLLM開発は、それぞれ異なるアプローチを持っています。米国は新技術の創出とイノベーションに重きを置く一方、中国は現地ニーズに即した効率的なソリューション提供に注力しています。両国間の競争は激化していますが、将来的には協力関係を築くことでグローバルAI産業全体の発展につながると期待されています。

6. **シンギュラリティの定義と、その実現時期**  
シンギュラリティとは、人工知能が人間の知能を超えることで引き起こされる「技術的特異点」を指します。この時点を超えると、人類社会は後戻りできないほど変容すると予測されています。レイ・カーツワイルによれば、2045年頃にシンギュラリティが到来するとされており、コンピュータが人間の脳の100億倍以上の計算能力を持つ時代になるとされています。

7. **AGIの実現時期と、現在の課題**  
汎用人工知能（AGI）は、ジョン・マッカーシーによって「人間と同じように様々な知的作業を遂行できる能力」と定義されました。現在の課題は、単一タスクではなく複数領域で学習・適応できるシステムの構築です。専門家の間では、2045年までにAGIが実現する可能性がある一方、ハードウェア限界やソフトウェア設計の難しさから達成時期は不透明です。

8. **人工知能で人々は豊かになるか**  
AIの進化により、一部の人々は富を享受できますが、それが全員に公平に分配される保証はありません。機械所有者が富の再分配に反対すれば、多くの人が貧困に苦しむリスクがあります。また、雇用形態や経済システム全体に大きな影響を与えるため、政策立案者にとって新たな課題となります。

9. **人工知能で失業は増えるか**  
過去の技術革新では、新しい職業が生まれることで総雇用量は減少しませんでした。しかし、AIによる自動化は中流階級の仕事に深刻な影響を与え、高賃金労働者から低賃金労働者への移行を招く恐れがあります。そのため、教育や再訓練プログラムの充実が必要です。

10. **人工知能の金融業界への応用**  
AIは金融取引のリアルタイム監視や不正検出、投資ポートフォリオの最適化に貢献しています。また、ロボアドバイザーを通じて顧客一人ひとりに合わせた財務プランニングを提供し、高頻度取引アルゴリズムによって損失最小化と利益最大化を図ります。

11. **人工知能の教育業界への応用**  
AIは個別学習プラットフォームを通じて生徒ごとにカスタマイズされたカリキュラムを作成します。オンラインコースやチャットボット型サポートツールによって、質の高い教育を世界中に拡大することが可能になります。

12. **人工知能の工業デザインへの応用**  
AIは仮想プロトタイピングや品質管理、予防保全モデルなどを通じて製造プロセスを効率化します。これにより、不良品の削減や設備故障の防止が可能となり、生産性向上に寄与します。

13. **人工知能の農業への応用**  
AIは精密農業を支える技術として、気象予測や土壌分析、病害虫検出に利用されます。これにより、水や肥料の使用量を最適化し、収穫量を増加させることができます。

14. **人工知能の文化への影響**  
AIは芸術作品の生成や翻訳、コンテンツ推薦システムを通じて文化表現に新たな可能性をもたらします。一方で、文化的価値観やアイデンティティに対する影響について議論が求められます。

15. **人工知能の社会への影響**  
AIは労働環境や日常生活を大きく変革します。特にホワイトカラー業務の自動化や家事ロボットの普及が進む中、人間らしいコミュニケーションスキルや創造性がますます重要視されます。

---

### 参考にしたコンテンツ用リスト化フォーマット:

1. **最近のLLMの進歩に関する重要なトピック** - China-U.S. Competition in Large Language Models
2. **日本のLLM開発における固有の特徴** - Comparison of LLM scalability and performance between the U.S. ...
3. **米国のLLM開発における固有の特徴** - Comparison of LLM scalability and performance between the U.S. ...
4. **中国のLLM開発における固有の特徴** - Comparison of LLM scalability and performance between the U.S. ...
5. **米国と中国のLLM開発のレベル差** - Comparison of LLM scalability and performance be-tween the U.S. ...
6. **シンギュラリティの定義と、その実現時期** - 技術的特異点
7. **AGIの実現時期と、現在の課題** - When Will We Reach The Singularity? - Quantum Zeit-geist
8. **人工知能で人々は豊かになるか** - 汎用人工知能
9. **人工知能で失業は増えるか** - 汎用人工知能
10. **人工知能の金融業界への応用** - 15 Applications of Artificial Intelligence - California Miramar University
11. **人工知能の教育業界への応用** - 15 Applications of Artificial Intelligence - California Miramar University
12. **人工知能の工業デザインへの応用** - 15 Applications of Artificial Intelligence - California Miramar University
13. **人工知能の農業への応用** - 15 Applications of Artificial Intelligence - California Miramar University
14. **人工知能の文化への影響** - 多摩地域 Society5.0 等対応 IT 教育プログラム開発事業
15. **人工知能の社会への影響** - 多摩地域 Society5.0 等対応 IT 教育プログラム開発事業

D先生： “推論の内容を見ると、特別に発見もなく「こんなものか」という感想です。ただし、情報はちょっと古い感覚がありますね。「最近」と言う言葉を、より細かく文書の発行期間を指定したほうがよかったのかもしれません。それにしても、参照された手法がE語とJ語だけしかないんですか？本来ならば、C語の言語資料もあるべきと思います。”

QEU:FOUNDER ： “Topicという名前の変数を見て・・・。”

D先生： “そういうことね・・・。じゃあ、今回のプロジェクト必要である「多様性の配慮」を達成しているとは言えませんね。”

![imageEHS1-4-5](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-5.jpg) 

QEU:FOUNDER ： “そうね。もっと追求しなければならない・・・。”

[![MOVIE1](http://img.youtube.com/vi/KuP7maNx_KA/0.jpg)](http://www.youtube.com/watch?v=KuP7maNx_KA "塚本レポ：AGIとASIの定義について")

D先生： “ASIね・・・。AGIとASIの定義はいろいろあるから、FOUNDERは「AGI>ASI」の特性を持つ定義を使っているようです。その定義であれば、**ASIは近々手が届きます**。”

![imageEHS1-4-6](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-6.jpg) 

D先生： “**偉大なGPT（汎用技術）がもうすぐ現れます**。**「昭和のGPT」にこだわっている暇はありません**。ノスタルジーに浸りたい、余裕のあるお方は勝手ですが・・・。”

QEU:FOUNDER ： “若い人には迷惑を掛けたくは無いモノです。さて、次も、多様性を追求するためにシステム設計の再トライです。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “小生は哲学をやっている人を尊敬しています。この人が言っていることは、いちいち重いんです。やっぱり**教養の差**だねえ・・・。”

![imageEHS1-4-7](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-7.jpg) 

QEU:FOUNDER ： “このコメント（↑）を読んだとき、思わず思ったもん・・・。**「ああ・・・。平成だ・・・。」**って・・・（笑）。”

[![MOVIE2](http://img.youtube.com/vi/fLs3Q337DAM/0.jpg)](http://www.youtube.com/watch?v=fLs3Q337DAM "内田樹さんに聞く!! 斎藤・石丸・立花3氏の共通点は「乱世のひと 長くは続かない」!?  現代社会の問題点と解決策についても語りました")

C部長 : “あの・・・、平成の定義は？”

QEU:FOUNDER ： “なっ！？平成の定義とな！？まあ、自分は**「今現在、(小生は)平成にいる」**と思っています。さて、この哲学者の先生が面白いことを言っていて、とても感心していました。**システムの受益者と受難者**って・・・。”

![imageEHS1-4-8](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-8.jpg) 

QEU:FOUNDER ： “(システムの)受難者って言葉を初めて聞きました。これは、マルクスの用語なのか？何はともあれ、平成の時代にはシステムはあるわけです。もう傾きまくって、半分沈んでいるけども、ともあれシステム（タイタニック）は、浮いているという・・・。そのために、そこの船長は、なるべく受難者を増やして何とかしよう・・・。”

![imageEHS1-4-9](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-9.jpg) 

C部長 : “いろいろあったなあ、平成って・・・。多くのモノを受難させて、なんとか**システムを維持してきた**んですね。”

![imageEHS1-4-10](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-10.jpg) 

QEU:FOUNDER ： “しかし、もうシステム自体がヤバくなってくると、**なりふり構わない**よね。最近、ああ、もうシステムがやばいんだって・・・。”

![imageEHS1-4-11](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-11.jpg) 

C部長 : “ビックリしましたよねえ。**「なぜ、アナタ（立〇）が賛成ですか？」**って・・・。”

QEU:FOUNDER ： “システムの受益者だから、アレが倒れたら困るんですよ。だから、本来は自分の仲間であっても、敢て目をつむって受難させるんです。もう、なにふりかまっていない・・・。そして、最後は、こうなるんだよ。”

![imageEHS1-4-12](/2025-06-03-QEUR23_EOHS3/imageEHS1-4-12.jpg) 

QEU:FOUNDER ： “これ（↑）を見たときに、思わず頭がクラッとしました。**「ああ、平成がとうとう終わるんだ」**って・・・。・・・でも、そのシステムが倒れた後にはどうなるんだろう。彼ら（↑）には、なんとかして**「あやかれる」ような勝算**があるんでしょうね。その意味で、すごい人たちだと思います。”
