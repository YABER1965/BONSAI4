---
title: QEUR23_BS4RVW1 - 閑話休題～Qwen-35B_QwQを使ってみる（Simple編）
date: 2025-01-24
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_BS4RVW1 - 閑話休題～Qwen-35B_QwQを使ってみる（Simple編）

## ～ 今回も閑話休題で気軽に、「Simple」に・・・ ～

### ・・・ 前回のつづきです ・・・

D先生（設定年齢65歳）： “今回は、FinetuneにPhi-4を使うんですか？もう、J語のインプットで十分ですね！”

QEU:FOUNDER（設定年齢65歳）   ： “もし、このモデルを使うんだったらJ語を使ってみたいね。でも、現在のところ、再発進した「BONSAI4」の進め方は、いま検討中です。なにしろ最近のAIの進歩は、すごいからねえ・・・。”

![imageBSR1-2-1](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-1.jpg)

D先生（設定年齢65歳）： “そんなに、さらに検討することがあるんですか？”

![imageBSR1-2-2](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-2.jpg)

D先生（設定年齢65歳） ： “千言万語・・・。”

QEU:FOUNDER ： “おっさん、古い・・・。最近、すごいモデルがリリースしているのよ。例の「C国」で・・・。**そのモデルには「検証、反省する能力がある」らしい**よ。”

D先生： “へえ・・・。（それが）なんの役に立つんですかねえ？”

[![MOVIE1](http://img.youtube.com/vi/n1QYofU3_hY/0.jpg)](http://www.youtube.com/watch?v=n1QYofU3_hY "Scaling Law - 大きい Transformer は強い【ディープラーニングの世界vol.38】#123")

QEU:FOUNDER ： “以下、あくまで個人的な意見です。小生は、**「Scaling Law（↑）は実質的にアウト」になっているのではないか**と思うんです。もうそろそろ、人類は、次のことを考えなければならない。”

D先生： “ほう・・・。その「実質的にアウト」とは？”

![imageBSR1-2-3](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-3.jpg)

QEU:FOUNDER  ： “確かに、法則なのだから、モデルのパラメータの規模を大きくすると賢くなるのでしょう。これは今後もそうなのだろうとは思います。しかし、その、「より大きくなったモデルに食わせる」ほど大きな情報を人間は引き続き提供できるのかという問題です。最近は、モデルの学習の効率化のために**蒸留**という技術が注目されているが、これは逆にいうと、**「本当に必要な情報はその程度しかない」**ということです。”

D先生： “現実には、世の中にある情報は無駄に重複している。つまり、外部から取り込める有効な情報は、すでに飽和している。”

QEU:FOUNDER ： “だから、いままでと違うモデル開発の方向を目指すべきでしょう。・・・さてと、そこでQwenのQwQモデルに注目しているんです。これは、まだPreview段階だが・・・。”

![imageBSR1-2-4](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-4.jpg)

QEU:FOUNDER ： “これ以上のくわしい解説は無です。QwQの特性は、テストのあとで分かるから・・・。それではプログラムの一部を紹介します。ちょっと解説付きでね・・・。”

```python
# ---
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    # Can select any from the below:
    # "unsloth/Qwen2.5-0.5B", "unsloth/Qwen2.5-1.5B", "unsloth/Qwen2.5-3B"
    # "unsloth/Qwen2.5-14B",  "unsloth/Qwen2.5-32B",  "unsloth/Qwen2.5-72B",
    # And also all Instruct versions and Math. Coding verisons!
    model_name = "unsloth/QwQ-32B-Preview-unsloth-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

# ---
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3442,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

# ---
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

from datasets import load_dataset
dataset = load_dataset("YxBxRyXJx/QAsimple_URC_250123", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)

```

D先生： “毎度おなじみのプログラムの流れです。このモデルは、ALPACAの**INSTRUCTのフォーマット**を使っているんですね。いまではCHATフォーマットが主流なのに・・・。それにしても、FOUNDER・・・、またもや**「自作のデータセット」**をつくりましたね。”

![imageBSR1-2-5](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-5.jpg)

QEU:FOUNDER ： “前回とほとんど同じです。今のところはね・・・。あとで、すこしづつ変わってきます。これを使って、QwQモデルを学習しましょう。コードの晒しは不要であるので、実行の結果だけをドン！！”

**（メモリの使用）**

![imageBSR1-2-6](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-6.jpg)

**（学習の状況）**

![imageBSR1-2-7](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-7.jpg)

D先生： “いやぁ～・・・。やっぱり、L4のGPUでは無理だったか・・・。それにしても、学習の収束が悪いですね。まあ、このような複雑なモノではこんなものか・・・。もっと学習率の値を低くしたほうがいいのかもしれないですね。さて、推論テストに行くべきでしょう。”

```python
# ---
# Inference
from IPython.display import Markdown, display

def create_response(model, str_question, str_input):
    # ---
    # alpaca_prompt = Copied from above
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
    inputs = tokenizer(
    [
        alpaca_prompt.format(
            str_question, # instruction
            str_input, # input
            "", # output - leave this blank for generation!
        )
    ], return_tensors = "pt").to("cuda")

    outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)
    decoded_outputs = tokenizer.batch_decode(outputs)
    str_instruction = f"{str_question}\n {str_input}"

    return str_instruction, decoded_outputs

# ---
# 推論し、出力する
str_question = "4月に日本に行きます。3泊四日で、おすすめの旅行プランを設定してください。STEP BY STEPで、最適な日程を生成してください。"
str_input = ""
str_instruction, str_response = create_response(model, str_question, str_input)
print("----  QUESTION  ----")
display(Markdown(f"<b>{str_instruction}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{str_response[0]}</b>"))

```

QEU:FOUNDER ： “じゃあ、簡単な推論テストからいきましょう。前回のPHI_4とほぼ同じ質問です。”

![imageBSR1-2-8](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-8.jpg)

D先生： “前回のPhi_4による推論と同じく、「3泊4日で東京、京都」ですか・・・。それにしても、途中で回答が止まっちゃいましたね。”

QEU:FOUNDER ： “最大トークン数が1024では足らなかったですね。A100のGPUを使っているので、まだメモリに余裕があるので、長さを2048にしてもいいと思います。さて、ここから本題にいきます。ちょっと複雑な推論をやります。”

![imageBSR1-2-9](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-9.jpg)

QEU:FOUNDER ： “質問に論理的な複雑性を入れました。初歩的な論理の代表は、やっぱり四則演算ですよね。”

D先生： “QwQは、うまく答えを出しますね。でも・・・、どうやってこのような結論になったのでしょうか・・・。”

QEU:FOUNDER ： “FINETUNINGをすると、中間にあるべき論理展開の表示が抜けるようです。さて、HuggingfaceのSpaceでも、このモデルを使えます。ここで、ちょっと回答例を見てみましょう。”

![imageBSR1-2-10](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-10.jpg)

D先生： “美しい論理ですね。さすが32bです。逆にいうと、**「パラメータが大きくなると頭がよくなる」という感想しかない**です。”

QEU:FOUNDER ： “じゃあ、次の回答事例をやります。これも、自分でSPACEで実行してください。あと、インストラクションの内容が「立山、じゃなく富士山」であれば、もっと答えがわかりやすくなります。”

![imageBSR1-2-11](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-11.jpg)

D先生： “すごい・・・。**QwQモデルが独り言を言っている**。”

QEU:FOUNDER ： “すんごいのよ。このモデルって・・・。自分で推論の手順を考えて、さらに自分で結果の検証もするんです。ただし、ここでタネアカシをすれば、今回の場合には「立山」を使っているので、答えが崩壊しています。立山は、このモデルにとってマイナーな山なので、うまく動かないのです。”

D先生： “これが最新のREASONINGモデルなのか・・・。”

QEU:FOUNDER ： “さっきの「Scaling Law」の話に戻ります。モデルが食う情報を大きくすれば、その分だけ賢くなります。その結果、この問題も大きなモデルでは簡単に解けるでしょう。でも、**それ（巨大なモデル）はムダではないのか？**むしろ、論理を扱う力を強化した比較的に小さなモデルで実用上は十分ではないのか？”

D先生： “なるほど。**そのようなモデルに「FINETUNING」や「RAG」を併用して使えば、無限の応用が期待できます**。”

QEU:FOUNDER ： “次回は、もう少し議論を進めましょう。”


## ～ まとめ ～

C部長 : “とうとうやりました！これは、とても大きな話ですね。”

[![MOVIE2](http://img.youtube.com/vi/KjsD7PhL5ww/0.jpg)](http://www.youtube.com/watch?v=KjsD7PhL5ww "フジテレビ労組会員数が急に六倍以上に増えた話の見どころ")

QEU:FOUNDER ： “もしも、**これ以上に貧乏になりたくなければ、この道を進むしかない**。今回のような、歴史上、**異常中の異常のイベント**により、ようやく**「扉が開いた」**のでしょう。”

![imageBSR1-2-12](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-12.jpg)

QEU:FOUNDER ： “企業単位でムラ化するJ国の文化、そして、その企業に**隷属する人たち**の存在が今回の根本問題でしょう。”

![imageBSR1-2-13](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-13.jpg)

C部長 : “C国は、J国よりも、この点で**はるかに民主である**。その差が、21世紀になって明確になってきたのです。”

[![MOVIE3](http://img.youtube.com/vi/REYlKRUtybw/0.jpg)](http://www.youtube.com/watch?v=REYlKRUtybw "読売新聞さんは立花孝志さんにお怒りです：15分朝刊チェック！ 2025年1月23日")

QEU:FOUNDER ： “一方のJ国は、**企業ムラ社会と新自由主義が妙な結合**をして、近年になり、社会がさらにおかしくなってきたんですよ。これも、ある意味で必然だったかな？”

![imageBSR1-2-14](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-14.jpg)

C部長 : “**新自由主義と通俗道徳**・・・。この一見に異なるモノが、こんなに**相性がいい**とは思わなかったですね。”

[![MOVIE4](http://img.youtube.com/vi/BM7ecZR3jJs/0.jpg)](http://www.youtube.com/watch?v=BM7ecZR3jJs "日本保守党に群がるのはおじいさんばっかり その謎を解き明かす")

C部長 : “**「新自由主義+通俗道徳」は世襲制とも相性がいい**んですよね。”

![imageBSR1-2-15](/2025-01-24-QEUR23_BS4RVW1/imageBSR1-2-15.jpg)

QEU:FOUNDER ： “そりゃあ、そうでしょう・・・。**双方ともに社会の階層を固定化する方向に作用する**のだから・・・。”

### 古い(平成)おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」
 
### 古い(平成)オッサン（＠車中、N社検査不正について）： 「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」
 
### 古い(平成)オッサン（海外工場のあいさつにて、なんと201X年のセリフじゃ！）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

C部長 : “それを全力で推進、展開しようとする。**自称「管理」**・・・（笑）。”

QEU:FOUNDER ： “小生はすでに十数年、この考え方を繰り返し言っています。**「オッサンがJ国を壊した」**って・・・。このように、（この考え方が）見事に**「だれの目にも見える形で証明される」**日が来るとは思わなかったです。でも、オッサンには幸せになって欲しい。彼らが幸せにならないと、我々も幸せにならないのだから・・・。”

