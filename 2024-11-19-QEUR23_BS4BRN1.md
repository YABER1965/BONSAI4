---
title: QEUR23_BS4BRN1 : BAAI-BAEのための学習データをつくる(Unsloth-inference)
date: 2024-11-19
tags: ["QEUシステム", "メトリックス", "Python言語", "LLM", "データセット", "Fine-tuning", "イノベーション","Embedding"]
excerpt: BAAI-BGEモデルをファインチューニングして最適化する
---

## QEUR23_BS4BRN1 : BAAI-BAEのための学習データをつくる(Unsloth-inference)

## ～ Unslothの推論は速い!!（ただし、条件付き） ～

QEU:FOUNDER ： “前回のEmbeddingを使った回帰分析で、各Embedding要素の特性を定量化できることがわかりました。次のステップは、このEmbeddingを生成するしくみをうまく操作したい。”

![imageJRF4-2-1](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-1.jpg)

D先生： “現在、embeddingを生成するために広く使われている**「bge-base-en(BAAI/BGE)」は、LLMで使うtokenizerと何が違う**んでしょうね？いきなり本質的な質問で、すまないです・・・。”

![imageJRF4-2-2](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-2.jpg)

QEU:FOUNDER ： “テキストをEmbeddingに変換するだけであるならば、LLMで使用するtokenizerも同じです。”

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline 

torch.random.manual_seed(0) 

model = AutoModelForCausalLM.from_pretrained( 
    "microsoft/Phi-3.5-MoE-instruct",  
    device_map="cuda",  
    torch_dtype="auto",  
    trust_remote_code=False,  
) 

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-MoE-instruct") 

messages = [ 
    {"role": "system", "content": "You are a helpful AI assistant."}, 
    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragon-fruits?"}, 
    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits to-gether: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."}, 
    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"}, 
] 

pipe = pipeline( 
    "text-generation", 
    model=model, 
    tokenizer=tokenizer, 
) 

generation_args = { 
    "max_new_tokens": 500, 
    "return_full_text": False, 
    "temperature": 0.0, 
    "do_sample": False, 
} 

output = pipe(messages, **generation_args) 
print(output[0]['generated_text'])

```

QEU:FOUNDER ： “BAAI/BGEモデルの機能は、テキストをベクトルに変換するだけなので、OPENAIのtokenizerのように計算や比較などの難しいロジックへの対応はいらないです。むしろ、**本来のBAAI/BGEモデルには学習データに偏りがない**ことが重要です。もちろん、その一方で、ユーザーのケースによっては敢えて**「ファイン・チューニングを使って、BAAI/BGEモデルにある種の偏りを持たせたい」**ですよね。”

D先生： “おっと、**「ある種の偏り」**とな？”

```python
# ---
# 回帰係数の分析結果
occupation_lawyer: 0.5418708615891569
occupation_tradesman/craftsman: 0.48331201858366213
overview_embedding_192: 0.42711222357928263
title_embedding_52: 0.4042839316110261
overview_embedding_727: 0.39196712932051775
title_embedding_735: 0.38356313424138166
overview_embedding_753: 0.381540914050532
title_embedding_162: 0.38019093765658213
title_embedding_538: 0.37404145780595766
overview_embedding_369: 0.3720279627275197
overview_embedding_447: 0.3644984191496451
title_embedding_77: 0.36027067109585975
overview_embedding_405: 0.3391560987900534
title_embedding_601: 0.3355712813077783
title_embedding_435: 0.3348001741628898
title_embedding_173: 0.33378261345805593
overview_embedding_323: 0.33349310090677975
title_embedding_293: 0.33246245218946696
title_embedding_545: 0.30785394335874394
overview_embedding_757: 0.3040394419624819
```

QEU:FOUNDER ： “前回の分析の回帰係数を例に取れば、小生は、係数の回帰係数の値（↑）をもっと高くしたい。さて、前回の分析対象データが「映画の口コミ・ランキング」だったので、映画に関するデータセットを生成してファイン・チューニングをすれば実現できるはず！！”

D先生： “う～ん。できるかなあ・・・。”

QEU:FOUNDER ： “イワシの頭も信心！それでは、プログラムをドン！！”

```python
# ---
# ここでは、LLM を活用して、ラベラーのないドキュメントのコーパスから (クエリ、関連ドキュメント) ペアの合成データセットを生成します。
# まず、LlamaIndex を利用してPDF(TXT)ファイルを読み込み、プレーンなテキスト・チャンクに解析/チャンク化することで、コーパスを作成します。
# ---
import json
from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core.schema import MetadataMode

# ---
# LIST OF DOCUMENTS: TRAIN_FILES
TRAIN_FILES = [
    'The Aesthetics and Psychology Behind Horror Films.pdf',
    'ArtistsintheMovies.docx',
    '9789048531042_ToC_Intro.pdf',
    'Transformation_Of_Film_Critics_Aesthetics_Of_Bad_M.pdf',
    'decades movies.pdf',
    '30 great films of the 2000s.docx',
    'WierzbickiJames2009-FilmMusic.AHistory.pdf',
    'The 100 Greatest Movies of All Time.docx',
    'Ill_be_back_The_Deconstruction_of_the_1980s.pdf',
    'movie-greats-a-critical-study-of-classic-cinema.pdf',
    '70s cinema critics.pdf'
]

# ---
# LIST OF DOCUMENTS: VAL_FILES 
VAL_FILES = [
    'HA26_Leitch.pdf',
    'So-Bad-Its-Good.pdf',
    '9789888139293.pdf',
    'The Relationship Between Socio-Political Changes and Film (Early 2000s).pdf',
    '2006MurrayPhD.pdf',
    'movies-and-the-1990s.pdf',
    'Kelly, 70s Horror Films.pdf',
    'Film Criticism in The 70s Biweekly_Cunliffe.pdf'
]

# ---
# ディレクトリを形成する
str_dir_docs = 'drive/MyDrive/llama_index/docs/'
str_dir_json = 'drive/MyDrive/llama_index/json/'

```

D先生： “いままでの説明で、FOUNDERのやりたいことはわかります。でも、どのようなコーパス（文書資料群）を使用するのですか？”

QEU:FOUNDER ： “映画の評論関係ですね。なにしろ、コーパスを抽出した後でQ&Aを作らなければいけません。文章には、まとまった量が必要です。”

![imageJRF4-2-3](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-3.jpg)

QEU:FOUNDER ： “さらにプログラムはつづきます。”

```python
# ---
TRAIN_CORPUS_FPATH = 'train_corpus.json'
VAL_CORPUS_FPATH   = 'val_corpus.json'

def load_corpus(str_dir, files, verbose=False):
    if verbose:
        print(f"Loading files {files}")

    for i in range(len(files)):
        files[i] = str_dir + files[i]

    # ---
    reader = SimpleDirectoryReader(input_files=files)
    docs = reader.load_data()
    if verbose:
        print(f'Loaded {len(docs)} docs')
    
    parser = SimpleNodeParser.from_defaults()
    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)

    if verbose:
        print(f'Parsed {len(nodes)} nodes')

    corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}
    return corpus

# ---
# Train コーパス群のファイルを train データセットとして、Val コーパス群を val データセットとして使用することで、非常に単純な train/val 分割を実行します。
# ---
train_corpus = load_corpus(str_dir_docs, TRAIN_FILES, verbose=True)
val_corpus = load_corpus(str_dir_docs, VAL_FILES, verbose=True)

with open(str_dir_json+TRAIN_CORPUS_FPATH, 'w+') as f:
    json.dump(train_corpus, f)

with open(str_dir_json+VAL_CORPUS_FPATH, 'w+') as f:
    json.dump(val_corpus, f)
	
```

QEU:FOUNDER ： “うへえ・・・。コーパスっていうのは、ファイルを開いて中身を見ているだけでもうんざりしますね。”

![imageJRF4-2-4](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-4.jpg)

QEU:FOUNDER ： “細かい作業は、機械がやってくれます。つづきにいきます。”

```python
# ---
import re
import uuid
from tqdm.notebook import tqdm

# ---
TRAIN_QUERIES_FPATH = 'train_queries.json'
TRAIN_RELEVANT_DOCS_FPATH = 'train_relevant_docs.json'

VAL_QUERIES_FPATH = 'val_queries.json'
VAL_RELEVANT_DOCS_FPATH = 'val_relevant_docs.json'

# ---
with open(str_dir_json+TRAIN_CORPUS_FPATH, 'r+') as f:
    train_corpus = json.load(f)
#print(train_corpus)

# ---
with open(str_dir_json+VAL_CORPUS_FPATH, 'r+') as f:
    val_corpus = json.load(f)
#print(val_corpus)

# ---
from unsloth import FastLanguageModel

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/gemma-7b-it-bnb-4bit",
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-Instruct",
    max_seq_length = 4096,
    load_in_4bit = True,
)

# ---
from transformers import TextStreamer
from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

```

D先生： “おっと！**Unsloth**がでてきました。でも、今回はFinetuningをしないのですね・・・。”

![imageJRF4-2-5](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-5.jpg)

QEU:FOUNDER ： “マニュアル（↑）の推論編にあったコードをそのまま使いました。よく考えれば、Unslothをinference(推論)専用として使ったことがないんです。どの程度の速さがあるのかをしりたかったんです。もちろん、必要なGPUはLlama3.1-8bで**T4(16GB)相当**です。”

```python
# ---
# get result from response 
def pickup_result(str_cut, result):
    index_cut = result.index(str_cut)
    result = result[index_cut:].replace(str_cut,"")
    result = result.replace("<|eot_id|>","")
    result = result.replace("\n","")

    return result

# ---
# generate queries for response
def generate_queries(text):
    # ----
    prompt_template = """Context information is below.
    
    ---------------------
    {context_str}
    ---------------------
    
    Given the context information and not prior knowledge. generate only questions based on the be-low query.
    
    You are a Teacher/ Professor. Your task is to setup only one question for an upcoming quiz/examination. 
    The question should be diverse in nature across the document. Restrict the questions to the context information provided.
    Do not use phrases such as 'according to the context' because the context will not revealed to oth-ers.
    AGAIN, you should write only one question.
    """

    query = prompt_template.format(context_str=text)

    # ---
    messages = [
        {"from": "human", "value": query},
    ]
    inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = "pt").to("cuda")
    text_streamer = TextStreamer(tokenizer)
    response = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
    response = tokenizer.batch_decode(response)
    result = str(response[0])

    return query, result

# ---
# ある文字列に特定の文字列が含まれているかを判定したい。このとき、特定の文字列が20種類あります。OR条件でつなげてください。Pythonプログラムを生成してください。
def contains_specific_strings(input_string):
    # 特定の文字列のリスト
    specific_strings = [
        "film",
        "films",
        "movie",
        "movies",
        "award",
        "awards",
        "genre",
        "actor",
        "actors",
        "actress",
        "actresses",
        "character",
        "casting",
        "director",
        "directors",
        "producer",
        "moviemaker",
        "audience",
        "theme",
        "theater",
        "script",
        "screenplay",
        "soundtrack",
        "scene",
        "scenery",
        "cinematography",
        "great",
        "amazing",
        "beautiful",
        "interesting",
        "anxious",
        "unpleasant",
        "reality",
        "adventures",
        "influence",
        "experience",
        "entertainment",
        "enjoyment",
        "camera",
        "music"
    ]

    # 各特定の文字列が入力された文字列に含まれているか確認
    for specific_string in specific_strings:
        if specific_string in input_string:
            return True  # 一つでも見つかればTrueを返す

    return False  # どれも見つからなければFalseを返す

```

D先生： “なんですか？この単語の羅列は？”

QEU:FOUNDER ： “チャンク化によって文章データが抽出されます。それが、我々の意図に合っているかどうかをキーワードでチェックしているわけです。”

```python
# ---
# Automatically generate hypothetical questions that could be answered with doc in the corpus.
def list_of_queries(corpus, num_qanda):
    # ---
    arr_query = []
    arr_doc = []
    arr_question = []
    arr_id = []
    icount = 0
    for node_id, text in tqdm(corpus.items()):
        # ---
        if len(text) > 1000 and len(text) < 2000 and icount < num_qanda:
            # ある文字列を含む
            if contains_specific_strings(text):
                # ---
                query, result = generate_queries(text)
                print(f"--- Query NO:{icount} --- length of text: {len(text)} ---")
                #print(text)
                # ---
                str_cut = '<|start_header_id|>human<|end_header_id|>'
                if str_cut in result:
                    result = pickup_result(str_cut, result)
                # ---
                str_cut = '<|start_header_id|>assistant<|end_header_id|>'
                if str_cut in result:
                    result = pickup_result(str_cut, result)
                # ---
                arr_query.append(query)
                arr_doc.append(text)
                arr_question.append(result)
                arr_id.append(node_id)
                # ---
                icount = icount + 1

    return arr_query, arr_doc, arr_question, arr_id

# ---
# <TRAIN>
# 質問(QUESTION)リストを形成する
num_qanda = 500
arr_query, arr_doc, arr_question, arr_id = list_of_queries(train_corpus, num_qanda)
#print(arr_question)

```

D先生： “ここでは、チャンクの文章をコンテキストとして読み込んで、質問を作成しているわけですね。それで、inferenceの応答は速いですか？”

![imageJRF4-2-6](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-6.jpg)

QEU:FOUNDER ： “**かなり応答が速いと思いました。**そもそもLlama3.1ならば、推論だけでもA100(48GB)がないとスムーズに動作しないと思います。その意味では、このパフォーマンスは「大満足」の部類です。”

D先生： “つぎは、同じコーパスと今回生成した質問(Question)を使って、LLMで回答(Answer)を生成するわけですね。”

```python
# ---
# generate answers for response
def generate_answers(text, question):
    # ----
    prompt_template = """Context information is below.
    
    ---------------------
    {context_str}
    ---------------------
    
    Also, question is given below
    
    ---------------------
    {question_str}
    ---------------------
    
    Given the context information and not prior knowledge, you should answer above question.   
    The answer should be diverse in nature across the document. Restrict the answer to the context in-formation provided.
    Do not use phrases such as 'according to the context' because the context will not revealed to oth-ers.
    """

    query = prompt_template.format(context_str=text, question_str=question)

    # ---
    messages = [
        {"from": "human", "value": query},
    ]
    inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = "pt").to("cuda")
    text_streamer = TextStreamer(tokenizer)
    response = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)
    response = tokenizer.batch_decode(response)
    result = str(response[0])

    return query, result

# ---
# Automatically generate hypothetical answers that could be answered with doc in the corpus.
def list_of_answers(corpus, queries):
    # ---
    arr_answer = []
    icount = 0
    for text, question in zip(corpus, queries):
        # ---
        query, result = generate_answers(text, question)
        print(f"--- Answer NO:{icount} --- length of text: {len(text)} ---")
        #print(query)
        # ---
        str_cut = '<|start_header_id|>human<|end_header_id|>'
        if str_cut in result:
            result = pickup_result(str_cut, result)
        # ---
        str_cut = '<|start_header_id|>assistant<|end_header_id|>'
        if str_cut in result:
            result = pickup_result(str_cut, result)
        # ---
        arr_answer.append(result)
        icount = icount + 1

    return arr_answer

# ---
# <TRAIN>
# 回答(ANSWER)リストを形成する
arr_answer = list_of_answers(arr_doc, arr_question)

```

QEU:FOUNDER ： “回答の生成も、質問と同じ要領であり、プロンプトが少しだけ変わっています。回答の応答速度は、さらに速いです。ちなみに、本来ならば、質問量を増やすために1つのチャンクに対して、2つ以上の質問を生成すべきでした。GPT-4みたいな高級なものを使うのであれば、小生も質問生成の複数化にチャレンジしました。しかし、今回は低コストを指向してLlama3.1を使ったので、1チャンクにつき1つだけの質問になっています。さて、最後に、生成されたQ&A情報をファイルに出力しましょう。”

```python
# ---
import pandas as pd

# ---
# データフレームの作成
df = pd.DataFrame({
    'id': arr_id,
    'doc': arr_doc,
    'question': arr_question,
    'answer': arr_answer
})
#df

# JSON形式で出力
json_output = df.to_json(orient='records', lines=False)
#json_output

# ---
# JSONファイルとして保存
with open('drive/MyDrive/movie_review_train.json', 'w') as f:
    f.write(json_output)

# EXCELファイルとして保存
df.to_excel('drive/MyDrive/movie_review_train.xlsx', sheet_name='new_sheet_name')

print("JSON/EXCELファイルが作成されました: movie_review_train.json/xlsx")

```

D先生： “そのようにQ&Aを出力してまとめた結果は、コレ（↓）ですね？すでに、J語に翻訳されています。それにしても、この内容は映画の口コミというには、ちょっと・・・。”

![imageJRF4-2-7](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-7.jpg)

QEU:FOUNDER ： “まあ、そこらへんのテーマの多少のズレはしようがないです。さきほど言ったように、口コミ情報でQ＆Aを作るのは簡単ではないのですよ。”

D先生： “何はともあれ、Embeddingモデルをファイン・チューニングするための学習データの準備ができたわけです。”

QEU:FOUNDER ： “例によって、あとでHugging faceにアップロードするつもりだがね・・・。”

D先生： “今回は、実のところUnslothを使ってみたくて、このようなコードにしたんでしょう？”

![imageJRF4-2-8](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-8.jpg)

QEU:FOUNDER ： “もちろん！今回は、純粋にinferenceだけの角度からUnslothの使いやすさを堪能しました。もうちょっとだけ、notebookを改良してもらえればなあ・・・。”

D先生： “ん！？推論(inference)だけで？”

![imageJRF4-2-9](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-9.jpg)

QEU:FOUNDER ： “今のところは、Unslothはfinetuning用にモデル毎のnotebookが準備されています。同様に、Inferenceでも簡単なnotebookを準備できないかな？もちろん、今回のinferenceの場合、**instructモデルだけになる**と思うが・・・。”

D先生： “そんなもの、本当に必要なのかなあ・・・。”

QEU:FOUNDER ： “必要です。実感しました・・・。LLMモデルによって推奨フォーマットが違うんです。今回のシステムを開発する過程で、別のフォーマットで入力して推論してみました。もちろん、それでもLLMは、きちんと出力します。しかし、この場合にはすごく応答が遅くなります。その意味で、instructモデルのファイン・チューニング前のモデルで、どのようなフォーマットが推奨（最速）なのかがわからないと使えません。”

D先生： “だから、今回はllama3.1モデルを使っているんですね。本来はllama3.2モデルにしたいのに・・・。”

QEU:FOUNDER ： “そういうこと・・・。さてと、コレを使ってデータセットの準備ができたら、finetuningをしてみましょう。”


## ～ まとめ ～

C部長 : “まさか、RAGのBAAI/BGEモデルでfinetuningをするとは思いませんでした。”

QEU:FOUNDER ： “あるテーマについてRAG抽出する場合、べつにBAAI/BGEモデルをfinetuneをする必要がないんです。”

![imageJRF4-2-10](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-10.jpg)

QEU:FOUNDER ： “でもね、そのテーマに自分なりの**「嗜好」が入る**と話が変わってきます。”

![imageJRF4-2-11](/2024-11-19-QEUR23_BS4BRN1/imageJRF4-2-11.jpg)

C部長 : “例えば、同じ「おいしい料理」についてLLMを使って調べているとします。そのとき、アメリカ人の考える「おいしい」と、フランス人のそれは違いますからね。”

QEU:FOUNDER ： “それを、RAGで実現するとどうなるか・・・？”

C部長 : “Embeddingモデルをfinetuneをすると、よりよい推論結果がでると思います。”

QEU:FOUNDER ： “**Unslothで、BAAI/BGEモデルをファイン・チューニング**できないかなあ・・・。ずいぶんと楽になると思うんですよ。”

