---
title: QEUR23_BS4RVW2 - Qwen-35B-Instructを使ってREASONINGを試す(Unsloth-SFT)
date: 2025-02-01
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_BS4RVW2 - Qwen-35B-Instructを使ってREASONINGを試す(Unsloth-SFT)

## ～ そのそも、「現代において」FINETUNINGは必要か？ ～


QEU:FOUNDER（設定年齢65歳）  ： “それでは、前回にテストをした新モデルを使ってFinetuneをしましょう。今回は、学習においてREASONINGの情報をつけています。”

![imageBSR1-3-1](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-1.jpg)


QEU:FOUNDER（設定年齢65歳）  ： “それでは、前回にテストをした新しいQwQモデルを使ってFinetuneをしましょう。今回は、学習情報の中に従来のQ&Aに追加して**REASONING（推論）情報**をインプットしてみます。”

```python
###########################################
# TEXT VANILLA (WITHOUT REASONING)
###########################################
# ---
alpaca_prompt_eng_wo = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# ---
def text_vanilla(instruction, input, output, lang):
    # Must add EOS_TOKEN, otherwise your generation will go on forever!
    text = alpaca_prompt_eng_wo.format(instruction, input, output) + EOS_TOKEN
    # ---
    # 必要ならば、INPUT文字列をカットする
    #str_cut = """### Input:
    #
    #"""
    #if input == "":
    #    text = text.replace(str_cut, "")

    return text

###########################################
# TEXT WITH REASONING
###########################################
# ---
alpaca_prompt_eng_rs = """Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request.
To improve quality of response, you should perform validation before writing. You should create REASONING for validation in advance, and improve the response if necessary.

### Instruction:
{}

### Input:
{}

### Reasoning:
{}

### Response:
{}"""

# ---
def text_reasoning(instruction, input, reasoning, output, lang):
    # Must add EOS_TOKEN, otherwise your generation will go on forever!
    text = alpaca_prompt_eng_rs.format(instruction, input, reasoning, output) + EOS_TOKEN
    # ---
    # 必要ならば、INPUT文字列をカットする
    #str_cut = """### Input:
    #
    #"""
    #if input == "":
    #    text = text.replace(str_cut, "")

    return text

# ---
EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    reasonings   = examples["reasoning"]
    outputs      = examples["output"]
    # syslang : Japanese / English / Chinese
    langs        = examples["syslang"]
    #flags        = examples["sysflag"]
    # ---
    texts = []
    for instruction, input, reasoning, output, lang in zip(instructions, inputs, reasonings, outputs, langs):
        # ---
        val_rand = np.random.rand()
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        if val_rand < 0.15:
            text = text_vanilla(instruction, input, output, lang)
        else: 
            text = text_reasoning(instruction, input, reasoning, output, lang)
        texts.append(text)
    return { "text" : texts, }
pass

# ---
from datasets import load_dataset

org_dataset = load_dataset("YxBxRyXJx/QAsimple_URC_250202", split = "train")
# syslang : Japanese / English / Chinese
train_dataset = org_dataset.filter(lambda example: example['sysflag'] == 'train')
# test_datasetはあとで使います
test_dataset = org_dataset.filter(lambda example: example['sysflag'] == 'test')
#train_dataset
dataset = train_dataset.map(formatting_prompts_func, batched = True,)

```

D先生（設定年齢65歳） ： “最近は、**「REASONINGを使った推論モデル」の予測精度が高いので注目されています**。”

QEU:FOUNDER ： “我々は、1年前あたりからREASONINGに注目していました。**「REASONING IS ALL I NEED」**ってプログラムを立ち上げてね・・・。ただし、その当時に学習に使うモデルが弱い（当時はPhi3-midium）なので、目覚ましい効果が出ませんでした。今回は14bから32bにモデルサイズを拡張しての再トライです。”

D先生： “この再トライに意義なし！！ただし、以前もやってみたプロジェクトです。一気に学習を終わらせて、答えを出してみましょう。ドン！！・・・あれ？”

![imageBSR1-3-2](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-2.jpg)

QEU:FOUNDER ： “なんだ、この結果（↑）は！？”

D先生： “これは、非常に**反日的なモデル**と言えます。いくら「ハルシネーション（幻覚）」とはいえ・・・。”

QEU:FOUNDER ： “このアウトプットは、ある程度は狙ったものとはいえ、まさか本当に出てくるとは思わなかったわ・・・。つまり、このQwenってモデルって、なんと「C国製」です。つまり、**このモデルのRLHF(Reinforcement Learning Human Feedback)の設計（タブー）思想が違うので、今までの西側モデルは出てこないような、「エッジが効いた出力」がでると思っていた**んですよ。”

D先生 ： “ともあれ、このモデル、反日は反日です。プンプン。ケシカラン・・・。この学習済みのモデル、今後どうします？”

![imageBSR1-3-3](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-3.jpg)

QEU:FOUNDER ： “もちろん、（このモデルは）不採用です。ただし、このFinetune済モデルの一番の問題は、文章生成が安定しないことです。たぶん、この原因は、QwQモデルが構築にあたり「循環推論」を採用しているからでしょう。だから、Finetune において外部からReasoningを注入すると、逆に推論がおかしくなっていると思います。ここでは原点に戻って、循環推論を使っていない、かつQwQと同レベルのモデルを採用したいと思います。ちょっと、概念が後退しちゃってつまらんのだが・・・。”

D先生： “このInstructモデル（↑）は、モデル構築に推論(REASONING)を使っているんですかね？”

QEU:FOUNDER ： “そこらへんのモデルの詳細は、調べても分からなかったです。それでは、いきなりUNSLOTHのFINETUNEで学習させますよ。ドン！！”

![imageBSR1-3-4](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-4.jpg)

D先生 ： “あら・・・。学習損失があまり減らないですね・・・。REASONINGのせいかな？”

QEU:FOUNDER ： “かもね・・・。最近、小生もFINETUNEの効果に懐疑的なんだよね・・・。ともあれ、学習効果の確認にいきましょう。”

**----  RESPONSE  ----**
Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request. To improve quality of response, you should perform validation before writing. You should create REASONING for validation in ad-vance, and improve the response if necessary.

**Instruction:**
私はマニラに住んでいます。4月の初旬に桜を見るために日本に行きます。4泊5日で、新幹線をフル活用した、おすすめの旅行プランを設定してください。

**Input:**

**Reasoning:**
この指示に対応するためには、まず日本の桜の見頃の時期と地域を特定する必要があります。次に、訪問する都市とその間の移動時間を考慮して、旅行プランを作成する必要があります。また、各都市での活動を計画し、観光名所や食事場所を特定する必要があります。さらに、新幹線の料金と予約方法を調査し、最適なルートを提案する必要があります。

**Response:**
4月1日：成田空港に到着後、東京駅に向かい、新幹線に乗り換える。 4月2日：京都駅に到着後、清水寺や金閣寺などを見学する。 4月3日：大阪駅に到着後、通天閣や道頓堀などを見学する。 4月4日：名古屋駅に到着後、名古屋城や久屋大通などを見学する。 4月5日：新幹線で帰国する。


QEU:FOUNDER  ： “まあ、スタイルの調整は、ちゃんとできています。REASONINGの出力は、しっかりと出てきますよね。ただし、**このReasoningは出力(OUTPUT RESULT)として出て来ているのか？それとも、出力をコントロールするための中間出力(INTERMIDIATE RESULT/INPUT)として出ているのか**・・・？”


D先生： “で？どうなんです？”

QEU:FOUNDER ： “・・・（笑）。もう一件、ちょっと違う事例をみてみましょう。これは、なかなか良い出来だと思いません？”

**----  RESPONSE  ----**
Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request. To improve quality of response, you should perform validation before writing. You should create REASONING for validation in advance, and improve the response if necessary.

**Instruction:**
地図を見ると、ロシアの面積がアフリカ大陸と同じ大きさに見えるがなぜなのか？本当は、大きさはどれほど違うのか？

**Input:**

**Reasoning:**
まず、地図の投影法について理解する必要があります。通常の地図は、球体である地球を平面に投影しているため、形状や大きさが変形します。この変形は、特に高緯度地域（北極や南極付近）で顕著になります。次に、ロシアとアフリカの面積を比較します。ロシアの面積は約17,098,246平方キロメートル、アフリカ大陸の面積は約30,370,000平方キロメートルです。これらを比較することで、実際の面積の差を理解することができます。

**Response:**
地図では、ロシアの面積がアフリカ大陸とほぼ同じように見えますが、実際にはロシアの面積は約17,098,246平方キロメートルで、アフリカ大陸の面積は約30,370,000平方キロメートルです。つまり、アフリカ大陸の方が約13,271,754平方キロメートルも大きいのです。この差異は、地図の投影法による変形によって生じるため、正確な面積を理解するためには、実際の面積を確認することが重要です。

D先生 ： “これは簡単な理論を含んだ問題だと思いますが、良い出力を出しますね。やっぱりモデルは出力の過程でREASONINGを使っているんじゃないですか？”

QEU:FOUNDER ： “もう一件、別の論理題をみてみましょう。”

![imageBSR1-3-5](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-5.jpg)

D先生： “ほう、これは「アンチ-ジャック-ジルの件」ですね。このような簡単な問題でも、LLMの答えは失敗しています。これ、QwQモデルでは正解が出るんでしょうね？”

QEU:FOUNDER ： “そこらへんは調べてはいないが、おそらく成功するでしょう。まあ、今回の場合、少なくともREASONINGを見る限り、とても成功しそうもないですね。”

D先生 ： “ちょっと昔の話に戻ります。昔、我々がPhi3-midium(14b)でreasoningモデルを構築しようと思ったときに、どのような困難があったんですか？”

QEU:FOUNDER ： “reasoningを少し改造して再度出力を要求するでしょ？そのときに、「（出力するために、）とんでもなく長い時間がかかる」んです（笑）。”

D先生： “その意味では、「（モデルは）むりやりREASONINGを使っている（笑）」とはいえますね。”

QEU:FOUNDER  ： “モデルが32bに大きくなって、この「貧しい環境が変わる」のかどうかが見ものなんです。”

D先生： “おもしろいですね。じゃあ、さっきの論理題をREASONINGを少しづつ変えて解いてみましょう。”

QEU:FOUNDER ： “というわけで、次回につづく・・・。”


## ～ まとめ ～

C部長 : “例のC国製造LLM（↓）の件。皆が、それぞれ好き好きに言っていますね（笑）。”

[![MOVIE1](http://img.youtube.com/vi/snn2JbAgVc4/0.jpg)](http://www.youtube.com/watch?v=snn2JbAgVc4 "【DeepSeek速報解説】40歳の超新星／超低コストで開発できた３つの工夫／OpenAIとGoogleの脅威ではない／NVIDIAのピンチか？／オープンソースで公開した狙い／米中対立への影響")

QEU:FOUNDER ： “まあ、楽しそうだからいいんじゃない（笑）？ただし、小生から2つだけ言えます。一つは、「そんなことは2年以上前からわかっている」ということ・・・。”

![imageBSR1-3-6](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-6.jpg)

QEU:FOUNDER ： “ある事故で、M社のLLAMAモデルの重みデータが外部に流出し、それがオープンLLMの誕生のきっかけになりました。その後、**G社が今後のLLMの発展を予測し、将来的にM社のモデルが圧勝するであろう**と・・・。”

C部長: “現在までのところ、オープンLLMの強さを証明しているようですね。もう一つは？”

[![MOVIE2](http://img.youtube.com/vi/Vvksm5G7wPk/0.jpg)](http://www.youtube.com/watch?v=Vvksm5G7wPk "DeepSeekは業界標準のCUDAを回避しPTXプログラミングを使用している")


QEU:FOUNDER ： “CUDAって、強力で便利な反面、普通の人が普通にLLMを使うための制約になっていました。それが変わるかもしれません。”
D先生： “これも煎じ詰めれば、オープンLLMの勝ちに行くんでしょ？”

QEU:FOUNDER ： “推論LLMがどこまで発展し、より便利になるのかによるよね。小生は、**必要な論理を扱えるのであれば、ほとんどすべての実用において最小サイズのLLMで十分と思っています**。”

C部長： “これはQwQのこと？”

![imageBSR1-3-7](/2025-02-02-QEUR23_BS4RVW2/imageBSR1-3-7.jpg)

QEU:FOUNDER ： “わからん・・・（笑）。さらに言えば、**「論理を扱うLLMでFinetuneは可能か？」**ということになってきます。「Deepなんとか」の話は、論理（推論：REASONING）LLMが主流になることが見えてきたということですからね。ちなみに、このモデルの学習ではSFT（教師あり学習）を使わなかったとか・・・。強化学習が効果あるらしいよ。”

D先生： “ゲッ・・・。我々は、今、SFTでFinetuneしているじゃないですか・・・。”

QEU:FOUNDER  ： “いづれ、すべての発想が見直しされる時期が来るかもしれん・・・。”
