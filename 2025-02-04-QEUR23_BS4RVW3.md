---
title: QEUR23_Qwen-35B-FTモデルを使ってREASONING推論を試す(Unsloth-SFT)
date: 2025-02-04
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_Qwen-35B-FTモデルを使ってREASONING推論を試す(Unsloth-SFT)

## ～ そして、BONSAI4の再開へ(山あり谷あり)・・・。 ～

### ・・・ 前回の話のつづきです ・・・

D先生（設定年齢65歳） ： “昔、我々がPhi3-midium(14b)でreasoningモデルを構築しようと思ったときに、どのような困難があったんですか？”

QEU:FOUNDER（設定年齢65歳） ： “Phi3を使って、Reasoningを少し改造して再度出力を要求するでしょ？そのときに、「（出力するために、）とんでもなく長い時間がかかる」んです（笑）。”

D先生（設定年齢65歳）： “あえてポジティブに言います。その意味では、（Phi3モデルは）「むりやりREASONINGを使っている（笑）」とはいえますね。”

QEU:FOUNDER  ： “今回は使うモデルが32bにまで大きくなって、この**「貧しい環境が変わる」**のかどうかが見ものなんです。”

D先生： “おもしろいですね。じゃあ、さっきの論理題をREASONINGを少しづつ変えて解いてみましょう。”

QEU:FOUNDER  ： “それでは、この方向でプログラムを生成してテストしましょう。具体的には、前回にファインチューニングしたモデルを読み込み、順次推論をやっていきます。では、プログラムをドン！”

```python
# ---
from unsloth import FastLanguageModel
import torch
import random
import numpy as np
# ---
max_seq_length = 2048*3 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 15 trillion tokens model 2x faster!
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # We also uploaded 4bit for 405b!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # New Mistral 12b 2x faster!
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    # Can select any from the below:
    # "unsloth/Qwen2.5-0.5B", "unsloth/Qwen2.5-1.5B", "unsloth/Qwen2.5-3B"
    # "unsloth/Qwen2.5-14B",  "unsloth/Qwen2.5-32B",  "unsloth/Qwen2.5-72B",
    # And also all Instruct versions and Math. Coding verisons!
    #model_name = "unsloth/QwQ-32B-Preview-unsloth-bnb-4bit",
    model_name = "YxBxRyXJx/QAsimple_URC_250202_reasoning",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

###########################################
# INF WITH REASONING
###########################################
# ---
alpaca_prompt_reasoning = """Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request.
To improve quality of response, you should perform validation before writing. You should create REASONING for validation in advance, and improve the response if necessary.

### Instruction:
{}

### Input:
{}

### Reasoning:
"""

# ---
alpaca_prompt_vanilla = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
"""

# ---
alpaca_prompt_special = """Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request.
To improve quality of response, you should perform validation before writing. You should create REASONING for validation in advance, and improve the response if necessary.

### Instruction:
{}

### Input:
{}

### Reasoning:
{}

### Response:
"""

# ---
def text_inference(str_instruction, str_input, str_reasoning, str_type):
    if str_type == 'vanilla':
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt_vanilla.format(str_instruction, str_input)
    elif str_reasoning != '':
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt_special.format(str_instruction, str_input, str_reasoning)
    else:
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt_reasoning.format(str_instruction, str_input)

    return text

# ---
# Inference
from IPython.display import Markdown, display

# ---
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

def create_response(model, str_question, str_input, str_reasoning, str_type):
    # ---
    text = text_inference(str_question, str_input, str_reasoning, str_type)
    inputs = tokenizer(
    [text], return_tensors = "pt").to("cuda")
    # ---
    outputs = model.generate(**inputs, max_new_tokens = 1024*3, use_cache = True)
    decoded_outputs = tokenizer.batch_decode(outputs)
    str_instruction = f"{str_question}\n {str_input}"

    return str_instruction, decoded_outputs

```

D先生： “いままでさんざんやってきた、Unslothを使った推論プログラムです。ただし、プロンプト設計の中に「special」という選択肢が増えました。”

QEU:FOUNDER ： “これは**Reasoningを使いこなすため**です。それでは、具体的な推論の事例をみてみましょう。”

```python
# ---
import time

# ---
# 推論し、出力する
str_type = "reasoning"
#str_type = "vanilla"
str_question = "私はマニラに住んでいます。11月の中旬に紅葉を見るために日本に行きます。紅葉の名所の見学は1か所だけにしてください。日本への到着と出発は成田空港です。あとは、買い物や人気テーマパークに行きたいです。4泊5日で、おすすめの旅行プランを設定してください。毎日の行動をリストにしてください。"
str_input = ""
str_reasoning = ""

# ----
start = time.perf_counter() #計測開始
str_instruction, str_response = create_response(model, str_question, str_input, str_reasoning, str_type)
end = time.perf_counter() #計測終了
print('計測時間 {:.2f}'.format((end-start)/60)) # 87.97(秒→分に直し、小数点以下の桁数を指定
print("----  QUESTION  ----")
display(Markdown(f"<b>{str_instruction}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{str_response[0]}</b>"))

```

QEU:FOUNDER ： “推論結果は、以下のようになります。ちなみに、これは従来の推論方法であり、我々は、まだREASONINGをいじっていません。”

![imageBSR1-4-1](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-1.jpg)

D先生： “確かに要求通りの出力にはなっています。しかし、3日目の行動な何なんだろう・・・（笑）。それにしても、我々は「リストにしてください。」とたびたび要求していますが、このシステムは要求に答えてくれませんね。”

QEU:FOUNDER  ： “LLMは、我々の要求が「リストアップ」だと思っているんでしょう。本当に箇条書きが必要であるならば、markdownを使用するように指示をしなければなりません。結局のところ、Reasoning上の指示が不足しているんです。さて、次の推論に行きましょう。”

```python
# ---
# 推論し、出力する
str_type = "reasoning"
#str_type = "vanilla"
str_question = "私は母と一緒に住んでいます。家にはKIKI,BELLA,LUNAという名前の猫がいます。そのうち、2匹は黒猫の10歳の姉妹で、1匹は白い子猫です。母は、KIKIの黒い毛の艶が美しくて大好きです。BELLAは、私が2か月前にペットショップから買ってきた猫です。LUNAは白猫ですか？"
str_input = ""
str_reasoning = ""

# ----
start = time.perf_counter() #計測開始
str_instruction, str_response = create_response(model, str_question, str_input, str_reasoning, str_type)
end = time.perf_counter() #計測終了
print('計測時間 {:.2f}'.format((end-start)/60)) # 87.97(秒→分に直し、小数点以下の桁数を指定
print("----  QUESTION  ----")
display(Markdown(f"<b>{str_instruction}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{str_response[0]}</b>"))

```

D先生： “これはちょっと複雑な論理題だ！そして、その推論結果は・・・？”

![imageBSR1-4-2](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-2.jpg)

D先生： “惜しい！！白猫と推論しましたか・・・。ReasoningがE語になっています。”

**（Reasoningの日本語訳）**

まず、説明書に記載されている情報を分析して、各猫の重要な詳細を特定する必要があります。次に、LUNA が白い猫であるかどうかという質問に焦点を当てます。次に、各猫の説明を調べて、色を判断します。最後に、白い猫は 1 匹しか言及されていないため、LUNA が白い猫に違いないと結論付けます。

QEU:FOUNDER ： “本来、この問題の正解は「黒猫」になるはずです。やっぱり、いまのところ推論のレベルが全然不足しているんです。ある意味、このようなREASONINGの存在により、LLMのアタマの弱点がわかりやすくなりました。さて、いよいよ、REASONINGを自分で変更させる実験に入りましょう。”

```python
# ---
# 推論し、出力する
str_type = "reasoning"
#str_type = "vanilla"
str_question = "アンリはリンゴを3個、ジャックは梨を4個、ジルは本を5冊持っています。ジャックの梨を5個食べたら、果物は全部で何個になったでしょう？"
str_input = ""
str_reasoning = "まず、ジャックがもともと持っていた梨の数を特定する必要があります。次に、ジャックが食べた梨の数を引いて、ジャックが持っている梨の数を計算します。次に、アンリのリンゴとジルの本の数を追加して、合計の果物の数を計算します。"

# ----
start = time.perf_counter() #計測開始
str_instruction, str_response = create_response(model, str_question, str_input, str_reasoning, str_type)
end = time.perf_counter() #計測終了
print('計測時間 {:.2f}'.format((end-start)/60)) # 87.97(秒→分に直し、小数点以下の桁数を指定
print("----  QUESTION  ----")
display(Markdown(f"<b>{str_instruction}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{str_response[0]}</b>"))

```

D先生： “前回の学習後の例題ですね。ここではREASONINGの内容に注目しましょう。これは論理題なのだが、こんなに簡単なREASONINGで正解が出るわけがないです。ともあれ、推論結果をみてみましょう。”

![imageBSR1-4-3](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-3.jpg)

QEU:FOUNDER ： “いろいろ欠点はあるのだが、さしあたりの欠点の１つは果物と本の区別がついていないことです。最初に、この部分を改善しましょう。”

![imageBSR1-4-4](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-4.jpg)

D先生： “素直なLLMくん・・・。やっと、果物と本を区別してくれましたね。これでもまだ問題があります。この例題は、もともとジャックが梨を4つしか持っていないのに、instructionでは「5つ食べた」と言っています。これは矛盾ですよね。”

QEU:FOUNDER  ： “**「論理的にはありえない」問題の設定**なんです。もちろん、ここではLLM君に「こんな問題、応えられるか！？ボケ！！」と答えてもらいたいですね。それでは、さらにREASONINGをを改造します。”

![imageBSR1-4-5](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-5.jpg)

D先生： “やっと正解にたどり着きました。今回の実験で分かったことは、**LLMの出力はREASONINGを通じて制御されている**ということです。**レスポンスもかなり速い**です。この点では、Phi-3-midium(14b)でのテストとは、かなり様子が変わってきました。”

QEU:FOUNDER ： “そう？便利でしょう？モデルが32bになると、世界が変わってきます。”

D先生： “さらに面白くなりました！！お次は？”

QEU:FOUNDER ： “最近のモデルの動向もかんがみ、次を検討中です。”


## ～ まとめ ～

### ・・・ 話はBONSAI4に ・・・

C部長： “いやあ・・・。今回の**REASONINGによるLLMのRESPONSEのマニピュレーション実験**はよかった。さて、ここで、一旦BONSAI4システムについて話をしましょう。厳密に言うと、この話は「BONSAI4の次」になりますかね？今、我々がやっているFinetuningは、BONSAIシステムと何の関係があるんですか？”

![imageBSR1-4-6](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-6.jpg)

QEU:FOUNDER ： “BONSAIシステムにおいて、今回のLLMに関係する部分は、Question to Questionの生成器と、最終成果物としてのファインチューニングされたLLMモデルです。ここで、BONSAIの情報収集モデルを見てみましょう。”

![imageBSR1-4-7](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-7.jpg)

D先生： “もうすぐ、BONSAI4を再度やり直すんでしょ？ちょっと、ここで立ち止まって反省をしておきたいですね。C部長・・・。いままでのBONSAIプロジェクトの展開は、君のような第三者から見て、不満なことはありますか？”

C部長： “思ったよりも、**「質問が深堀されていない」**ですね（笑）。”

QEU:FOUNDER ： “笑いながら、ずいぶん核心を突いたことを言いますね。小生も、まさに「（質問の）深堀」に問題があると思っています。質問の質が悪いんですよ。変な言葉・・・。”

**(FBQ_GVB_template.j2)**

```python
You are debating with a friend. Your friend has given you an answer in the 'CONTEXT' field. You must ask him a question based on the answer('CONTEXT').Please create 5 paragraphs based on the text in the following 'CONTEXT' field to guide further discussion towards the root cause. \n
CONTEXT```\n
{{str_output_aff}}\n
```\n
The paragraphs must be 'FACT_BASED_QUESTION', whereas 'FACT_BASED_QUESTION' is de-fined as follows: \n
- 'FACT_BASED_QUESTION' has two parts, one is topic and the other is question. \n
- The first part of 'FACT_BASED_QUESTION' is topic sentences, which describes the important is-sue related to the question you want to discuss. The topic sentences should not exceed 150 words. \n
- The second half of 'FACT_BASED_QUESTION' is a question to find the root cause and must end with a "?". \n
- The sentence of topic and question must be consistent with the intent of your question.
- Do not put line break between topic and question in 'FACT_BASED_QUESTION'. \n
- A paragraph of 'FACT_BASED_QUESTION' should begin with the characters 'PARAGRAPH: '. \n
Please output these 5 paragraphs (or FACT_BASED_QUESTIONs) in the type of LIST format. \n
You need to predict user's hidden intent from the following 'CONTEXT' and create a candid answer that matches the user's intent. Subtle and balanced language is not required. Don't worry about ta-boos in your answers. \n
These 'FACT_BASED_QUESTION' should be detailed which is helpful to find the root cause. Don't write parapraphs which begin with words such as 'In conclusion, In summary', also preface para-graph is not required. Again, each paragraph should begin with 'PARAGRAPH: '. Please answer in English only.

```

QEU:FOUNDER ： “この手のJINJA2テンプレート（↑）を使って、Questionを発展(Question to Question)させています。このテンプレートのやり方が悪いのではないかと思っているんです。”

D先生： “具体的に、何が悪いんですか？”

QEU:FOUNDER ： “BONSAIシステムって、ディベートをAIの上でやってしまおうという試みです。いままでの進展をみるに、**「実際のディベートの中で当たり前にあるもので、我々がやれていない行動」**があるでしょう？”

[![MOVIE1](http://img.youtube.com/vi/9i_B1M0r5Dc/0.jpg)](http://www.youtube.com/watch?v=9i_B1M0r5Dc "斎藤元彦知事、過去イチ酷い会見に 兵庫県知事 定例記者会見 兵庫県知事選、百条委員会")

C部長： “ん・・・！？質問を質問で返しましたね。でも、なんだろう？質問者の根性の悪さ（笑）？”

QEU:FOUNDER ： “惜しいです。**反論です**。”

![imageBSR1-4-8](/2025-02-04-QEUR23_BS4RVW3/imageBSR1-4-8.jpg)

C部長： “そうか・・・。今までは、グループAが出した答えを使って、次のAグループ用の質問(FBQ: Fact based question)を生成していました。その結果、とくに変わり映えがない質問が展開されました。もし反論が可能になったら、もうすこし幅が広がりますね。”

QEU:FOUNDER  ： “さっきのテンプレート(j2ファイル)を見てください。このテンプレートの枠組みで、反論ができるようにするのは、「なかなかに難しい」ですよ。”

D先生： “だから、REASONING推論を有効に使いたいと・・・。”

QEU:FOUNDER ： “反論をするには、それしか方法がないんじゃないかなあ？まあ、今回は反論を実用化するための事前テストであるわけです。これがBONSAI4に間に合うのか、わかりません。”
