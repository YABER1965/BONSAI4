---
title: QEUR23_BS4RVW12 - 閑話休題～Solve it with code準備編(Qwen-coder_dqn)
date: 2025-03-14
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "DeepSeek"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_BS4RVW12 - 閑話休題～Solve it with code準備編(Qwen-coder_dqn)

## ～ Qwen-coderもすごいのですが・・・ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “もしも、QwQモデルのREASONINGで「システム要件定義書」が出てくるのであれば、コーディングの世界に革命が起きるでしょう。ユーザーがREASONINGを読みながら、気に入らない部分を変更していけば、ユーザーが望むコードが、極めて短期間で得られます。”

![imageBSR1-12-1](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-1.jpg) 

D先生： “もう一つ付け加えると、出力されるコードは「モジュール単位」であるべき。ここまでくると、少なくとも2年後には**「（創造性を持たない）プログラマは要らなくなる」**なあ・・・。“

![imageBSR1-12-2](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-2.jpg) 

QEU:FOUNDER ： “同意、**少なくとも2年以内**・・・。それでもね、ここまででも「EVERYBODY IS CREATIVE CLASS」の実現に十分に近づいたと言えます。ただし、これには注釈が付きます。誰もが、CREATIVE CLASSになれるわけではないです。**CREATIVITYを持っている誰もが、CREATIVE CLASSになる**わけです。”

D先生：“CREATIVEな人って、いったい何人いるのやら・・・。”

![imageBSR1-12-3](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-3.jpg) 

QEU:FOUNDER ： “CREATIVEな人は3種類いると思います。まず一つは、今までにない理論の構築ができる人です。最近、AI界隈では、このような人ばかり脚光が当たるのが気になります。もう一つは、顧客満足を高めるアプリケーションを作る能力がある人、つまり設計者です。最後は、**「誰も持っていないデータを得ることができる人」**です。これが、実は圧倒的に多いです。現実に作業をしたときに得られるデータが最も貴重です。”

D先生：“FOUNDER・・・。本件（CODE生成の自動化）をもう一歩進めませんか？”

QEU:FOUNDER ： “じゃあ、CODE生成用のLLMモデルを使ってみましょうか。これも、Unslothにたくさんのモデルがあります。ここでは、もっとも強力なInstructionモデルを使います。”

![imageBSR1-12-4](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-4.jpg) 

QEU:FOUNDER ： “それでは、推論用プログラムをドン！！”

```python
# ---
from unsloth import FastLanguageModel, PatchFastRL

from unsloth import is_bfloat16_supported
import torch
max_seq_length = 1024*4*2 # Can increase for longer reasoning traces
#lora_rank = 64 # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    #model_name = "Qwen/Qwen2.5-3B-Instruct",
    #model_name = "unsloth/QwQ-32B-unsloth-bnb-4bit",
    model_name = "unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    #max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.95, 
)

# ---
text = tokenizer.apply_chat_template([
    {"role" : "user", "content" : "1から50までの整数の合計を出力するPythonプログラムを生成してください。"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.8,
    top_p = 0.95,
    max_tokens = 1024*3,
)
output = model.fast_generate(
    [text],
    sampling_params = sampling_params,
    lora_request = None,
)[0].outputs[0].text

# ---
# Inference
from IPython.display import Markdown, display

# ---
# 推論し、出力する
print("----  RESPONSE  ----")
display(Markdown(f"<b>{output}</b>"))
```

D先生： “Unslothのinferenceの機能だけを使ったんですね。 まあ、この程度の質問であれば、簡単に答えるでしょう。“

![imageBSR1-12-5](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-5.jpg) 

QEU:FOUNDER ： “じゃあ、次にいきましょう。今度は、少し難しいよ。”

```python
# ----
SYSTEM_PROMPT = """
あなたは優秀なプログラマです。今から、ユーザーの要求に基づき、PYTHONプログラムを作成します。
INSTRUCTIONの文章には、生成すべきプログラムの要求事項が示されています。
そして、INPUTの文章には、さらに詳細の要求事項や参考プログラムが示されています。
あなたは、それらを参考にしてPythonプログラムを生成し、そのプログラムの使い方、注意事項を生成してください。

# INSTRUCTION
...


# INPUT
...

# RESPONSE
"""
# ---
def generate_answer(str_instruction, str_input):
    # ----
    INPUT_PROMPT = f"""
    以下のINSTRUCTIONとINPUTの情報に基づき、Pythonプログラムを生成し、同時に、そのプログラムの使い方、注意事項を生成してください。

    # INSTRUCTION
    {str_instruction}

    # INPUT
    {str_input}

    # RESPONSE
    """
    #print(INPUT_PROMPT)
    # ----
    text = tokenizer.apply_chat_template([
        {"role" : "system", "content" : SYSTEM_PROMPT},
        {"role" : "user", "content" : INPUT_PROMPT},
    ], tokenize = False, add_generation_prompt = True)

    from vllm import SamplingParams
    sampling_params = SamplingParams(
        temperature = 0.8,
        top_p = 0.95,
        max_tokens = 1024*3,
    )
    output = model.fast_generate(
        text,
        sampling_params = sampling_params,
        lora_request = None,
    )[0].outputs[0].text

    return INPUT_PROMPT, output

# ---
str_instruction = """以下の2048ゲームを改造して環境として使った、強化学習プログラムを生成してください。Step数は20000回にしてください。
モデルは3層のDQNモデルを使用し、隠れ層のパラメタ数は256としてください。
pygameによる環境のレンダリングは不要です。ただし、5ステップに1回は、ゲームオーバー時の状態を表示してください。
各ステップの学習後の学習損失値とrewardsはリストに保管して、matplotlibで折れ線グラフにしてください。
グラフには、生の変動値とともに移動平均値を表示してください。２つのグラフはsubplotで結合させてください。"""
str_input = """import pygame
import random

pygame.init()

WINDOW_SIZE = 400
GRID_SIZE = 4
TILE_SIZE = WINDOW_SIZE // GRID_SIZE
MARGIN = 5

window = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))
pygame.display.set_caption("2048")

WHITE = (255, 255, 255)
GRAY = (200, 200, 200)
BLACK = (0, 0, 0)
COLORS = {
    0: (204, 192, 180),
    2: (238, 228, 218),
    4: (237, 224, 200),
    8: (242, 177, 121),
    16: (245, 149, 99),
    32: (246, 124, 95),
    64: (246, 94, 59),
    128: (237, 207, 114),
    256: (237, 204, 97),
    512: (237, 200, 80),
    1024: (237, 197, 63),
    2048: (237, 194, 46),
}

font = pygame.font.SysFont("Arial", 30)

def initialize_grid():
    grid = [[0] * GRID_SIZE for _ in range(GRID_SIZE)]
    add_new_tile(grid)
    add_new_tile(grid)
    return grid

def add_new_tile(grid):
    empty_cells = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE) if grid[i][j] == 0]
    if empty_cells:
        i, j = random.choice(empty_cells)
        grid[i][j] = 2 if random.random() < 0.9 else 4

def compress_and_merge(row):
    compressed = [num for num in row if num != 0]
    compressed += [0] * (len(row) - len(compressed))
    merged = []
    has_merged = False
    i = 0
    while i < len(compressed):
        if i < len(compressed) - 1 and compressed[i] == compressed[i+1]:
            merged.append(compressed[i] * 2)
            i += 2
            has_merged = True
        else:
            merged.append(compressed[i])
            i += 1
    new_compressed = [num for num in merged if num != 0]
    new_row = new_compressed + [0] * (len(row) - len(new_compressed))
    has_moved = (new_row != list(row)) or has_merged
    return new_row, has_moved

def move_left(grid):
    new_grid = []
    moved = False
    for row in grid:
        new_row, has_moved = compress_and_merge(row)
        new_grid.append(new_row)
        if has_moved:
            moved = True
    return new_grid, moved

def move_right(grid):
    new_grid = []
    moved = False
    for row in grid:
        reversed_row = row[::-1]
        new_reversed_row, has_moved = compress_and_merge(reversed_row)
        new_row = new_reversed_row[::-1]
        new_grid.append(new_row)
        if has_moved:
            moved = True
    return new_grid, moved

def move_up(grid):
    transposed = list(zip(*grid))
    new_transposed = []
    moved = False
    for row in transposed:
        new_row, has_moved = compress_and_merge(row)
        new_transposed.append(new_row)
        if has_moved:
            moved = True
    new_grid = list(zip(*new_transposed))
    return [list(row) for row in new_grid], moved

def move_down(grid):
    transposed = list(zip(*grid))
    new_transposed = []
    moved = False
    for row in transposed:
        reversed_row = row[::-1]
        new_reversed_row, has_moved = compress_and_merge(reversed_row)
        new_row = new_reversed_row[::-1]
        new_transposed.append(new_row)
        if has_moved:
            moved = True
    new_grid = list(zip(*new_transposed))
    return [list(row) for row in new_grid], moved

def is_game_over(grid):
    for row in grid:
        if 0 in row:
            return False
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE - 1):
            if grid[i][j] == grid[i][j+1]:
                return False
    for j in range(GRID_SIZE):
        for i in range(GRID_SIZE - 1):
            if grid[i][j] == grid[i+1][j]:
                return False
    return True

def draw_grid(window, grid):
    window.fill(WHITE)
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE):
            x = j * TILE_SIZE + MARGIN
            y = i * TILE_SIZE + MARGIN
            rect = pygame.Rect(x, y, TILE_SIZE - 2*MARGIN, TILE_SIZE - 2*MARGIN)
            value = grid[i][j]
            color = COLORS.get(value, (204, 192, 180))
            pygame.draw.rect(window, color, rect)
            if value != 0:
                text = font.render(str(value), True, BLACK)
                text_rect = text.get_rect(center=rect.center)
                window.blit(text, text_rect)
    pygame.display.update()

def main():
    grid = initialize_grid()
    running = True
    game_over = False
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_q:
                    running = False
                if not game_over:
                    if event.key in (pygame.K_LEFT, pygame.K_RIGHT, pygame.K_UP, pygame.K_DOWN):
                        old_grid = [row.copy() for row in grid]
                        if event.key == pygame.K_LEFT:
                            new_grid, moved = move_left(grid)
                        elif event.key == pygame.K_RIGHT:
                            new_grid, moved = move_right(grid)
                        elif event.key == pygame.K_UP:
                            new_grid, moved = move_up(grid)
                        elif event.key == pygame.K_DOWN:
                            new_grid, moved = move_down(grid)
                        if moved:
                            grid[:] = new_grid
                            add_new_tile(grid)
                        game_over = is_game_over(grid)
        draw_grid(window, grid)
        if game_over:
            text = font.render("Game Over! Press 'q' to quit", True, BLACK)
            text_rect = text.get_rect(center=(WINDOW_SIZE//2, WINDOW_SIZE//2))
            window.blit(text, text_rect)
            pygame.display.update()
    pygame.quit()

if __name__ == "__main__":
    main()"""
# ---
# Inference
from IPython.display import Markdown, display

# ---
# 推論し、出力する
str_input, str_output = generate_answer(str_instruction, str_input)
#print(str_output)
# ---
print("----  QUESTION  ----")
display(Markdown(f"<b>{str_input}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{str_output}</b>"))

```

D先生： “なんだ・・・。これは、前回にQwen-maxで質問した質問じゃないですか。”

QEU:FOUNDER ： “同じ質問で、どのような差異が出てくるのかが知りたくてね。それでは、結果をドン！！まずは、CODERが出力した設計思想を見てみましょう。コードと一緒に出てきます。”

**(使い方)**

まず、必要なライブラリをインストールしてください:
pip install numpy torch matplotlib

上記のコードをファイルに保存し（例：dqn_2048.py）、Pythonで実行してください:

python dqn_2048.py

- 学習過程で、5ステップごとにゲームオーバー時の状態が表示されます。
- 学習終了後、学習損失値と報酬の折れ線グラフが描画されます。

**(注意事項)**

- このプログラムは、強化学習モデルの性能を最適化するためのハイパーパラメータ調整がされていないため、モデルのパフォーマンスは調整が必要です。
- 学習は時間がかかるため、GPUを使用する場合は、コードを適宜修正してGPU上で動作させることが推奨されます。
- 学習結果のグラフは、損失値と報酬の生の変動値とともに、50エピソードの移動平均値も表示しています。これにより、学習の進捗状況をよりよく理解できます。


QEU:FOUNDER  ： “そして、肝心のコードがこれです。このプログラムが、前述の「python dqn_2048.py」になるんでしょうね。”

```python
# ---
import numpy as np
import random
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
from IPython.display import clear_output

# 2048ゲームの環境設定
GRID_SIZE = 4
ACTIONS = ['left', 'right', 'up', 'down']

# DQN モデルの定義
class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 2048ゲーム環境
class Game2048Env:
    def __init__(self):
        self.grid = None
        self.score = 0
        self.reset()

    def reset(self):
        self.grid = [[0] * 4 for _ in range(4)]
        self.add_new_tile()
        self.add_new_tile()
        self.score = 0
        return self.get_state()

    def add_new_tile(self):
        empty_cells = [(i, j) for i in range(4) for j in range(4) if self.grid[i][j] == 0]
        if empty_cells:
            i, j = random.choice(empty_cells)
            self.grid[i][j] = 2 if random.random() < 0.9 else 4

    def step(self, action):
        old_score = self.score
        moved = False

        if action == 0:  # Left
            new_grid, moved = self.move_left(self.grid)
        elif action == 1:  # Right
            new_grid, moved = self.move_right(self.grid)
        elif action == 2:  # Up
            new_grid, moved = self.move_up(self.grid)
        elif action == 3:  # Down
            new_grid, moved = self.move_down(self.grid)
        else:
            raise ValueError("Invalid action")

        if moved:
            self.grid = new_grid
            self.add_new_tile()

        reward = self.score - old_score
        done = self.is_game_over()
        return self.get_state(), reward, done, {}

    def move_left(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row)
            new_grid.append(new_row)
            if has_moved:
                moved = True
        return new_grid, moved

    def compress_and_merge(self, row):
        compressed = [num for num in row if num != 0]
        compressed += [0] * (4 - len(compressed))
        merged = []
        has_merged = False
        i = 0
        while i < len(compressed):
            if i < len(compressed)-1 and compressed[i] == compressed[i+1]:
                merged_val = compressed[i] * 2
                merged.append(merged_val)
                self.score += merged_val
                i += 2
                has_merged = True
            else:
                merged.append(compressed[i])
                i += 1
        new_compressed = merged + [0] * (4 - len(merged))
        has_moved = (new_compressed != row) or has_merged
        return new_compressed, has_moved

    def move_right(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_grid.append(new_row[::-1])
            if has_moved:
                moved = True
        return new_grid, moved

    def move_up(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row)
            new_transposed.append(new_row)
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def move_down(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_transposed.append(new_row[::-1])
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def is_game_over(self):
        for row in self.grid:
            if 0 in row:
                return False
        for i in range(4):
            for j in range(3):
                if self.grid[i][j] == self.grid[i][j+1]:
                    return False
        for j in range(4):
            for i in range(3):
                if self.grid[i][j] == self.grid[i+1][j]:
                    return False
        return True

    def get_state(self):
        state = []
        for row in self.grid:
            for num in row:
                state.append(0 if num == 0 else (num / 2048.0))
        return torch.FloatTensor(state).unsqueeze(0)

    def render(self):
        print(f"Score: {self.score}")
        for row in self.grid:
            print([num for num in row])
        print()

# DQNエージェントの定義
class DQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.98, epsilon=1.0, epsi-lon_decay=0.999, epsilon_min=0.01, memory_size=10000, batch_size=32):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.memory_size = memory_size
        self.batch_size = batch_size
        self.memory = deque(maxlen=memory_size)
        self.model = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        state = state.clone().detach()  # Correctly copy the tensor
        state = state.unsqueeze(0)  # Add a batch dimension
        q_values = self.model(state)
        return torch.argmax(q_values).item()

    # replay関数内の該当部分
    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        minibatch = random.sample(self.memory, self.batch_size)
        for state, action, reward, next_state, done in minibatch:
            # 修正版: clone().detach()を使用
            state = state.clone().detach().view(-1, self.state_size)
            next_state = next_state.clone().detach().view(-1, self.state_size)
            target = self.model(state).clone().detach()  # 重要: clone().detach()を追加
            target[0][action] = reward if done else reward + self.gamma * torch.max(self.model(next_state)).item()
            self.optimizer.zero_grad()
            loss = self.criterion(target, self.model(state))
            loss.backward()
            self.optimizer.step()
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        return loss.item()

# 学習プロセス
def train_dqn(episodes=2000):
    state_size = GRID_SIZE * GRID_SIZE
    action_size = len(ACTIONS)
    agent = DQNAgent(state_size, action_size)
    env = Game2048Env()
    losses = []
    rewards = []
    moving_avg_losses = []
    moving_avg_rewards = []
    moving_avg_window = 20

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            loss = agent.replay()
            
        if episode % 2 == 0:
            print(f"Episode {episode}: Total Reward = {total_reward}, Loss = {loss}")
            #print(env.grid)

        if loss is not None:
            losses.append(loss)
        rewards.append(total_reward)

        if episode % 6 == 0:
            env.render()

        # 显示信息
        if episode % 100 == 0:
            clear_output(wait=True)

        if len(losses) >= moving_avg_window:
            moving_avg_losses.append(np.mean(losses[-moving_avg_window:]))
            moving_avg_rewards.append(np.mean(rewards[-moving_avg_window:]))

    return losses, rewards, moving_avg_losses, moving_avg_rewards

# メイン処理
if __name__ == "__main__":
    losses, rewards, moving_avg_losses, moving_avg_rewards = train_dqn()

```

D先生： “こっ、これは・・・。**「DQN with experience replay」**じゃないですか・・・。Qwen-maxモデルを使った推論では、このような複雑なプログラムが出てこなかったです。このコードは、すぐに動きましたか？“

QEU:FOUNDER  ： “実は、ずいぶんと手間がかかりました。おそらく、もっとも大きな原因はプログラムの複雑さなのだろうね。あと、このモデルは推論を持っていないので、精度が高くないんでしょう。”

D先生： “このプログラムは動くんですか？ “

![imageBSR1-12-6](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-6.jpg) 

QEU:FOUNDER  ： “まあ、そこそこには動きます。自分で、直しちゃったから・・・。さて、前回のQwen-max(推論付き)と**Qwen-coder（推論なし、ただしコード生成に特化）**を見比べてみて、今、話題のこの件について思い出しました。”

![imageBSR1-12-7](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-7.jpg) 

D先生：“最近、C国の創造性が爆発しています。これを、やってみたんですか？”

[![MOVIE1](http://img.youtube.com/vi/D6jxT0E7tzU/0.jpg)](http://www.youtube.com/watch?v=D6jxT0E7tzU "11")

QEU:FOUNDER ： “現時点では、誰もが使えるわけではないです。それでも、事例のデモを見ることができます。どうやら、複数のLLMモデルを直列に並べているんじゃないか？そうしてみると、我々としても、以下のように「AGI for coders」を提案することができます。”

![imageBSR1-12-8](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-8.jpg) 

D先生： “「**中間情報**とは？」、「**微調整**とは？」・・・。“

QEU:FOUNDER  ： “それがわかると、小生でもMANUSもどきを作れるって・・・。”

D先生： “ここまでする必要はあるのかなあ・・・。CODERモデルにGRPOモデルによる推論機能を付ければいいだけではないでしょうか。“

QEU:FOUNDER  ： “それが、実はわからないんです。「I bought a gift for my girlfriend.」、もうひとつ「for i in range(10)」という2つの句があります。さて、ここでいう「for」という意味は同じであろうか？”

D先生： “もちろん、全然違う意味です。 “


QEU:FOUNDER ： “CODERにREASONINGを付加するのは、一見すると良い考えです。しかし、REASONINGを加えると、あまりにも自然言語（「I bought a gift for my girlfriend.」）の比重が多くなるのではないか。そうすれば、**CODERの能力が低下するのではないかという危惧がある**んです。”


D先生： “なるほどねえ・・・。で、次は何をします？”

QEU:FOUNDER： “もうちょっと、遊んでみたいと思っています。次回に続く。”


## ～ まとめ ～

### ・・・ 前回の続きです ・・・

QEU:FOUNDER ： “彼は、ディープステートと戦う「光の戦士」なのだから・・・。”

![imageBSR1-12-9](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-9.jpg) 

D先生：“へえ・・・、FOUNDERって「こんな人（笑）」だったんだ・・・。私には、彼は「ただのブレイカー」にしかみえませんでしたが・・・。”

![imageBSR1-12-10](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-10.jpg) 

QEU:FOUNDER ： “なんか、ずっと昔、E国で起こった**「クロムウエルの清教徒革命(Civil war)」**って、こういう感じじゃなかったのかと思います。彼は、自由主義の清教徒なんじゃないですか？ディープステートって、結局のところ象徴的な単語にしか過ぎない。”

- 官僚主義
- 新自由主義
- A国の覇権主義

D先生：“「3つの主義」のね・・・。でも、この主義って、その根底には搾取があります。でも、彼らが欲しがっている搾取って、ディープステートがないとできないの？”

QEU:FOUNDER ： “資本主義の理論的にはできるはずです。それを実証したい。だから、小生は、彼を清教徒だと思っています。”

![imageBSR1-12-11](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-11.jpg) 

QEU:FOUNDER ： “資本主義の原則に則れば、理論的には（搾取は）簡単にできるはずです。「光の戦士」は、それを実証したい。だから、小生は、彼を清教徒だと思っています。”

D先生：“「3つの主義」のね・・・。でも、この主義って、その根底には搾取があります。でも、**彼らが欲しがっている搾取って、ディープステートがないとできないの**？”

![imageBSR1-12-12](/2025-03-14-QEUR23_BS4RVW12/imageBSR1-12-12.jpg) 

QEU:FOUNDER ： “来るべき**「黄金時代」**には、それが可能になるのでしょう。”
