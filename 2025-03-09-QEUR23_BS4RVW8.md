---
title: QEUR23_BS4RVW8 - 小さなDeepSeek型(構造変換)モデルにORPOを使って微調整をしてみる(Unsloth)
date: 2025-03-09
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "DeepSeek"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_BS4RVW8 - 小さなDeepSeek型(構造変換)モデルにORPOを使って微調整をしてみる(Unsloth)

## ～ あ～あ、ちゃんとした強化学習(RL)にはA100でも足らないんだよなあ・・・ ～

### ・・・ さて、前回のつづきです ・・・

QEU:FOUNDER ： “次は、ORPOによる追加学習ですが、今回はウォーミングです。BONSAIディベート用のPreference情報なしでやってみます。”

![imageBSR1-9-1](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-1.jpg) 

D先生： “GRPO処理したモデルで、追加でORPO処理をするんですよね。これによって、ある偉人の名言の妥当性をLLMで検証することになります。”

![imageBSR1-9-2](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-2.jpg) 

D先生： “最近は、GRPOを可能にしたので、Unslothという軽量なLLMファインチューニングのツールも、だいぶ有名になってきなたあ・・・。”

[![MOVIE1](http://img.youtube.com/vi/juOh1afy-IE/0.jpg)](http://www.youtube.com/watch?v=juOh1afy-IE "This ONE TRICK Turns your LLM like DeepSeek R1💥 Train your own DeepLlama for Free! 💥")

QEU:FOUNDER ： “残念ながら、今回は、前回に我々が作成したGRPOモデル(Phi4)は使わないよ。その代替は、これ（↓）です・・・。”

![imageBSR1-9-3](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-3.jpg) 

D先生：“あれ？ Unsloth既製モデルを使うんですか？”

QEU:FOUNDER ： “あれから、いろいろやってみました・・・（遠い目）。結局、**素うどんモデルにGRPOをやったあとで、さらにORPOをやるのは大変**なんですよ。結局、一番楽なのはUnslothが準備した（GRPO済）モデルを使うのがベストですよ。。”


D先生：“へえ・・・、何があったんですか？”

QEU:FOUNDER  ： “ものすごく障害があります。少しだけ、後で話しますよ。さて、今回のORPOで使うデータセットは、コレ（↓）です！”

![imageBSR1-9-4](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-4.jpg) 

D先生： “J語の内容が多いようです。これは、14bの軽量モデルにはきついのか？ “

QEU:FOUNDER  ： “さらにREASONINGも載せているからね。それでは、プログラムを見てみましょう。”

```python
# ---
from unsloth import FastLanguageModel
import torch

max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# ---
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Qwen-14B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# ---
# We now add LoRA adapters so we only need to update 1 to 10% of all parameters!
# ---
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

# ---
# The data must be formatted with appropriate prompt template first.
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""
# ---
response_format = "<think>{}</think>{}"

# ---
EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN

def format_prompt(sample):
    str_instruction = sample["instruction"]  # Extract the instruction from the sample
    str_input = sample["input"]              # Extract the input from the sample
    # ORPOTrainer expects prompt/chosen/rejected keys
    sample["prompt"] = alpaca_prompt.format(str_instruction, str_input, "")  # Format the prompt using the template
    # ---
    accepted_output    = sample["output(CHOSEN)"]           # Extract the accepted response from the sample
    accepted_reasoning = sample["reasoning(CHOSEN)"]        # Extract the accepted response from the sample
    accepted_part  = response_format.format(accepted_reasoning, accepted_output)  # Format the prompt using the template
    # ---
    rejected_output    = sample["output(REJECTED)"]           # Extract the accepted response from the sample
    rejected_reasoning = sample["reasoning(REJECTED)"]        # Extract the accepted response from the sample
    rejected_part  = response_format.format(rejected_reasoning, rejected_output)  # Format the prompt using the template
    # ---
    #sample["accepted"] = accepted_output  # Append EOS token to the accepted response
    #sample["chosen"] = accepted_output + EOS_TOKEN  # Append EOS token to the accepted re-sponse
    #sample["rejected"] = rejected_output + EOS_TOKEN  # Append EOS token to the rejected re-sponse
    sample["accepted"] = accepted_part  # Append EOS token to the accepted response
    sample["chosen"] = accepted_part + EOS_TOKEN  # Append EOS token to the accepted response
    sample["rejected"] = rejected_part + EOS_TOKEN  # Append EOS token to the rejected response

    return sample  # Return the modified sample

pass

from datasets import load_dataset

#dataset = load_dataset("reciperesearch/dolphin-sft-v0.1-preference")["train"]
dataset = load_dataset("YxBxRyXJx/QADS_GRPO_250307")["train"]

# ---
# 次に、この関数は、dataset.map を使用してデータセット内の各サンプルに適用されます。dataset.map は、format_prompt 関数を通じて各サンプルを処理し、フォーマットされたデータセットを返します。
dataset = dataset.map(format_prompt,)
dataset = dataset.remove_columns(['question', 'output(CHOSEN)','reasoning(CHOSEN)', 'out-put(REJECTED)','reasoning(REJECTED)',])
dataset = dataset.remove_columns(['syslang', '__index_level_0__',])
dataset[0]
```

D先生： “REASONINGのデータは、THINKタグの中に含まれるわけですね。“


{'instruction': 'Find the square root of 156.',

 'input': '',

 'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nFind the square root of 156.\n\n### Input:\n\n\n### Response:\n',
 
 'accepted': '<think>I should break down the number 156 into its prime factors. I should look for two numbers whose product is 156 and whose sum can be easily determined. I should also consider the perfect squares closest to 156, which are 144 and 169, to get an estimate of the square root.</think>The square root of 156 is approximately 12.49. \n\nWe can break down 156 into its prime factors: 156 = 2^2 * 3 * 13. We can pair the factors to get: (2 * 13) and 3 * 12, which gives us two numbers whose product is 156: 26 and 6. The sum of these two numbers is 32, so an estimate for the square root is 16. Knowing that 144 is too small and 169 is too large, we can deduce that the square root is slightly more than 12. Using a calculator, we find the square root to be approximately 12.49.',
 
 'chosen': '<think>I should break down the number 156 into its prime factors. I should look for two numbers whose product is 156 and whose sum can be easily determined. I should also consider the perfect squares closest to 156, which are 144 and 169, to get an estimate of the square root.</think>The square root of 156 is approximately 12.49. \n\nWe can break down 156 into its prime factors: 156 = 2^2 * 3 * 13. We can pair the factors to get: (2 * 13) and 3 * 12, which gives us two numbers whose product is 156: 26 and 6. The sum of these two numbers is 32, so an estimate for the square root is 16. Knowing that 144 is too small and 169 is too large, we can deduce that the square root is slightly more than 12. Using a calculator, we find the square root to be approximately 12.49.<｜end▁of▁sentence｜>',
 
 'rejected': '<think>I should first recognize the type of mathematical operation required, in this case, finding the square root of a number. I should then recall the fundamental property of square roots, where the square root of a number is a value that, when multiplied by itself, gives the original num-ber. To provide a more accurate solution, I should consider using numerical methods or calculator functions specifically designed for square root calculations, taking into account the possibility of dec-imal values. Additionally, I should verify the answer by plugging the calculated value back into the original equation to ensure its correctness. By following this structured approach, the assistant can provide a reliable and precise solution to the problem.</think>The square root of 156 is 12.48664.<｜end▁of▁sentence｜>'}

QEU:FOUNDER ： “それでは、次のスニペットにいきましょう。いよいよRL学習に入ります。”

```python
# ---
# Enable reward modelling stats
from unsloth import PatchDPOTrainer
PatchDPOTrainer()
# ---
from trl import ORPOConfig, ORPOTrainer
#from unsloth import is_bfloat16_supported

orpo_trainer = ORPOTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = ORPOConfig(
        max_length = max_seq_length,
        max_prompt_length = max_seq_length//2,
        max_completion_length = max_seq_length//2,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        beta = 0.1,
        logging_steps = 1,
        optim = "adamw_8bit",
        lr_scheduler_type = "linear",
        max_steps = 30, # Change to num_train_epochs = 1 for full training runs
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        output_dir = "outputs",
        #report_to = "wandb", # Use this for WandB etc  , none
    ),
)


# ---
trainer_stats = orpo_trainer.train()

```

D先生： “うへえ・・・。たった14bのモデルでも、**A100のGPUメモリがギリギリ（96%）になる**んだ・・・。”

![imageBSR1-9-5](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-5.jpg) 

QEU:FOUNDER： “REASONINGを使うと**コンテクスト長さ**がネックなるからね。さらに、CHOSENとREJECTEDが共存します。ちなみに、DeepSeek処理としていないQwen14bで、同じデータセットでORPO処理をするとこうなります。”

![imageBSR1-9-6](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-6.jpg) 


D先生： “少しだけ軽いですね。やっぱり、GRPO済モデルではGPUに対して多めの負荷がかかるのでしょうね。肝心のRL学習はどうですか？“

![imageBSR1-9-7](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-7.jpg) 


QEU:FOUNDER  ： “注意・・・、いまのところは30epochしか学習していないからね。テストなのだから・・・。学習が安定してモデルが所定の効果を発揮するには、**最低でも200epochは必要**でしょう。ともあれ、30epoch段階での推論結果を見てみましょう。”

![imageBSR1-9-8](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-8.jpg) 

D先生： “なるほど。回答に「THINKタグ（推論）」が出てきますね。**GRPOモデルの機能継承はされています**ね。当たり前ですが・・・。“


QEU:FOUNDER ： “いや、意外と当たり前でもないよ。さっき、素うどんのQwen-14bでやってみたと言ってでしょ？そのときには、回答には推論（THINKタグ）が出てこなかったです。ちょっと、出力文の詳細の内容をみてみましょう。E語で、かつベクトル表示で恐縮だが・・・。”

**(Deepseek_Qwen14B)**

["<｜begin▁of▁sentence｜>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n

### Instruction:\n

I live in Manila. I will be going to Japan in early April to see the cherry blossoms. Please provide me with a recommended itinerary for 4 nights and 5 days, making full use of the Shinkansen. Please also provide a list of places to visit and activities to do each day.\n\n

### Input:\n\n\n

### Response:\n

Certainly! Below is a suggested 4-night/5-day itinerary for your trip to Japan in early April to en-joy the cherry blossoms, utilizing the Shinkansen (bullet train) for efficient travel between major cit-ies. Each day includes a mix of sightseeing, cultural experiences, and opportunities to appreciate the beautiful cherry blossoms.\n\n**Day 1: Tokyo**\n- **Morning:**\n  - Arrive in Tokyo (Narita or Haneda Airport).\n  - Transfer to your hotel and check in.\n  - Enjoy breakfast at the hotel.\n- **Afternoon:**\n  - Visit **Ueno Park** to see the cherry blossoms. Ueno Park is famous for its han-ami (cherry blossom viewing) spots and is close to Ueno Zoo and the surrounding temples and shrines.\n  - Visit **Ueno Zoo**, one of Tokyo's oldest zoos, and home to the famous giant pandas.\n  - Explore **Ueno Ameya-Yokocho**, a bustling open-air market where you can try local snacks and souvenirs.\n- **Evening:**\n  - Head to **Asakusa** to experience traditional Tokyo. Visit **Senso-ji Temple**, Tokyo's oldest and most famous Buddhist temple, and explore the surrounding area, in-cluding Nakamise Street, where you can shop for traditional goods and snacks.\n  - Enjoy dinner at a local restaurant in Asakusa, perhaps trying traditional Tokyo-style ramen or soba.\n\n**Day 2: To-kyo (Cherry Blossom Viewing)**\n- **Morning:**\n  - Visit **Chidorigafuchi** (Osaka Castle Park), known for its beautiful cherry blossoms and views of the castle.\n  - Alternatively, visit **Shinjuku Gyoen National Garden**, a large park with stunning cherry blossoms and a perfect spot for hana-mi.\n- **Afternoon:**\n  - Explore **Shinjuku**, a vibrant area with a mix of traditional and mod-ern Tokyo. Visit **Shinjuku Shrine**, a beautiful Shinto shrine surrounded by cherry trees, and **Golden Gai**, a cluster of tiny, unique bars and restaurants.\n  - Visit **Omoide Yokocho** (Memory Lane), a narrow shopping street in Shinjuku filled with shops selling retro goods and snacks.\n- **Evening:**\n  - Enjoy the evening in **Odaiba**, an artificial island known for its fu-turistic attractions. Take a short train ride from Shinjuku to Odaiba and explore **TeamLab Bor-derless**, an interactive digital art exhibition space.\n  - Alternatively, return to central Tokyo and enjoy the nightlife in areas like **Roppongi** or **Ginza**.\n\n**Day 3: Tokyo to Kyoto via Shinkansen**\n- **Morning:**\n  - Take the Shinkansen from Tokyo Station to **Kyoto Station** (about 2.5 hours).\n  - Upon arrival in Kyoto, check into your hotel and have lunch.\n- **Afternoon:**\n  - Visit **Kinkaku-ji** (Golden Pavilion), a stunning Zen temple covered in gold leaf. It's a must-see during cherry blossom season as the surrounding cherry trees add a magical touch.\n  - Explore **Arashiyama Bamboo Grove**, a beautiful bamboo forest with a historic feel. Walk along the bamboo paths and enjoy the serene atmosphere.\n- **Evening:**\n  - Head to **Gion**, Kyoto's historic geisha district. While you may not spot a geisha, you can still experience the traditional atmosphere and enjoy a traditional Kyoto-style dinner at a ryokan (traditional inn) or a kaiseki (multi-course) restaurant.\n\n**Day 4: Kyoto**\n- **Morning:**\n  - Visit **Fushimi Inari Taisha**, the most famous shrine in Kyoto. The iconic torii gates at the entrance are a symbol of Ja-pan, and the path leading up the mountain is lined with thousands of red torii, perfect for cherry blossom viewing.\n- **Afternoon:**\n  - Explore **Nijo Castle**, a historic samurai castle with beautiful cherry blossoms in spring. The castle's gardens and structures are a great way to learn about Japan's feudal history.\n  - Visit **Kyoto Imperial Palace**, the former residence of the Em-peror of Japan. The palace grounds are vast and include beautiful traditional Japanese gardens.\n- **Evening:**\n  - Enjoy a traditional tea ceremony at a local tea house, a great way to immerse your-self in Japanese culture.\n  - Alternatively, take a relaxing soak in a **rotenburo** (open-air hot spring bath) at a local onsen (hot spring resort).\n\n**Day 5: Kyoto to Hakone via Shinkansen**\n- **Morning:**\n  - Take the Shinkansen from Kyoto to **Hakone**, known for its hot springs and views of Mount Fuji. The journey takes about 2.5 hours.\n  - Upon arrival in Hakone, check into your hotel and have lunch.\n- **Afternoon:**\n  - Visit **Hakone Open-Air Museum**, a unique museum featuring modern and contemporary art displayed in an open-air setting. The museum's cherry trees will also be in bloom, adding to the beauty.\n  - Take a short boat cruise on **Lake Ashi** (Lake Ha-kone), where you can enjoy the stunning views of Mount Fuji and the surrounding cherry blos-soms.\n- **Evening:**\n  - Relax in one of Hakone's famous **rotenburo** (open-air hot springs), enjoying the natural onsen while taking in the serene views of the surrounding landscape.\n  - Enjoy a traditional Japanese dinner at a local restaurant, perhaps trying **yudofu** (boiled tofu) or other local specialties.\n\n**Day 6: Hakone to Tokyo via Shinkansen**\n- **Morning:**\n  - Take the Shinkansen from Hakone to **Shinagami** (Tokyo), transferring to your hotel or the airport.\n  - If time permits, visit **Odaiba**, an artificial island known for its futuristic attractions. Take a short train ride from Shinagami to Odaiba and explore **TeamLab Borderless**, an interactive digital art exhibition space.\n- **Evening:**\n  - Return to your hotel and prepare for your departure the next day.\n\n**Day 7: Departure**\n- **Morning:**\n  - Check out of your hotel and transfer to the air-port for your flight back to Manila.\n  - If time allows, do some last-minute shopping or sightseeing in the area around the airport.\n\n**Additional Tips:**\n- **Cherry Blossom Season:** Cherry blossoms in Tokyo typically bloom from late March to early April, with peak bloom around mid-April. Be sure to check the weather and bloom forecast before your trip.\n- **Reservations:** Some of the popular attractions and restaurants may require advance reservations, especially during peak season. Plan accordingly to avoid disappointment.\n- **Transportation:** Japan's Shinkansen net-work is efficient and convenient. Purchase a Japan Rail Pass if you plan to use the Shinkansen multi-ple times, as it can save you money.\n- **Local Etiquette:** Remember to respect local customs and etiquette, such as removing your shoes in traditional spaces like shrines, temples, and some restau-rants.\n\nThis itinerary offers a mix of cultural experiences, nature, and modern attractions, all while making the most of the Shinkansen for efficient travel between cities. Enjoy your trip to Japan and the beautiful cherry blossoms!\n</think>

\n\nCertainly! Below is a suggested 4-night/5-day itinerary for your trip to Japan in early April to enjoy the cherry blossoms, utilizing the Shinkansen (bullet train) for efficient travel between major cities. Each day includes a mix of sightseeing, cultural experiences, and opportunities to appreciate the beautiful cherry blossoms.\n\n**Day 1: Tokyo**\n- **Morning:**\n  - Arrive in Tokyo (Narita or Haneda Airport).\n  - Transfer to your hotel and check in.\n  - Enjoy breakfast at the hotel.\n- **Afternoon:**\n  - Visit **Ueno Park** to see the cherry blossoms. Ueno Park is famous for its han-ami (cherry blossom viewing) spots and is close to Ueno Zoo and the surrounding temples and shrines.\n  - Visit **Ueno Zoo**, one of Tokyo's oldest zoos, and home to the famous giant pandas.\n  - Explore **Ueno Ameya-Yokocho**, a bustling open-air market where you can try local snacks and souvenirs.\n- **Evening:**\n  - Head to **Asakusa** to experience traditional Tokyo. Visit **Senso-ji Temple**, Tokyo's oldest and most famous Buddhist temple, and explore the surrounding area, in-cluding Nakamise Street, where you can shop for traditional goods and snacks.\n  - Enjoy dinner at a local restaurant in Asakusa, perhaps trying traditional Tokyo-style ramen or soba.\n\n**Day 2: To-kyo (Cherry Blossom Viewing)**\n- **Morning:**\n  - Visit **Chidorigafuchi** (Osaka Castle Park), known for its beautiful cherry blossoms and views of the castle.\n  - Alternatively, visit **Shinjuku Gyoen National Garden**, a large park with stunning cherry blossoms and a perfect spot for hana-mi.\n- **Afternoon:**\n  - Explore **Shinjuku**, a vibrant area with a mix of traditional and mod-ern Tokyo. Visit **Shinjuku Shrine**, a beautiful Shinto shrine surrounded by cherry trees, and **Golden Gai**, a cluster of tiny, unique bars and restaurants.\n  - Visit **Omoide Yokocho** (Memory Lane), a narrow shopping street in Shinjuku filled with shops selling retro goods and snacks.\n- **Evening:**\n  - Enjoy the evening in **Odaiba**, an artificial island known for its fu-turistic attractions."]

D先生： “見にくいです・・・。”

QEU:FOUNDER ： “そのまま晒すと、THINKタグの間の表示が見えなくなるんです。まあ、相変わらずREASONINGとRESPONSEの内容が重複しています。”


D先生： “あれ？ある種の推論において、REASONINGとRESPONSEの内容が重複する不具合を是正するのが目的では？“


QEU:FOUNDER ： “もう一度言いますよ。たった「30epoch」・・・（笑）。ただし、どこまで悪い癖が治るのかはわかりません。”

D先生： “GRPOの後でORPOをやったのが、一応の成果ですね・・・（笑）。 “

QEU:FOUNDER ： “そうそう・・・（笑）。あのね・・・。たかがFS(Feasibility Study)のためのテストで、300epochも学習できないでしょうに・・・。”


D先生： “次は、いよいよBONSAIの再起動ですね。**「EVERYBODY IS CREATIVE CLASSとは何か？」**が見えてきました。”

![imageBSR1-9-9](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-9.jpg) 

QEU:FOUNDER ： “BONSAIの再起動はやるが、AIの使い方はどうするのかは「ペンディング」です。”

D先生： “どうして？結構、いい出来なのに・・・。”

QEU:FOUNDER ： “その理由の詳細は、次の「まとめ編」で・・・。”


## ～ まとめ ～


### ・・・ 結構、手間がかかるORPOくんでした ・・・


D先生：“ここでは、すでに述べた「本編のタネアカシ」をするという趣向でしたね。ホントに、ずいぶんに手間がかかるプロジェクトです。やっぱり、ORPO用のデータセットの準備は大変でしたよね？”

QEU:FOUNDER ： “同じ質問に対して、2つの答えを準備し、かつ「（良し悪しの）差をつける」のは、いかに大変なことか・・・。”

D先生：“だったら、この(DeepSeekQwen)モデルを有効に使えればいいでしょうに・・・。”

![imageBSR1-9-10](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-10.jpg) 

QEU:FOUNDER ： “14bのモデルでは、言語能力が低いでしょ？それで、より新しく大きめのLLMモデルをやってみましょう。”

![imageBSR1-9-11](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-11.jpg) 

C部長： “うわあ・・・。J語の能力がツヨツヨ・・・。”

D先生： “これは期待が持てます。GRPOの結果は・・・？”

![imageBSR1-9-12](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-12.jpg) 

QEU:FOUNDER ： “GRPO学習が出来ました。それだけでもすごいでしょ？”

D先生：“そうか・・・。そういう見方もできますよね。ORPOでは14bのモデルの学習が最大でした。GRPOは、まだ余裕があるんですね。それにしてもREWARDの値の高さに驚きました。はやい段階のepochから、プラス値になりますよね。”

QEU:FOUNDER ： “このモデルはJ語を理解できるんですよ。”

C部長： “そういうことか・・・。逆に言うと、14bモデルには、あまり期待できないんだ。このモデルをORPCすると、どれぐらいのGPUメモリが必要？”

QEU:FOUNDER ： “60GBぐらいじゃないかなあ・・・。今、C部長がもった問題意識をもとに、この表（↓）をもう一度見てみて・・・。”

**（注意：再掲です）**

![imageBSR1-9-13](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-13.jpg) 

C部長： “このラインナップでは、ホントにきついなあ・・・。もう、J語は終わりなのか？”

D先生： “そもそもJ語だけモデルの開発のコストが、メリットに見合うのか・・・。”

![imageBSR1-9-14](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-14.jpg) 

QEU:FOUNDER ： “特定言語重視のOSS-LLMモデルを作るのはキツイ。むしろ、**マルチ言語指向のLLMモデルに希望がある**のかなあ。”

D先生：“それでも、24bでは、きついでしょう。”

![imageBSR1-9-15](/2025-03-09-QEUR23_BS4RVW8/imageBSR1-9-15.jpg) 

QEU:FOUNDER ： “Cohereなんかは、multi-lingualを指向しています。14bとなると、少し古いモデルになるが、このモデル（↑）の進化版がGRPO化されているとうれしい。”

D先生：“Cohere のモデルだと、かなり良いパフォーマンスを出すでしょうね。”

[![MOVIE2](http://img.youtube.com/vi/BCHa4Dcl3OI/0.jpg)](http://www.youtube.com/watch?v=BCHa4Dcl3OI "【岸田文雄前総理が語る政治とAI】”被害当事者”として語るディープフェイクの問題点｜広島AIプロセス立ち上げの背景｜中国DeepSeekはどう評価している？｜国産AIをどうするか")

QEU:FOUNDER ： “みなさん、大きなLLMの話ばかりしているが、**すでに時代は「RLで武装した小さなLLM」に移っている**んですよね。皆が好きにファインチューニングをすれば、コストはかかりません。”

C部長： “**マルチ言語LLMがDeepSeek化されると、世の中が変わりますかね？**”

QEU:FOUNDER ： “モデルに要求するのは**「地アタマの良さ**」だけ。モデルの知識の薄さは、RAG技術との併用で、かなり解決されます。”
