---
title: QEUR23_BS4RVW4 ： Qwen-35B(DeepSeek_Distil)を使ってREASONINGを試す(Unsloth-SFT)
date: 2025-02-06
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_BS4RVW4 ： Qwen-35B(DeepSeek_Distil)を使ってREASONINGを試す(Unsloth-SFT)

## ～ いま、はやりの・・・ ～

### ・・・ 前回のつづきです ・・・

D先生（設定年齢65歳）： “今回の実験で分かったことは、Qwen-32b-LLMの出力はREASONINGを通じて制御しているということです。レスポンスもかなり速いです。この点では、Phi-3-midium(14b)でのテストとは、かなり様子が変わってきました。”

![imageBSR1-5-1](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-1.jpg)

QEU:FOUNDER （設定年齢65歳）： “そうなの？でも、便利になってきたでしょう？**モデルが(Phi4の)14bから32bになると、世界が変わってきます**。”

D先生（設定年齢65歳）： “さらに面白くなりました！！お次は？”

QEU:FOUNDER ： “このモデル（↓）をやってみましょう！**Unsloth様の努力に大感謝**です！！”

![imageBSR1-5-2](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-2.jpg)

QEU:FOUNDER ： “すごいでしょう？とうとう**「蒸留されたオープン・モデルが使える時代」**になったんです！！”

D先生： “蒸留って、むりやりモデルに情報を別に移植することでしょう？モデルの大きさが全然ちがうじゃないですか？”

QEU:FOUNDER ： “まあね。DeepSeekのモデルは、おそらく数千億パラメタ、対する今回のQwenモデルは320億です・・・（笑）。とてもじゃないが、全部の情報が入るわけがない。じゃあ、これから簡単にプログラムに入ります。コードは以前とほとんど同じなので、無視してかまいません。”

```python
# ---
import torch
from unsloth import FastLanguageModel

max_seq_length = 2048*3 
dtype = None 
load_in_4bit = True 

# ---
# Loading the model and tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    #model_name = "unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit",
    model_name = "unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit", 
    #model_name = "YxBxRyXJx/QADSR1_test_250204_simple", 
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    token = hf_token, 
)

# ---
# Model inference before fine-tuning
prompt_style = """Below is an 'Instruction' that describes a question, paired with an 'Input' that provides further context. 
Write a response that appropriately completes the request. 
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
{}

### Input:
{}

### Response:
<think>{}"""

# ---
FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!

# ---
# Inference
from IPython.display import Markdown, display

# ---
def create_response_simple(model, str_instruction, str_input, str_reasoning):

    inputs = tokenizer([prompt_style.format(str_instruction, str_input, str_reasoning)], re-turn_tensors="pt").to("cuda")

    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=1024*4,
        use_cache=True,
    )
    response = tokenizer.batch_decode(outputs)
    #print(response[0].split("### Response:")[1])
    decoded_outputs = response[0].split("### Response:")[1]
    out_instruction = f"{str_instruction}\n {str_input}"

    return out_instruction, decoded_outputs

# ---
# 推論し、出力する(Japanese)
str_instruction = "私はマニラに住んでいます。4月の初旬に桜を見るために日本に行きます。4泊5日で、新幹線をフル活用した、おすすめの旅行プランを設定してください。毎日の訪問場所と活動内容をリストにしてください。"
str_input = ""
str_reasoning = ""
# ---
out_instruction, out_response = create_response_simple(model, str_instruction, str_input, str_reasoning)
print("----  QUESTION  ----")
display(Markdown(f"<b>{out_instruction}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{out_response}</b>"))

```

D先生： “ここでのトライアルは、「生のモデルがどのような出力を出すのか」という点に注目です。 “

QEU:FOUNDER ： “今回は、**「J語」、「E語」、「C語」で比較**していきます。今回は、言語の問題が非常に大事になります。”

**（CASE1）**

![imageBSR1-5-3](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-3.jpg)

**(CASE2A)**

![imageBSR1-5-4](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-4.jpg)

**(CASE2B)**

![imageBSR1-5-5](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-5.jpg)

**(CASE3A)**

![imageBSR1-5-6](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-6.jpg)

**(CASE3B)**

![imageBSR1-5-7](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-7.jpg)

D先生： “え～っ！こういう蒸留方法を取ったんですか！？これじゃあ、**REASONINGとRESPONSEには区切りがない**です。それにしても、J語とEC語の間に実力差がありすぎます。今まで使っていたQwen-32bモデルには、こんなことはなかったのに・・・。 “

QEU:FOUNDER ： “詳細の蒸留の設計思想は分からないが、少なくとも結果としてはモデルのJ語の処理能力は大いに低下したようですね。それでは、コードのつづきに行きます。ただし、トレーニングの過程は省略します。ドン！”

```python
# ---
# Loading and processing the dataset
train_prompt_style = """Below is an 'Instruction' that describes a question, paired with an 'Input' that provides further context. 
Write a response that appropriately completes the request. 
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
{}

### Input:
{}

### Response:
<think>
{}
</think>
{}"""
EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN

# ---
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    cots = examples["reasoning"]
    outputs = examples["output"]
    texts = []
    for str_instruction, str_input, str_cot, str_output in zip(instructions, inputs, cots, outputs):
        text = train_prompt_style.format(str_instruction, str_input, str_cot, str_output) + EOS_TOKEN
        texts.append(text)
    return {
        "text": texts,
    }

# ---
from datasets import load_dataset

org_dataset = load_dataset("YxBxRyXJx/QADSR1_TEST_250205", split = "train")
# syslang : Japanese / English / Chinese
# sysflag : train / test
train_dataset = org_dataset.filter(lambda example: example['sysflag'] == 'train')
# test_datasetはあとで使います
test_dataset = org_dataset.filter(lambda example: example['sysflag'] == 'test')
#train_dataset
dataset = train_dataset.map(formatting_prompts_func, batched = True,)
#dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT","en", split = "train[0:500]",trust_remote_code=True)
dataset = dataset.map(formatting_prompts_func, batched = True,)
dataset["text"][0]

# 省略

# ---
# Inference
from IPython.display import Markdown, display

# ---
def create_response_split(model, str_instruction, str_input, str_reasoning):

    inputs = tokenizer([prompt_style.format(str_instruction, str_input, str_reasoning)], re-turn_tensors="pt").to("cuda")

    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=1024*4,
        use_cache=True,
    )
    response = tokenizer.batch_decode(outputs)
    #print(response[0].split("### Response:")[1])
    decoded_outputs = response[0].split("### Response:")[1]
    out_instruction = f"{str_instruction}\n {str_input}"

    # <think>タグの内容を抽出
    parts = decoded_outputs.split('<think>')
    #print(parts)

    # REASONINGの内容を抽出
    if '</think>' in parts[1]:
        out_reasoning = parts[1].split('</think>')[0].strip()

        # </think>タグ以降の内容を抽出（タグを除く）
        out_response = parts[1].split('</think>')[1].strip()
        out_response = out_response.replace("<｜end▁of▁sentence｜>","")
    else:
        out_reasoning = "ERROR"
        out_response = decoded_outputs

    return out_instruction, decoded_outputs, out_reasoning, out_response

# ---
# INFERENCE
FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!

# ---
# 推論し、出力する
str_instruction = "地図を見ると、ロシアの面積がアフリカ大陸と同じ大きさに見えるがなぜなのか？本当は、大きさはどれほど違うのか？"
str_input = ""
str_reasoning = ""
# ---
out_instruction, decoded_outputs, out_reasoning, out_response = create_response_split(model, str_instruction, str_input, str_reasoning)
print("----  QUESTION  ----")
display(Markdown(f"<b>{out_instruction}</b>"))
print("----  REASONING  ----")
display(Markdown(f"<b>{out_reasoning}</b>"))
print("----  RESPONSE  ----")
display(Markdown(f"<b>{out_response}</b>"))

```

QEU:FOUNDER ： “ここからが、**「ファインチューニング後」の推論**になります。”

**(Case1)**

![imageBSR1-5-8](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-8.jpg)

**(Case2)**

**----  QUESTION  ----**

When I look at a map, Russia appears to be the same size as the African continent. Why? And how much bigger are they, in reality?

**----  REASONING  ----**

You should first recognize that maps often distort the actual size of countries due to projection meth-ods. Then, You should explain how the Mercator projection inflates the size of countries near the poles, such as Russia, while minimizing the size of countries near the equator, such as Africa. Next, You should provide the approximate land areas of Russia and Africa to highlight the significant dif-ference in their actual sizes. Finally, You should conclude by emphasizing that while maps can be use-ful for navigation, they often misrepresent the true scale of countries.

まず、地図は投影法によって国の実際の大きさを歪めてしまうことが多いことを認識する必要があります。次に、メルカトル図法ではロシアなどの極地に近い国の大きさが誇張され、アフリカなどの赤道に近い国の大きさが小さくなる理由を説明する必要があります。次に、ロシアとアフリカのおおよその陸地面積を示して、実際の大きさの大きな違いを強調する必要があります。最後に、地図はナビゲーションには便利ですが、国の実際の規模を誤って表すことが多いことを強調して結論を出します。


**----  RESPONSE  ----**

When you look at a map, Russia appears to be the same size as the African continent because of the way maps are projected. Maps are two-dimensional representations of a three-dimensional Earth, and the process of projecting the Earth's surface onto a flat map distorts the size of countries. Russia appears to be the same size as Africa because it is near the top of the map, and the projection used to create the map inflates the size of countries near the poles. In reality, Africa is much larger than Rus-sia. Africa is approximately 11,730,000 square miles (30,400,000 square kilometers) in area, while Russia is approximately 6,601,668 square miles (17,098,242 square kilometers) in area.

地図を見ると、地図の投影方法により、ロシアはアフリカ大陸と同じ大きさに見えます。地図は 3 次元の地球を 2 次元で表現したもので、地球の表面を平面の地図に投影するプロセスにより、国の大きさが歪んでしまいます。ロシアがアフリカと同じ大きさに見えるのは、地図の上部近くにあり、地図を作成するために使用された投影により、極地に近い国の大きさが誇張されているからです。実際には、アフリカはロシアよりもはるかに大きいです。アフリカの面積は約 11,730,000 平方マイル (30,400,000 平方キロメートル) で、ロシアの面積は約 6,601,668 平方マイル (17,098,242 平方キロメートル) です。

**(Case3)**

**----  QUESTION  ----**

從地圖上看，俄羅斯的面積似乎和非洲大陸一樣大。真的嗎，尺寸差異有多大？

**----  REASONING  ----**

您應該首先將俄羅斯的面積與非洲的面積進行比較。然後，您應該考慮地形和地理位置的差異。您應該還應該考慮人口密度和資源分配等因素。這樣，您可以提供一個更全面的比較，而不仅仅是比较面积。

まずロシアの大きさとアフリカの大きさを比較する必要があります。次に、地形や地理の違いを考慮する必要があります。人口密度や資源配分などの要素も考慮する必要があります。この方法により、単に面積を比較するのではなく、より包括的な比較を提供できます。

**----  RESPONSE  ----**

是的，俄羅斯的面積與非洲大陸的面積相近。俄羅斯的面積為17,098,242平方公里，而非洲大陸的面積為30,370,000平方公里。因此，俄羅斯的面積比非洲大陸小約48%。

はい、ロシアはアフリカ大陸とほぼ同じ大きさです。ロシアの面積は17,098,242平方キロメートルですが、アフリカ大陸の面積は30,370,000平方キロメートルです。その結果、ロシアはアフリカ大陸よりも約 48% 小さくなりました。

D先生： “この質問は、E語の勝ちです。一方のJ語は、またもや惨敗です。C語は悪くはない。・・・というか、私は、この感じの返答の方が好きです。“

QEU:FOUNDER ： “さらに、次にいきましょう。”

**(CASE1)**

![imageBSR1-5-9](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-9.jpg)

D先生： “あ～あ・・・。やっぱりJ語は全然だめだわ。 “

**(CASE2)**

**----  QUESTION  ----**

I live in Bangkok. I will be going to Kyushu, Japan at least once at the end of March to enjoy the world fa-mous hot springs. Please set up a recommended itinerary for 6 nights and 7 days, departing and arriving at Fukuoka International Airport. Please list the places to visit and activities to do each day.

**----  REASONING  ----**

You should start by identifying the key factors that contribute to a successful itinerary, such as the travel-er's interests, the duration of the trip, and the transportation logistics. Then, You should break down the trip into smaller components, such as arrival and departure, transportation, and activities, to ensure a log-ical and comprehensive plan. You should also consider the cultural and historical significance of each des-tination, as well as the practical aspects of travel, such as accommodation and transportation, to create a well-rounded itinerary.

まず、旅行者の興味、旅行期間、交通手段など、旅程を成功させるのに重要な要素を特定する必要があります。次に、到着と出発、交通手段、アクティビティなど、旅行をより細かい要素に細分化して、論理的で包括的な計画を立てます。また、各目的地の文化的および歴史的重要性、および宿泊や交通手段などの旅行の実際的な側面を考慮して、バランスの取れた旅程を作成する必要があります。

**----  RESPONSE  ----**

Day 1: Arrival in Fukuoka

- Arrive at Fukuoka International Airport
- Check in at a hotel in Fukuoka
- Explore the city and visit Fukuoka Castle, Hakozaki Shrine, and the Nakasu district
- Have a traditional Japanese dinner

Day 2: Travel to Beppu

- Take a train to Beppu
- Check in at a ryokan (Japanese inn)
- Visit the Beppu Hot Springs
- Explore the Beppu Onsen area
- Have a traditional Japanese dinner

Day 3: Travel to Oita

- Take a train to Oita
- Check in at a ryokan
- Visit the Oita Hot Springs
- Explore the Oita Onsen area
- Have a traditional Japanese dinner

Day 4: Travel to Kagoshima

- Take a train to Kagoshima
- Check in at a ryokan
- Visit the Kagoshima Hot Springs
- Explore the Kagoshima Onsen area
- Have a traditional Japanese dinner

Day 5: Travel to Takayama

- Take a train to Takayama
- Check in at a ryokan
- Visit the Takayama Hot Springs
- Explore the Takayama Onsen area
- Have a traditional Japanese dinner

Day 6: Travel to Hakone

- Take a train to Hakone
- Check in at a ryokan
- Visit the Hakone Hot Springs
- Explore the Hakone Onsen area
- Have a traditional Japanese dinner

Day 7: Return to Fukuoka

- Take a train to Fukuoka
- Check in at a hotel
- Explore the city and visit Fukuoka Castle, Hakozaki Shrine, and the Nakasu district
- Have a traditional Japanese dinner
- Depart from Fukuoka International Airport

**(CASE3)**

**----  QUESTION  ----**

我住在曼谷。三月底，我要去日本九州享受至少一次世界聞名的溫泉。制定從福岡國際機場出發、抵達的6晚7天的建議旅遊計畫。列出您將要參觀的地方和每天要做的活動。

**----  REASONING  ----**

您應該考慮多個因素，包括旅遊目的地的天氣、旅遊目的地的旅遊熱點、旅遊目的地的旅遊活動以及旅遊目的地的旅遊住宿。您應該考慮旅遊目的地的天氣，以確保旅遊行程不會受到不利天氣條件的影響。您應該考慮旅遊目的地的旅遊熱點，以確保旅遊行程豐富有趣。您應該考慮旅遊目的地的旅遊活動，以確保旅遊行程充滿活力和冒險。您應該考慮旅遊目的地的旅遊住宿，以確保旅遊行程舒適和便利。

目的地の天候、目的地の観光スポット、目的地の観光アクティビティ、目的地の観光宿泊施設など、いくつかの要素を考慮する必要があります。ツアーが悪天候の影響を受けないように、旅行先の天候を考慮する必要があります。旅行が面白く充実したものになるように、目的地の観光スポットを考慮する必要があります。ツアーの旅程がダイナミックで冒険的なものになるように、目的地での観光アクティビティを考慮する必要があります。旅行の旅程が快適で便利になるように、目的地での観光宿泊施設を検討する必要があります。

**----  RESPONSE  ----**

Day 1: Arrival in Fukuoka

- Arrive at Fukuoka International Airport
- Check into a hotel or ryokan (Japanese inn)
- Explore the city center, including Fukuoka Castle and Nakasu district
- Enjoy local cuisine at a izakaya (Japanese pub)

Day 2: Fukuoka

- Visit the Fukuoka Art Museum
- Explore the Fukuoka Botanical Garden
- Take a day trip to the nearby city of Kitakyushu
- Enjoy a traditional Japanese tea ceremony

Day 3: Beppu

- Take a train to Beppu
- Visit the Beppu Onsen (hot springs)
- Explore the city's traditional hot spring towns
- Enjoy a relaxing soak in the hot springs

Day 4: Oita

- Take a train to Oita
- Visit the Oita Prefectural Art Museum
- Explore the city's traditional hot spring towns
- Enjoy a traditional Japanese tea ceremony

Day 5: Yufu

- Take a train to Yufu
- Visit the Yufu Onsen (hot springs)
- Explore the city's traditional hot spring towns
- Enjoy a relaxing soak in the hot springs

Day 6: Kagoshima

- Take a train to Kagoshima
- Visit the Kagoshima Prefectural Art Museum
- Explore the city's traditional hot spring towns
- Enjoy a traditional Japanese tea ceremony

Day 7: Return to Fukuoka

- Take a train back to Fukuoka
- Check out of the hotel or ryokan
- Return to Fukuoka International Airport

QEU:FOUNDER ： “DeepSeek(DS)を使っている人であればわかりますが、この出力フォーマットはDSのモノです。REAPONSEについては、ケース別の差異が明らかなので、今回は解説をしません。”

D先生： “これをみると、この蒸留モデルではE語とC語がおススメです。ただし、少しE語が上になるのかな？ “

QEU:FOUNDER ： “じゃあ、次は**「論理題」**をやってみましょう。”

**(Case1)**

![imageBSR1-5-10](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-10.jpg)

**(Case2)**

**----  QUESTION  ----**

亨利有 3 個蘋果，雅克有 4 個梨，吉爾有 5 本書。如果傑克吃了五個梨，那麼他總共有多少顆水果？

**----  REASONING  ----**

You should identify the relevant information in the problem, which is the number of apples Henry has, the number of pears Jack has, and the number of books Jill has. You should recognize that the problem is asking for the total number of fruits that Jack has after eating five pears. You should cal-culate the total number of pears Jack has after eating five, which is 4 - 5 = -1, but since you can't have negative fruits, you should recognize that Jack cannot eat more pears than he has. You should then conclude that Jack has 4 pears and 5 books, but since books are not fruits, the total number of fruits Jack has is 4 pears.

問題の中で、ヘンリーが持っているリンゴの数、ジャックが持っている梨の数、ジルが持っている本の数が関連している情報を特定する必要があります。問題は、ジャックが梨を 5 個食べた後に持っている果物の合計数を尋ねていることを認識する必要があります。ジャックが 5 個食べた後に持っている梨の合計数を計算すると、4 - 5 = -1 になりますが、負の果物はあり得ないため、ジャックは持っている数より多くの梨を食べることはできないことを認識する必要があります。次に、ジャックは梨を 4 個と本を 5 冊持っていると結論付けますが、本は果物ではないため、ジャックが持っている果物の合計数は梨を 4 個です。


**----  RESPONSE  ----**

傑克有4個梨，所以總共有4個水果。

ジャックは梨を 4 個持っているので、果物は全部で 4 個あります。

D先生： “答えはダメだが、REASONINGはイイセンをいっています。これもJ語は、論外です。FOUNDER・・・。この蒸留モデルを、今後も使うんですか？ “

QEU:FOUNDER ： “率直に言って使えないよね。理由・・・？J語は、どうでもいいです。いままでもE語で議論していたので・・・。このモデルの問題は、DSモデルのフォーマットの影響が強く出過ぎています。これを修正するのは、大変になると思いますよ。”


D先生： “残念ですが、それが妥当でしょうね。でも、せっかくだから、ここで得た情報をFINETUNEのために使いたいですね。 “


## ～ まとめ ～

C部長： “とうとう、こんな時代（↓）が来たんですね。まさか、**「CIAの3文字」がSNSで踊り狂う時代**になるとは思わなかったです”

![imageBSR1-5-11](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-11.jpg)

QEU:FOUNDER ： “ほんとに腰が抜けました。”

![imageBSR1-5-12](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-12.jpg)

D先生： “CIAの情報って、一部の人以外はあまり知られていないと思いますが、LLMが学習した膨大な言語データにはしっかりあります。ちなみに、以下の結果（↓）はハルシネーションに注意です。”

![imageBSR1-5-13](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-13.jpg)

C部長： “もう一度念をおします。**以上の結果（↑）はハルシネーションに注意**です（笑）。ある意味、こういう情報が踊り狂う時代なのでしょうね？”

QEU:FOUNDER ： “これだけの巨大かつ高品質のLLMが誰にも手に入る時代では、情報分析は簡単にできます。だから、はっきりいって、**情報（インテリジェンス）機関って、そこの職員は3分の1ぐらいになってもいい**と思います。”

D先生： “ホントに…。米国のT大統領は、いいところを見ています。”

![imageBSR1-5-14](/2025-02-06-QEUR23_BS4RVW4/imageBSR1-5-14.jpg)

QEU:FOUNDER ： “ネットは**「CIAザマミロ～♪」**の一色ですが、小生は、いよいよ「ＭＡＧＡ」の一歩が始まったと思っています。このステップは侮れません。”

