---
title: QEUR23_BS4RVW13 - 閑話休題～Solve it with code準備編(Qwen-max_ppo)
date: 2025-03-15
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "DeepSeek"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_BS4RVW13 - 閑話休題～Solve it with code準備編(Qwen-max_ppo)

## ～ 結局、Qwen-coderしかないのか・・・ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “CODERにREASONINGを付加するのは、一見すると良い考えです。しかし、REASONINGを加えると、あまりにも自然言語（「I bought a gift for my girlfriend.」）の比重が多くなるのではないか。そうすれば、CODERの能力が低下するのではないかという危惧があるんです。”

![imageBSR1-13-1](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-1.jpg) 

D先生： “なるほどねえ・・・。で、次は何をします？”

QEU:FOUNDER： “もうちょっと、遊んでみたいと思っています。Qwen-coderを使って、DQNのプログラムをPPOに変えてしまおうと思います。それでは、Unslothのinferenceコマンドをドン！！”

```python
# ---
str_instruction = """以下の2048ゲームを環境として使ったDQN強化学習プログラムをPPOを使った強化学習に改造してください。
もし、GPUを使って学習を加速することができるのであるならば、GPU用に改造してください。"""
str_input = """import numpy as np
import random
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
from IPython.display import clear_output

# 2048ゲームの環境設定
GRID_SIZE = 4
ACTIONS = ['left', 'right', 'up', 'down']

# DQN モデルの定義
class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 2048ゲーム環境
class Game2048Env:
    def __init__(self):
        self.grid = None
        self.score = 0
        self.reset()

    def reset(self):
        self.grid = [[0] * 4 for _ in range(4)]
        self.add_new_tile()
        self.add_new_tile()
        self.score = 0
        return self.get_state()

    def add_new_tile(self):
        empty_cells = [(i, j) for i in range(4) for j in range(4) if self.grid[i][j] == 0]
        if empty_cells:
            i, j = random.choice(empty_cells)
            self.grid[i][j] = 2 if random.random() < 0.9 else 4

    def step(self, action):
        old_score = self.score
        moved = False

        if action == 0:  # Left
            new_grid, moved = self.move_left(self.grid)
        elif action == 1:  # Right
            new_grid, moved = self.move_right(self.grid)
        elif action == 2:  # Up
            new_grid, moved = self.move_up(self.grid)
        elif action == 3:  # Down
            new_grid, moved = self.move_down(self.grid)
        else:
            raise ValueError("Invalid action")

        if moved:
            self.grid = new_grid
            self.add_new_tile()

        reward = self.score - old_score
        done = self.is_game_over()
        return self.get_state(), reward, done, {}

    def move_left(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row)
            new_grid.append(new_row)
            if has_moved:
                moved = True
        return new_grid, moved

    def compress_and_merge(self, row):
        compressed = [num for num in row if num != 0]
        compressed += [0] * (4 - len(compressed))
        merged = []
        has_merged = False
        i = 0
        while i < len(compressed):
            if i < len(compressed)-1 and compressed[i] == compressed[i+1]:
                merged_val = compressed[i] * 2
                merged.append(merged_val)
                self.score += merged_val
                i += 2
                has_merged = True
            else:
                merged.append(compressed[i])
                i += 1
        new_compressed = merged + [0] * (4 - len(merged))
        has_moved = (new_compressed != row) or has_merged
        return new_compressed, has_moved

    def move_right(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_grid.append(new_row[::-1])
            if has_moved:
                moved = True
        return new_grid, moved

    def move_up(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row)
            new_transposed.append(new_row)
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def move_down(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_transposed.append(new_row[::-1])
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def is_game_over(self):
        for row in self.grid:
            if 0 in row:
                return False
        for i in range(4):
            for j in range(3):
                if self.grid[i][j] == self.grid[i][j+1]:
                    return False
        for j in range(4):
            for i in range(3):
                if self.grid[i][j] == self.grid[i+1][j]:
                    return False
        return True

    def get_state(self):
        state = []
        for row in self.grid:
            for num in row:
                state.append(0 if num == 0 else (num / 2048.0))
        return torch.FloatTensor(state).unsqueeze(0)

    def render(self):
        print(f"Score: {self.score}")
        for row in self.grid:
            print([num for num in row])
        print()

# DQNエージェントの定義
class DQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.98, epsilon=1.0, epsi-lon_decay=0.999, epsilon_min=0.01, memory_size=10000, batch_size=32):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.memory_size = memory_size
        self.batch_size = batch_size
        self.memory = deque(maxlen=memory_size)
        self.model = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        state = state.clone().detach()  # Correctly copy the tensor
        state = state.unsqueeze(0)  # Add a batch dimension
        q_values = self.model(state)
        return torch.argmax(q_values).item()

    # replay関数内の該当部分
    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        minibatch = random.sample(self.memory, self.batch_size)
        for state, action, reward, next_state, done in minibatch:
            # 修正版: clone().detach()を使用
            state = state.clone().detach().view(-1, self.state_size)
            next_state = next_state.clone().detach().view(-1, self.state_size)
            target = self.model(state).clone().detach()  # 重要: clone().detach()を追加
            target[0][action] = reward if done else reward + self.gamma * torch.max(self.model(next_state)).item()
            self.optimizer.zero_grad()
            loss = self.criterion(target, self.model(state))
            loss.backward()
            self.optimizer.step()
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        return loss.item()

# 学習プロセス
def train_dqn(episodes=2000):
    state_size = GRID_SIZE * GRID_SIZE
    action_size = len(ACTIONS)
    agent = DQNAgent(state_size, action_size)
    env = Game2048Env()
    losses = []
    rewards = []
    moving_avg_losses = []
    moving_avg_rewards = []
    moving_avg_window = 20

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            loss = agent.replay()
            
        if episode % 2 == 0:
            print(f"Episode {episode}: Total Reward = {total_reward}, Loss = {loss}")
            #print(env.grid)

        if loss is not None:
            losses.append(loss)
        rewards.append(total_reward)

        if episode % 6 == 0:
            env.render()

        # 显示信息
        if episode % 100 == 0:
            clear_output(wait=True)

        if len(losses) >= moving_avg_window:
            moving_avg_losses.append(np.mean(losses[-moving_avg_window:]))
            moving_avg_rewards.append(np.mean(rewards[-moving_avg_window:]))

    return losses, rewards, moving_avg_losses, moving_avg_rewards

# メイン処理
if __name__ == "__main__":
    losses, rewards, moving_avg_losses, moving_avg_rewards = train_dqn()

    # ----
    # 結果のグラフ表示
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(rewards, label='Raw Scores')
    plt.plot(moving_avg_rewards, label='Moving Average', color='red')
    plt.title('Score Progress')
    plt.xlabel('Episode')
    plt.ylabel('Score')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(losses, label='Raw Losses')
    plt.plot(moving_avg_losses, label='Moving Average', color='red')
    plt.title('Training Loss')
    plt.xlabel('Episode')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()"""

```

QEU:FOUNDER ： “PPOって、最近のLLMの強化学習ツールのファミリーじゃないですか。GRPOとか、ORPOとか・・・。”

![imageBSR1-13-2](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-2.jpg) 

D先生： “それでは、推論した結果をみてみましょう。まずは、使用方法から・・・。 “

**(使用方法)**

Pythonの実行環境: Python 3.6以上が必要です。
必要なパッケージ: numpy, torch, matplotlib をインストールしてください。インストールは以下のコマンドで行います。

pip install numpy torch matplotlib

プログラムの実行: 上記のコードをPythonファイル（例: train_2048_ppo.py）に保存し、以下のコマンドで実行します。

python train_2048_ppo.py

GPU利用: GPUが利用可能な場合は、自動的にGPUを使用します。利用しない場合は、device 変数を torch.device("cpu") に変更してください。

**(注意事項)**

- 学習時間: PPOはDQNと比べて学習時間が長くなる可能性があります。適切なエピソード数を設定してください。
- ハイパーパラメータ調整: 学習効果を上げるために、ハイパーパラメータ（learning_rate, gamma, clip_param, ppo_epochs, batch_sizeなど）を調整してください。
- メモリ使用量: 学習中に使用するメモリ量が多い場合があります。GPUのメモリ容量を確認してください。
- 安定性: 学習途中にエラーが発生することがあります。コードを適切に修正してください。
- 結果の解釈: 学習結果はランダム性に左右されるため、複数回実行して平均的な性能を確認してください。

QEU:FOUNDER ： “CODERモデルは、すごいですね。こんなに良い情報を即時に出してくれます。つぎは、いよいよPPOのコードです。ちなみに、彼らのいうところのPPOね・・・。”

```python
# ---
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
from IPython.display import clear_output

# GPUが利用可能か確認
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2048ゲームの環境設定
GRID_SIZE = 4
ACTIONS = ['left', 'right', 'up', 'down']

# PPO モデルの定義
class PPOModel(nn.Module):
    def __init__(self, state_size, action_size):
        super(PPOModel, self).__init__()
        self.fc1 = nn.Linear(state_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.actor = nn.Linear(256, action_size)
        self.critic = nn.Linear(256, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        action_prob = torch.softmax(self.actor(x), dim=-1)
        state_value = self.critic(x)
        return action_prob, state_value

# 2048ゲーム環境
class Game2048Env:
    def __init__(self):
        self.grid = None
        self.score = 0
        self.reset()

    def reset(self):
        self.grid = [[0] * 4 for _ in range(4)]
        self.add_new_tile()
        self.add_new_tile()
        self.score = 0
        return self.get_state()

    def add_new_tile(self):
        empty_cells = [(i, j) for i in range(4) for j in range(4) if self.grid[i][j] == 0]
        if empty_cells:
            i, j = random.choice(empty_cells)
            self.grid[i][j] = 2 if random.random() < 0.9 else 4

    def step(self, action):
        old_score = self.score
        moved = False

        if action == 0:  # Left
            new_grid, moved = self.move_left(self.grid)
        elif action == 1:  # Right
            new_grid, moved = self.move_right(self.grid)
        elif action == 2:  # Up
            new_grid, moved = self.move_up(self.grid)
        elif action == 3:  # Down
            new_grid, moved = self.move_down(self.grid)
        else:
            raise ValueError("Invalid action")

        if moved:
            self.grid = new_grid
            self.add_new_tile()

        reward = self.score - old_score
        done = self.is_game_over()
        return self.get_state(), reward, done, {}

    def move_left(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row)
            new_grid.append(new_row)
            if has_moved:
                moved = True
        return new_grid, moved

    def compress_and_merge(self, row):
        compressed = [num for num in row if num != 0]
        compressed += [0] * (4 - len(compressed))
        merged = []
        has_merged = False
        i = 0
        while i < len(compressed):
            if i < len(compressed)-1 and compressed[i] == compressed[i+1]:
                merged_val = compressed[i] * 2
                merged.append(merged_val)
                self.score += merged_val
                i += 2
                has_merged = True
            else:
                merged.append(compressed[i])
                i += 1
        new_compressed = merged + [0] * (4 - len(merged))
        has_moved = (new_compressed != row) or has_merged
        return new_compressed, has_moved

    def move_right(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_grid.append(new_row[::-1])
            if has_moved:
                moved = True
        return new_grid, moved

    def move_up(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row)
            new_transposed.append(new_row)
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def move_down(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_transposed.append(new_row[::-1])
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def is_game_over(self):
        for row in self.grid:
            if 0 in row:
                return False
        for i in range(4):
            for j in range(3):
                if self.grid[i][j] == self.grid[i][j+1]:
                    return False
        for j in range(4):
            for i in range(3):
                if self.grid[i][j] == self.grid[i+1][j]:
                    return False
        return True

    def get_state(self):
        state = []
        for row in self.grid:
            for num in row:
                state.append(0 if num == 0 else (num / 2048.0))
        return torch.FloatTensor(state).to(device).unsqueeze(0)

    def render(self):
        print(f"Score: {self.score}")
        for row in self.grid:
            print([num for num in row])
        print()

# このクラスの名前を変更
class PPOAgent:  # 名前を変更
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, clip_param=0.2, ppo_epochs=4, batch_size=32):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.clip_param = clip_param
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.model = PPOModel(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

    def act(self, state):
        with torch.no_grad():
            action_probs, _ = self.model(state)
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action), action_probs

    def train(self, memory):
        # メモリからランダムにサンプルを取得
        batch = random.sample(memory, self.batch_size)
        
        # サンプルを分解
        states, actions, log_probs, rewards, dones, next_states = zip(*batch)
        
        # ここでテンソルに変換
        states = torch.cat(states)
        actions = torch.tensor(actions).to(device)
        log_probs = torch.cat(log_probs)
        rewards = torch.tensor(rewards).to(device)
        dones = torch.tensor(dones).to(device)
        next_states = torch.cat(next_states)

        # 以下は元のコードのロジックを適用
        rewards = rewards.unsqueeze(1)
        dones = dones.unsqueeze(1).float()

        with torch.no_grad():
            _, old_state_values = self.model(states)
            _, next_state_values = self.model(next_states)

        returns = []
        discounted_reward = 0
        for reward, done in zip(reversed(rewards), reversed(dones)):
            if done:
                discounted_reward = 0
            discounted_reward = reward + self.gamma * discounted_reward
            returns.insert(0, discounted_reward)
        returns = torch.tensor(returns, dtype=torch.float).to(device).unsqueeze(1)

        advantages = returns - old_state_values

        for _ in range(self.ppo_epochs):
            for state, action, old_log_prob, advantage in zip(states, actions, log_probs, advantages):
                action_probs, state_values = self.model(state)

                dist = torch.distributions.Categorical(action_probs)
                log_prob = dist.log_prob(action)
                entropy = dist.entropy().mean()

                ratio = (log_prob - old_log_prob).exp()
                surr1 = ratio * advantage
                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantage
                actor_loss = -torch.min(surr1, surr2).mean()
                critic_loss = (returns - state_values).pow(2).mean()

                loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

# 学習プロセス
def train_ppo(episodes=2000):
    state_size = GRID_SIZE * GRID_SIZE
    action_size = len(ACTIONS)
    agent = PPOAgent(state_size, action_size)
    env = Game2048Env()
    memory = deque(maxlen=500)
    rewards = []
    moving_avg_rewards = []
    moving_avg_window = 20

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action, log_prob, action_prob = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            memory.append((state, action, log_prob, reward, done, next_state))
            state = next_state
            total_reward += reward

            if len(memory) == agent.batch_size:
                agent.train(memory)
                memory.clear()

        rewards.append(total_reward)

        if episode % 2 == 0:
            print(f"Episode {episode}: Total Reward = {total_reward}")

        if episode % 6 == 0:
            env.render()

        # 显示情報
        if episode % 100 == 0:
            clear_output(wait=True)

        if len(rewards) >= moving_avg_window:
            moving_avg_rewards.append(np.mean(rewards[-moving_avg_window:]))

    return rewards, moving_avg_rewards

# メイン処理
#if __name__ == "__main__":
rewards, moving_avg_rewards = train_ppo()

# ---
import matplotlib.pyplot as plt

# 結果のグラフ表示
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(rewards, label='Raw Scores')
plt.plot(moving_avg_rewards, label='Moving Average', color='red')
plt.title('Score Progress')
plt.xlabel('Episode')
plt.ylabel('Score')
plt.legend()

plt.tight_layout()
plt.show()

```

D先生：“私は、PPOをやってみたことがないので、コードの出来栄えの評価のしようがないです。ともあれ、このプログラムは動きましたか？”

QEU:FOUNDER ： “もちろん、当初は動きませんが、少しだけ修正すれば動くようになります。このプログラムは、動くことは動くんだが、すごく遅いんですよ。PPOは、GPUを使っても実行速度が遅いのは知られた事実ではあるのだが・・・。”

D先生：“このプログラムで大丈夫なのかなあ・・・。”

![imageBSR1-13-3](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-3.jpg) 

QEU:FOUNDER ： “じゃあ、何はともあれ**「推論精度だけは強いQwen-max」**を使ってみましょう。以下のような質問を投げかけてみましょう。”

```python
以下のプログラムは、ゲーム2048を環境、PPOをエージェントとした強化学習のプログラムです。
Google colabのT4ーGPUで実行したところ、計算時間が非常にかかっています。
バグがないかを確認してください。パグがあれば修正してください。
実行速度を上げるための改善点があれば、必要に応じて修正してください。
'''
# ---
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
from IPython.display import clear_output

# GPUが利用可能か確認
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2048ゲームの環境設定
GRID_SIZE = 4
ACTIONS = ['left', 'right', 'up', 'down']

# PPO モデルの定義
class PPOModel(nn.Module):
    def __init__(self, state_size, action_size):
        super(PPOModel, self).__init__()
        self.fc1 = nn.Linear(state_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.actor = nn.Linear(256, action_size)
        self.critic = nn.Linear(256, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        action_prob = torch.softmax(self.actor(x), dim=-1)
        state_value = self.critic(x)
        return action_prob, state_value

# 2048ゲーム環境
class Game2048Env:
    def __init__(self):
        self.grid = None
        self.score = 0
        self.reset()

    def reset(self):
        self.grid = [[0] * 4 for _ in range(4)]
        self.add_new_tile()
        self.add_new_tile()
        self.score = 0
        return self.get_state()

    def add_new_tile(self):
        empty_cells = [(i, j) for i in range(4) for j in range(4) if self.grid[i][j] == 0]
        if empty_cells:
            i, j = random.choice(empty_cells)
            self.grid[i][j] = 2 if random.random() < 0.9 else 4

# … 中間は省略します …

# 学習プロセス
def train_ppo(episodes=1000):
    state_size = GRID_SIZE * GRID_SIZE
    action_size = len(ACTIONS)
    agent = PPOAgent(state_size, action_size)
    env = Game2048Env()
    memory = deque(maxlen=300)
    rewards = []
    moving_avg_rewards = []
    moving_avg_window = 20

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action, log_prob, action_prob = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            memory.append((state, action, log_prob, reward, done, next_state))
            state = next_state
            total_reward += reward

            if len(memory) == agent.batch_size:
                agent.train(memory)
                memory.clear()

        rewards.append(total_reward)

        if episode % 2 == 0:
            print(f"Episode {episode}: Total Reward = {total_reward}")

        if episode % 6 == 0:
            env.render()

        # 显示情報
        if episode % 100 == 0:
            clear_output(wait=True)

        if len(rewards) >= moving_avg_window:
            moving_avg_rewards.append(np.mean(rewards[-moving_avg_window:]))

    return rewards, moving_avg_rewards

# メイン処理
#if __name__ == "__main__":
rewards, moving_avg_rewards = train_ppo()
'''
```

D先生： “いやあ、ずいぶん正直な「告白」ですねえ・・・（笑）。推論(REASONING)の一部を見てみましょう。“

Okay, let's see. The user provided a Python script for training a PPO agent to play 2048, and they're experiencing very slow computation on a Google Colab T4 GPU. They want me to check for bugs and suggest improvements to speed things up.

First, I need to look through the code for possible bugs. Let's start with the environment implemen-tation. The Game2048Env's move functions: when moving left, right, up, down, are they correctly implemented? For example, in move_left, the compress_and_merge function is applied to each row. Then for right, it reverses the row, processes it, then reverses back. That seems right. Similarly for up and down, transposing the grid. Maybe there's an issue with how the grid is being modified. Wait, when moving up, the code transposes the grid, then processes each row (which was a column original-ly). Then after moving, transposes back. That should work. But maybe there's a bug in the com-press_and_merge function. Let me check that.

In compress_and_merge, the code first removes zeros, then merges adjacent equal tiles. After merg-ing, it pads with zeros. The merged list is created by checking if consecutive elements are equal. But when merging, after merging two tiles, the next tile is skipped. For example, if the row is [2,2,2,0], compressing gives [2,2,2], then merging the first two into 4, then the next 2 remains. So merged be-comes [4,2], then padded to [4,2,0,0]. That seems correct. Also, the has_merged flag is set if any merging happened. The moved flag is set if the new_compressed is different from the original row or if merging occurred. That seems correct.

では、見てみましょう。ユーザーは、PPO エージェントをトレーニングして 2048 をプレイするための Python スクリプトを提供しましたが、Google Colab T4 GPU での計算が非常に遅いという問題に直面しています。バグがないか確認し、処理を高速化するための改善を提案してほしいとのことです。

まず、コードを調べてバグがないか確認する必要があります。環境の実装から始めましょう。Game2048Env の移動関数: 左、右、上、下に移動する場合、正しく実装されていますか? たとえば、move_left では、compress_and_merge 関数が各行に適用されます。次に、右の場合は、行を反転して処理し、元に戻します。これで正しいようです。同様に、上下の場合は、グリッドを転置します。グリッドの変更方法に問題があるのかもしれません。待ってください。上に移動する場合、コードはグリッドを転置し、各行 (元々は列でした) を処理します。次に、移動後に転置します。これで動作するはずです。しかし、compress_and_merge 関数にバグがある可能性があります。確認してみましょう。

compress_and_merge では、コードは最初にゼロを削除し、次に隣接する等しいタイルをマージします。マージ後、ゼロで埋めます。マージされたリストは、連続する要素が等しいかどうかをチェックすることによって作成されます。ただし、マージする場合、2 つのタイルをマージした後、次のタイルがスキップされます。たとえば、行が [2,2,2,0] の場合、圧縮すると [2,2,2] になり、最初の 2 つが 4 にマージされ、次の 2 つが残ります。したがって、マージされた値は [4,2] になり、次に [4,2,0,0] に埋められます。これは正しいようです。また、マージが発生した場合は has_merged フラグが設定されます。new_compressed が元の行と異なる場合、またはマージが発生した場合は moved フラグが設定されます。これは正しいようです。

Next, the get_state function returns a flattened grid where each cell is 0 or normalized by 2048. That's okay, but maybe using log scaling could help, but that's more of a feature engineering thing. Not a bug.

Now, the PPOAgent's act function: when sampling an action, it uses the Categorical distribution. That's correct since the model outputs a softmax probability. So action sampling should be okay.

Looking at the training loop. The agent.train(memory) is called whenever the memory reaches batch_size. But the memory is a deque with maxlen 300. Wait, the code appends transitions to memory, and when len(memory) == batch_size, it trains and clears the memory. But if the batch_size is 32, then every 32 steps, it trains once. But in PPO, typically you collect a certain number of steps, then perform multiple epochs of training on that data. However, in the current code, after each batch_size steps, the memory is cleared, which might not be ideal. Also, the way the batch is sampled is by taking random samples from the memory, which might not maintain the sequential nature of the trajectories. But PPO usually uses trajectories and calculates advantages over them. The current implementation might be breaking the trajectories, leading to incorrect advantage estimates. That could be a problem.

次に、get_state 関数は、各セルが 0 または 2048 で正規化された平坦化されたグリッドを返します。これは問題ありませんが、ログ スケーリングを使用すると役立つ可能性がありますが、これは機能エンジニアリングに関することです。バグではありません。

次に、PPOAgent の act 関数について説明します。アクションをサンプリングするときに、カテゴリ分布を使用します。モデルがソフトマックス確率を出力するので、これは正しいです。したがって、アクションのサンプリングは問題ないはずです。

トレーニング ループを見てみましょう。メモリが batch_size に達するたびに、agent.train(memory) が呼び出されます。ただし、メモリは maxlen が 300 の deque です。待ってください。コードは遷移をメモリに追加し、len(memory) == batch_size になると、メモリをトレーニングしてクリアします。ただし、batch_size が 32 の場合、32 ステップごとに 1 回トレーニングします。ただし、PPO では通常、特定の数のステップを収集してから、そのデータに対して複数のエポックのトレーニングを実行します。ただし、現在のコードでは、batch_size ステップごとにメモリがクリアされるため、理想的ではない可能性があります。また、バッチのサンプリング方法はメモリからランダム サンプルを取得することであるため、軌跡の連続性が維持されない可能性があります。ただし、PPO は通常、軌跡を使用して、それに対する利点を計算します。現在の実装では、軌跡が壊れて、利点の推定が不正確になる可能性があります。これが問題になる可能性があります。

Another issue: in the train function, when the batch is sampled, it's using random.sample(memory, batch_size). But if the memory has exactly batch_size elements, this is okay, but if it's larger, this would sample without replacement. However, in the code, after each training step, the memory is cleared. So the memory is filled until it reaches batch_size, then trained and cleared. So each training step uses exactly batch_size transitions. But in PPO, you usually collect a batch of experiences, then train on that batch for multiple epochs. But here, the code is using each batch once, then discarding it. That might be a problem because PPO typically requires multiple epochs over the same data. But in the current setup, the agent.train(memory) is called once per batch, and then the memory is cleared. So each batch is used once. That's not standard PPO. The standard approach is to collect a batch of trajectories, then perform multiple epochs of training on that same batch. So the code here might not be implementing PPO correctly, which could lead to poor learning and inefficient use of data.

Another possible issue: the calculation of returns and advantages. In the code, returns are calculated by iterating backwards through the rewards and dones. But since each batch is a collection of inde-pendent transitions (not trajectories), the returns might not be calculated correctly. For example, if a trajectory is split across batches, the return calculation would be incorrect. Because in the code, each call to train uses a batch of transitions that are not necessarily from the same episode. This could lead to incorrect advantage estimates, which would harm learning.

Also, in the code, when calculating returns, it's using the next_state_values for the next state. But in the current setup, since the transitions are not part of a trajectory, the next_state_values might not correspond to the actual next state in the trajectory, leading to incorrect TD errors.

So, the way the memory is handled and the training loop is structured might be a significant issue. This could be a bug in the PPO implementation.

Another thing: the PPOAgent's train function is looping over each sample in the batch for each PPO epoch. That is, for each epoch, it's iterating through each state, action, etc., in the batch and perform-ing a gradient update for each individual sample. This is not efficient. Typically, you would process the entire batch at once using vectorized operations, which is much faster. Processing each sample individually in a loop would be very slow, especially on a GPU, which is optimized for batch pro-cessing.

もう 1 つの問題: トレーニング関数では、バッチがサンプリングされるときに random.sample(memory, batch_size) が使用されています。ただし、メモリに batch_size の要素が正確に含まれていれば問題ありませんが、それより大きい場合は、置換なしでサンプリングされます。ただし、コードでは、各トレーニング ステップの後にメモリがクリアされます。したがって、メモリは batch_size に達するまで満たされ、その後トレーニングされてクリアされます。したがって、各トレーニング ステップでは、正確に batch_size の遷移が使用されます。ただし、PPO では通常、エクスペリエンスのバッチを収集し、そのバッチで複数のエポックにわたってトレーニングします。ただし、ここでは、コードは各バッチを 1 回使用してから破棄しています。PPO では通常、同じデータに対して複数のエポックが必要になるため、これは問題になる可能性があります。ただし、現在の設定では、agent.train(memory) がバッチごとに 1 回呼び出され、その後メモリがクリアされます。したがって、各バッチは 1 回使用されます。これは標準的な PPO ではありません。標準的なアプローチは、軌跡のバッチを収集し、同じバッチで複数のエポックのトレーニングを実行することです。したがって、このコードは PPO を正しく実装していない可能性があり、学習の質が低下し、データの非効率的な使用につながる可能性があります。

もう 1 つの考えられる問題は、リターンと利点の計算です。コードでは、リターンは報酬と完了を逆順に反復して計算されます。ただし、各バッチは独立した遷移 (軌道ではない) のコレクションであるため、リターンが正しく計算されない可能性があります。たとえば、軌道がバッチに分割されている場合、リターンの計算は正しくありません。コードでは、train の各呼び出しで、必ずしも同じエピソードからのものではない遷移のバッチが使用されるためです。これにより、利点の推定が不正確になり、学習に悪影響を与える可能性があります。

また、コードでは、リターンを計算するときに、次の状態の next_state_values を使用しています。ただし、現在の設定では、遷移は軌道の一部ではないため、next_state_values が軌道の実際の次の状態に対応しない可能性があり、TD エラーが不正確になります。

したがって、メモリの処理方法とトレーニング ループの構造が重大な問題である可能性があります。これは、PPO 実装のバグである可能性があります。

もう 1 つ: PPOAgent のトレーニング関数は、各 PPO エポックのバッチ内の各サンプルをループしています。つまり、各エポックで、バッチ内の各状態、アクションなどを反復処理し、各サンプルの勾配更新を実行します。これは効率的ではありません。通常、ベクトル化された操作を使用してバッチ全体を一度に処理します。これははるかに高速です。ループで各サンプルを個別に処理すると、特にバッチ処理に最適化された GPU では非常に遅くなります。

So, the main issues I see are:

Incorrect handling of trajectories and advantage calculation, leading to improper PPO updates.
Inefficient training loop that processes each sample individually in a loop, leading to slow computa-tion.
The memory is cleared after each training step, which might not allow for proper use of past experi-ences.
Let's tackle these one by one.

First, the PPO implementation. PPO typically collects a batch of experiences (a trajectory or multi-ple trajectories) and then performs multiple epochs of training on that batch. The current code col-lects transitions until the memory reaches batch_size, then trains once and clears the memory. This is not ideal. Instead, the agent should collect a full trajectory (or multiple trajectories) and then train on that data for multiple epochs. But in the current setup, each episode might be longer than the batch size, leading to incomplete trajectories.

Alternatively, maybe the code is using a batch of transitions sampled from the memory, but since the memory is cleared after each training step, it's not using past experiences. This is more like a stand-ard online learning approach, but PPO usually uses a replay buffer of recent experiences. However, in the code, the memory is a deque with maxlen 300, but each time the batch is filled (to batch_size), it's trained and then cleared. So the memory is not being used as a replay buffer but as a temporary storage for each batch.

私が見つけた主な問題は次のとおりです。

軌道とアドバンテージ計算の不適切な処理により、PPO が適切に更新されない。

各サンプルをループで個別に処理する非効率的なトレーニング ループにより、計算が遅くなる。
各トレーニング ステップの後にメモリがクリアされるため、過去の経験を適切に使用できない可能性があります。
これらを 1 つずつ解決しましょう。

まず、PPO の実装。PPO は通常、経験のバッチ (1 つまたは複数の軌道) を収集し、そのバッチで複数のエポックのトレーニングを実行します。現在のコードは、メモリが batch_size に達するまで遷移を収集し、1 回トレーニングしてメモリをクリアします。これは理想的ではありません。代わりに、エージェントは完全な軌道 (または複数の軌道) を収集し、そのデータで複数のエポックにわたってトレーニングする必要があります。ただし、現在の設定では、各エピソードがバッチ サイズよりも長くなる可能性があり、軌道が不完全になります。

あるいは、コードはメモリからサンプリングされた遷移のバッチを使用しているかもしれませんが、メモリは各トレーニング ステップの後にクリアされるため、過去の経験は使用されていません。これは、標準的なオンライン学習アプローチに似ていますが、PPO は通常、最近の経験の再生バッファーを使用します。ただし、コードでは、メモリは最大長が 300 の deque ですが、バッチがいっぱいになるたびに (batch_size まで)、トレーニングされてからクリアされます。そのため、メモリは再生バッファーとしてではなく、各バッチの一時ストレージとして使用されています。

（以下省略）

QEU:FOUNDER ： “人間に、このようなデバッグの解説をさせると、たぶん発狂するよね（笑）。”

D先生： “それにしても、いままで問題になった**「REASONING段階で際限なく吐き出されるコード情報」という問題**は出て来ていません。こういうREASONINGであれば、多少文章量が長くても、大歓迎です。”

QEU:FOUNDER ： “REASONING-LLMによるデバッグがこんなに強力であるという事実に驚きました。ともあれ、肝心のコードを晒します。”

```python
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from IPython.display import clear_output

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

GRID_SIZE = 4
ACTIONS = ['left', 'right', 'up', 'down']

class PPOModel(nn.Module):
    def __init__(self, state_size, action_size):
        super(PPOModel, self).__init__()
        self.fc1 = nn.Linear(state_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.actor = nn.Linear(256, action_size)
        self.critic = nn.Linear(256, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        action_prob = torch.softmax(self.actor(x), dim=-1)
        state_value = self.critic(x)
        return action_prob, state_value

class Game2048Env:
    def __init__(self):
        self.grid = np.zeros((4, 4), dtype=np.int32)
        self.score = 0
        self.reset()

    def reset(self):
        self.grid = np.zeros((4, 4), dtype=np.int32)
        self.add_new_tile()
        self.add_new_tile()
        self.score = 0
        return self.get_state()

    def add_new_tile(self):
        empty = np.argwhere(self.grid == 0)
        if len(empty) > 0:
            idx = empty[np.random.randint(len(empty))]
            self.grid[tuple(idx)] = 2 if np.random.rand() < 0.9 else 4

    def step(self, action):
        old_score = self.score
        moved = False

        if action == 0:  # Left
            new_grid, moved = self.move_left(self.grid)
        elif action == 1:  # Right
            new_grid, moved = self.move_right(self.grid)
        elif action == 2:  # Up
            new_grid, moved = self.move_up(self.grid)
        elif action == 3:  # Down
            new_grid, moved = self.move_down(self.grid)
        else:
            raise ValueError("Invalid action")

        if moved:
            self.grid = new_grid
            self.add_new_tile()

        reward = self.score - old_score
        done = self.is_game_over()
        return self.get_state(), reward, done, {}

    def move_left(self, grid):
        new_grid = np.empty_like(grid)
        moved = False
        for i in range(4):
            row = grid[i]
            new_row, has_moved = self.compress_and_merge(row)
            new_grid[i] = new_row
            if has_moved:
                moved = True
        return new_grid, moved

    def compress_and_merge(self, row):
        non_zero = row[row != 0]
        merged = []
        has_merged = False
        i = 0
        while i < len(non_zero):
            if i < len(non_zero)-1 and non_zero[i] == non_zero[i+1]:
                merged_val = non_zero[i] * 2
                merged.append(merged_val)
                self.score += merged_val
                i += 2
                has_merged = True
            else:
                merged.append(non_zero[i])
                i += 1
        new_row = np.zeros(4, dtype=np.int32)
        new_row[:len(merged)] = merged
        has_moved = (new_row != row).any() or has_merged
        return new_row, has_moved

    def move_right(self, grid):
        new_grid, moved = self.move_left(grid[:, ::-1])
        return new_grid[:, ::-1], moved

    def move_up(self, grid):
        new_grid, moved = self.move_left(grid.T)
        return new_grid.T, moved

    def move_down(self, grid):
        new_grid, moved = self.move_right(grid.T)
        return new_grid.T, moved

    def is_game_over(self):
        if np.any(self.grid == 0):
            return False
        for i in range(4):
            for j in range(3):
                if self.grid[i, j] == self.grid[i, j+1]:
                    return False
        for j in range(4):
            for i in range(3):
                if self.grid[i, j] == self.grid[i+1, j]:
                    return False
        return True

    def get_state(self):
        state = np.where(self.grid == 0, 0, self.grid / 2048.0).flatten()
        return torch.FloatTensor(state).unsqueeze(0).to(device)

    def render(self):
        print(f"Score: {self.score}")
        print(self.grid)
        print()

class PPOAgent:
    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99, 
                 clip_param=0.2, ppo_epochs=4, batch_size=64):
        self.gamma = gamma
        self.clip_param = clip_param
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        
        self.model = PPOModel(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)

    def act(self, state):
        with torch.no_grad():
            action_probs, _ = self.model(state)
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def train(self, memory):
        states, actions, old_log_probs, rewards, dones, next_states = zip(*memory)
        
        states = torch.cat(states)
        actions = torch.tensor(actions).to(device)
        old_log_probs = torch.cat(old_log_probs)
        rewards = torch.tensor(rewards).unsqueeze(1).to(device)
        dones = torch.tensor(dones).unsqueeze(1).float().to(device)
        next_states = torch.cat(next_states)

        with torch.no_grad():
            _, state_values = self.model(states)
            _, next_values = self.model(next_states)
            td_targets = rewards + self.gamma * next_values * (1 - dones)
            advantages = td_targets - state_values

        for _ in range(self.ppo_epochs):
            action_probs, state_values = self.model(states)
            dist = torch.distributions.Categorical(action_probs)
            new_log_probs = dist.log_prob(actions)
            
            ratio = (new_log_probs - old_log_probs).exp()
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            critic_loss = 0.5 * (td_targets - state_values).pow(2).mean()
            entropy = dist.entropy().mean()
            
            loss = actor_loss + critic_loss - 0.01 * entropy

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

def train_ppo(episodes=500):
    env = Game2048Env()
    state_size = GRID_SIZE * GRID_SIZE
    action_size = len(ACTIONS)
    agent = PPOAgent(state_size, action_size, batch_size=32)
    rewards = []
    memory = deque(maxlen=128)
    
    for episode in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        traj = []
        
        while not done:
            action, log_prob = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            traj.append((state, action, log_prob, reward, done, next_state))
            state = next_state
            total_reward += reward
            
            if len(traj) >= agent.batch_size:
                agent.train(traj)
                traj = []
        
        if traj:
            agent.train(traj)
        
        rewards.append(total_reward)
        
        if episode % 2 == 0:
            #clear_output(wait=True)
            print(f"Episode {episode}/{episodes}, Reward: {total_reward}")

        if episode % 100 == 0:
            clear_output(wait=True)

    return rewards

rewards = train_ppo()
print(rewards)

# ---
import matplotlib.pyplot as plt
 
# ----
# 結果のグラフ表示
plt.figure(figsize=(8, 5))
 
plt.subplot(1, 1, 1)
plt.plot(rewards, label='Rewards')
plt.title('Training Rewards')
plt.xlabel('Episode')
plt.ylabel('Rewards')
plt.legend()

plt.show()
```

D先生： “あれ？これはPPOなのか？それともActor-Critic（AC）なのか？ “

QEU:FOUNDER  ： “我々はPPOのプログラムを自分で作成した経験がないので、ACとの区別がつかないよね。ともあれ、今回のプログラムは、ほとんど修正なしでもうごきました！”

D先生： “すごいですね！！その結果は？ “

![imageBSR1-13-4](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-4.jpg) 

QEU:FOUNDER  ： “この「改善版」でも、やっぱり時間がかかります。Colabでは、500回が限界です。全然学習量がたりません。まあ、たまに出てくる高スコアにかなりの希望が見られますが・・・。”

D先生：“驚いた・・・。学習初期のスコアが6000も行くんですか？”

QEU:FOUNDER ： “これはActor-Criticの特徴ですね。ポリシーが「ツボにはまる」とスコアが上がるんですよ。コレを見ると、学習時間がかかった理由がわかります。ゲーム量が他の手法よりも多かったんです。ですから、速度のパフォーマンスから見て、コードが改善されたのは間違いでしょう。・・・さて、今回の知見をまとめてみたい。もし、**小生がコードを自動生成するAGIを設計する**となると、こうしたい・・・。”

![imageBSR1-13-5](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-5.jpg) 

D先生： “なるほどね。これは、うまく行く可能性が高いな・・・。14bの小さなモデルで行けるでしょうか？“

QEU:FOUNDER  ： “高度なREASONINGが欲しいので、少なくとも32bは欲しい。そうすると、たとえUnslothでもA100x1程度のGPUでは、強化学習ができません。残念・・・。”

D先生： “「妥当性確認」という言葉にノスタルジーを感じます（笑）。品質ISO9001の製品実現の用語ですね。そういえば、**「Design for Six Sigma(DFSS)」**の改善フローって、どうでしたっけ？“

![imageBSR1-13-6](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-6.jpg) 

QEU:FOUNDER  ： “今回、我々が提案するAGIのイメージは、DFSSのフローのうち、MとAのステップを外したものです。”

D先生： “コード生成に特化したAGIなので、MとAを外すのは妥当ですね。もしも、他の用途に汎用化させると、そのAGIはMeasureステップやAnalyzeのステップが必要になるでしょう。 “

QEU:FOUNDER ： “おそらく、今話題の「MANUS」のシステム群は、そういう構成になっているはずです。MANUSって、おそらくDFSSの各段階にファインチューニングされたDeepSeep-R1が配置されているんですよ。”

D先生： “わかりました。ここまで来ただけでも上出来、これ以上は資源不足で行きませんから・・・。で、次は何をします？”

QEU:FOUNDER： “BONSAI4に戻りましょう。とても長い閑話休題でした・・・（笑）。”


## ～ まとめ ～

QEU:FOUNDER ： “いやいや、いやいや・・・。すごいことになっていますね。”

[![MOVIE1](http://img.youtube.com/vi/-CcgTIP1u5A/0.jpg)](http://www.youtube.com/watch?v=-CcgTIP1u5A "3/14(金) 21:45~ プレミア配信(尾形×金子勝)【In Focus 立花氏への襲撃、現場からの緊急報告／財務省解体デモの場で、何があったのか？／目撃者の話は？】")

D先生：“この件（↑）、違和感ありませんか？”

QEU:FOUNDER ： “「（財務省）解体」って・・・。解体する対象が間違っていないか？”

![imageBSR1-13-7](/2025-03-15-QEUR23_BS4RVW13/imageBSR1-13-7.jpg) 

D先生：“**むしろ解体すべきのは、経済産業省でしょうに・・・**。彼らは、行政における経済問題のCOOでしょ？企業で言えば・・・。パフォーマンスが悪ければ、首になるのは当たり前です。**民間ならば**・・・。”

QEU:FOUNDER ： “それ以前に、世の中の構成が変わっている中で、行政システムがアップデートされているのかが気になりますね。だから、我々の言いたいことは、**「解体＝アップデート」**というわけ。”
