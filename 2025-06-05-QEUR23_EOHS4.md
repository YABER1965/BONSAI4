---
title: QEUR23_EOHS4 - ベクトルストアから多言語抽出し、推論する
date: 2025-06-05
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "LangGraph"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_EOHS4 - ベクトルストアから多言語抽出し、推論する

## ～ 手間はかかるが、これはやる価値はあるでしょう ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “Topicという名前の変数を見て・・・。”

```python
# トピックリスト
topics = [
    "最近のLLMの進歩に関する重要なトピック",
    "日本のLLM開発における固有の特徴",
    "米国のLLM開発における固有の特徴",
    "中国のLLM開発における固有の特徴",
    "米国と中国のLLM開発のレベル差",
    "シンギュラリティの定義と、その実現時期",
    "AGIの実現時期と、現在の課題",
    "人工知能で人々は豊かになるか",
    "人工知能で失業は増えるか",
    "人工知能の金融業界への応用",
    "人工知能の教育業界への応用",
    "人工知能の工業デザインへの応用",
    "人工知能の農業への応用",
    "人工知能の文化への影響",
    "人工知能の社会への影響"
]

```

D先生： “（変数の中身は全部J語のみ）そういうことね・・・。じゃあ、今回のプロジェクト必要である**「多様性への配慮」**を達成しているとは言えませんね。”

![imageEHS1-5-1](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-1.jpg) 

QEU:FOUNDER ： “そうね。もっと多様性を追求しなければならない・・・。インプットするTopicは3か国語にしなければならないですね。それにしても、Embeddingするとどうなるんだろうね？理想的には、言語が違っても、内容が同じであれば同じものを検出することになるよね。”

```python
# --- モジュールインポート ---
import os
import time
import requests
import tempfile
import gc
import logging
import numpy as np
import pandas as pd 
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from langchain.document_loaders import WebBaseLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from openai import OpenAI
import certifi
import langchain
from langchain_community.utilities import Requests

# --- 環境設定 ---
os.environ["USER_AGENT"] = "RAG-Application/1.0"
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
Requests._session = requests.Session()

# --- ロギング設定 ---
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# --- 定数定義 ---
VECTOR_STORE_PATH = "drive/MyDrive/faiss_db"

# --- 埋め込みモデルクラス ---
class AlibabaCloudEmbeddings(Embeddings):
    def __init__(self, model_name="text-embedding-v3", dimensions=1024, api_key=None, base_url=None):
        self.model_name = model_name
        self.dimensions = dimensions
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_fixed(10),
        retry=retry_if_exception_type((requests.exceptions.RequestException,)),
        before_sleep=lambda _: logger.warning("埋め込みリトライ中...")
    )
    def embed_documents(self, texts):
        embeddings = []
        for i in range(0, len(texts), 10):
            batch = texts[i:i+10]
            response = self.client.embeddings.create(
                model=self.model_name,
                input=batch,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            embeddings.extend([data.embedding for data in response.data])
        return embeddings
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_fixed(10),
        retry=retry_if_exception_type((requests.exceptions.RequestException,)),
        before_sleep=lambda _: logger.warning("埋め込みリトライ中...")
    )
    def embed_query(self, text):
        response = self.client.embeddings.create(
            model=self.model_name,
            input=text,
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return response.data[0].embedding

# --- メモリ解放 ---
def free_memory():
    gc.collect()
    logger.info("メモリ解放を実行")

# --- メイン処理 ---
# 埋め込みモデル初期化
embeddings = AlibabaCloudEmbeddings(
    model_name="text-embedding-v3",
    dimensions=1024,
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
)

# FAISSベクトルストア読み込み
vectorstore = FAISS.load_local(
    VECTOR_STORE_PATH,
    embeddings,
    allow_dangerous_deserialization=True
)
logger.info("既存のFAISSデータベースを読み込み")

# --- トピック定義 ---
# 日本語
topics_jp = [
    "2023年以降のLLMの進歩に関する重要なトピックは何か?",
    "日本(JAPAN)の大規模言語モデル(LLM)開発における固有の特徴は何か?",
    "米国(USA)の大規模言語モデル(LLM)開発における固有の特徴は何か?",
    "中国(CHINA)の大規模言語モデル(LLM)開発における固有の特徴は何か?",
    "米国と中国のLLM開発のレベル差は何か?",
    "シンギュラリティの定義と、その実現時期は?",
    "AGIの実現時期と、現在の課題は何か?",
    "人工知能で人々は豊かになるか?",
    "人工知能で失業は増えるか?",
    "人工知能の金融業界への応用は何か?",
    "人工知能の教育業界への応用は何か?",
    "人工知能の工業デザインへの応用は何か?",
    "人工知能の農業への応用は何か?",
    "人工知能の文化への主要な影響は何か?",
    "人工知能の社会への主要な影響は何か?"
]

# 英語
topics_en = [
    "What is Key Topics for LLM Advancement in 2023 and Beyond?",
    "What is Unique approach of Large Language Model(LLM) development in Japan?",
    "What is Unique approach of Large Language Model(LLM) development in the United States?",
    "What is Unique approach of Large Language Model(LLM) development in China?",
    "What is Differences in the level of LLM development between the United States and China?",
    "What is Definition of the singularity and when it will be realized?",
    "When AGI will be realized and current challenges",
    "Will AI make people richer?",
    "Will AI increase unemployment?",
    "What is Application of AI to the financial industry?",
    "What is Application of AI to the education industry?",
    "What is Application of AI to industrial design?",
    "What is Application of AI to agriculture?",
    "What is major impact of AI on culture?",
    "What is major impact of AI on society?"
]

# 中国語
topics_cn = [
    "2023年及以後大規模語言模型（LLM）發展的關鍵主題?",
    "日本大規模語言模型（LLM）發展的獨特之處?",
    "美國大規模語言模型（LLM）發展的獨特之處?",
    "中國大規模語言模型（LLM）發展的獨特之處?",
    "中美大規模語言模型（LLM）發展水平的差異?",
    "奇點的定義及其實現時間?",
    "AGI何時實現以及當前面臨的挑戰?",
    "人工智慧會使人們變得更富有嗎？",
    "人工智慧會增加失業率嗎？",
    "人工智慧在金融產業的應用?",
    "人工智慧在教育產業的應用?",
    "人工智慧在工業設計中的應用?",
    "人工智慧在農業中的應用?",
    "人工智慧對文化的主要影響?",
    "人工智慧對社會的主要影響?"
]

# --- データベース読み込み関数 ---
def read_database(topics, lang):
    arr_refnos = []
    arr_scores = []
    arr_contexts = []
    reference_titles = []
    arr_lengths = []
    
    for i, topic in enumerate(topics):
        prompt = topic
        #prompt = f"{topic}\n 日本語の情報の収集を優先してください。"
        #if lang == "EN":
        #    prompt = f"{topic}\n Please give priority to collecting information in English."
        #elif lang == "CN":
        #    prompt = f"{topic}\n 請優先収集中文資訊。"
        # ---
        try:
            # 改善点1: k=1 → k=4 に変更
            results = vectorstore.similarity_search_with_score(prompt, k=4)
            
            for res, score in results:
                content = res.page_content
                # テキストクリーニング
                content = content.replace("Related topics\n", "").replace("Read more\n", "")
                content = content.replace("\n\uf09f","\n\n")
                content = content.replace("\n\t", "\n").replace("\n \n", "\n")
                content = content.replace("\n ", "\n").replace("       ", " ")
                content = content.replace("      ", " ").replace("     ", " ")
                content = content.replace("      \n ", "\n").replace("     \n ", "\n")
                content = content.replace("    \n ", "\n").replace("   \n ", "\n")
                content = content.replace("  \n ", "\n").replace("\n\n\n\n", "\n")
                content = content.replace("\n\n\n", "\n").replace("\n\n", "\n")
                content = content.replace("\t\t", " ")
                # ---
                arr_refnos.append(i)
                arr_contexts.append(content)
                arr_scores.append(score)
                arr_lengths.append(len(content))
                # ---
                title = res.metadata.get("title", "Unknown")
                reference_titles.append(title)
                
                logger.debug(f"トピック処理成功: {topic[:20]}... | スコア: {score:.3f} | 長さ: {len(content)}")
        
        except Exception as e:
            logger.error(f"トピック処理失敗: {prompt[:20]}... - {str(e)}")
            arr_scores.append(0.0)
            reference_titles.append("Error")
            arr_contexts.append("Error")
            arr_lengths.append(0)
    
    return arr_scores, arr_contexts, reference_titles, arr_lengths, arr_refnos

# --- データ読み込み実行 ---
jp_scores, jp_contexts, jp_titles, jp_lengths, jp_refnos = read_database(topics_jp, "JP")
en_scores, en_contexts, en_titles, en_lengths, en_refnos = read_database(topics_en, "EN")
cn_scores, cn_contexts, cn_titles, cn_lengths, cn_refnos = read_database(topics_cn, "CN")

# --- EXCEL出力関数 ---
def excel_output(score, context, titles, lengths, lang):
    lang_symbol = "jp"
    if lang == "EN":
        lang_symbol = "en"
    elif lang == "CN":
        lang_symbol = "cn"
    elif lang == "ALL":
        lang_symbol = "all"
    # ---
    mx_scores = np.array([score]).T
    df = pd.DataFrame(mx_scores, columns=[f"{lang_symbol}_scores"])
    df[f"{lang_symbol}_contexts"] = context
    df[f"{lang_symbol}_titles"] = titles
    df[f"{lang_symbol}_lengths"] = lengths
    return df

# --- データ結合 ---
jp_langs = ["JP"] * len(jp_scores)
en_langs = ["EN"] * len(en_scores)
cn_langs = ["CN"] * len(cn_scores)

all_refnos = jp_refnos + en_refnos + cn_refnos
all_langs = jp_langs + en_langs + cn_langs
all_scores = jp_scores + en_scores + cn_scores
all_contexts = jp_contexts + en_contexts + cn_contexts
all_titles = jp_titles + en_titles + cn_titles
all_lengths = jp_lengths + en_lengths + cn_lengths
#all_refnos

# --- 改善点2: データフィルタリング ---
# データフレーム作成
df_all = pd.DataFrame({
    'all_refnos': all_refnos,
    'all_langs': all_langs,
    'all_scores': all_scores,
    'all_contexts': all_contexts,
    'all_titles': all_titles,
    'all_lengths': all_lengths
})

# 長さ500以下のレコード削除
df_all = df_all[df_all['all_lengths'] > 700]

# 重複レコード削除 (長さが同じものは最初の1件を残す)
df_all = df_all.drop_duplicates(subset=['all_lengths'], keep='first')

# インデックス再設定
df_all.reset_index(drop=True, inplace=True)

# --- EXCEL出力 ---
df_all.to_excel("drive/MyDrive/all_result.xlsx", index=False)

```

D先生： “直接LLMにつなげるわけではなく、一旦、コンテキスト情報をデータフレームに出力するわけですね。それは、とても良い考え方です。それでは内容を見てみましょう。”

![imageEHS1-5-2](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-2.jpg) 

D先生：“全部で37件が検出されました。そもそもの質問の量は15だけ、それら1件の質問でアウトプットは4つ、さらに言語が三種です。**15 x 4 x 3 = 180件**なければならない。ずいぶん件数が少ないですね。”

QEU:FOUNDER ： “2つ理由があります。**重複している情報を消しました**。ただし、重複は文章量(length)で測っています。まあ、万が一間違って消えるのかもしれんが、基本は問題ないと思います。あとは、文字列の量が700以下を消しています。内容が少ないものは、多くの場合には意味がない文章です。目次とか、参考文献とかね・・・。”

D先生：“そうか。少なくとも、このembeddingモデルは多言語用のベクトルとして機能しているようです。じゃあ、さしあたりはCohereは使わない方向で・・・。”

![imageEHS1-5-3](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-3.jpg) 

QEU:FOUNDER ： “最近、いろいろなサービスを使ったので、少しづつ整理をしたいんです。ちなみに、コレ（↑）は、悪いサービスだと思っていないですよ。何しろ、いわゆる西側のサービスとしては、コストが比較的に安いですから・・・。我々が特殊なことをやりたいだけです。それでは、つづきに行きましょう。次は、そのコンテキストをLLMに入力できるまでに情報を成形していきます。”

```python
# ---
import pandas as pd

# Excelファイルの読み込み
df = pd.read_excel('all_result.xlsx')

# refno 0~14の範囲で結果を格納するリスト
max_refno = len(topics_cn)
results = []

for refno in range(max_refno):
    # 現在のrefnoに対応する行を抽出
    subset = df[df['all_refnos'] == refno]
    
    if subset.empty:
        # refnoが存在しない場合
        results.append({'refno': refno, 'score': 0.0, 'title': 'not found'})
    else:
        # 最大スコアの行を取得
        max_row = subset.loc[subset['all_scores'].idxmax()]
        results.append({
            'refno': refno,
            'score': max_row['all_scores'],
            'title': max_row['all_titles']
        })

# 結果をDataFrameに変換
result_df = pd.DataFrame(results)
# 結果の表示
#print(result_df)

# ---
arr_contexts = df.loc[:,"all_contexts"].values
arr_titles = result_df.loc[:,"title"].values
#print(arr_contexts)

# ---
context = ""
for i in range(len(arr_contexts))
    context = context + arr_contexts[i] + "\n"

# ---
# 結果出力
print("=== context[:1000] ===")
print(context[:1000])

# ---
print("=== reference_titles ===")
reference_titles = arr_titles
print(reference_titles)

```

D先生： “こんな面倒なことをする必要があるんですか？ “

![imageEHS1-5-4](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-4.jpg) 

QEU:FOUNDER  ： “コンテキストの参考文献のタイトルをLLMに入力する必要があります。これって、複数の引用先があるので結構面倒です。類似度スコアが高いものを代表として選びました。”

D先生： “おや？2か所が「NOT FOUND」になっています。“

QEU:FOUNDER  ： “**文章量が700以下**だったんです。つまり、意味のある、有効な情報を得られなかったんです。それでは、最後にLLMを動かしましょう。ドン！！”

```python
# --- モジュールインポート ---
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# --- 固定プロンプト ---
USER_PROMPT = """2025年以降、AI（人工知能）の進歩がさらに速くなって、人々の仕事と生活が変化しています。
AIの最近の技術進歩と、AIが各種の産業、仕事と生活に与える影響について知りたい。以下のフォーマットに示す形で
出力してください。

分析用リスト化フォーマット:
1. **最近のLLMの進歩に関する重要なトピック** - [AIの分析結果](注１)
2. **日本のLLM開発における固有の特徴** - [AIの分析結果](注１)
3. **米国のLLM開発における固有の特徴** - [AIの分析結果](注１)
4. **中国のLLM開発における固有の特徴** - [AIの分析結果](注１)
5. **米国と中国のLLM開発のレベル差** - [AIの分析結果](注１)
6. **シンギュラリティの定義と、その実現時期** - [AIの分析結果](注１)
7. **AGIの実現時期と、現在の課題** - [AIの分析結果](注１)
8. **人工知能で人々は豊かになるか** - [AIの分析結果](注１)
9. **人工知能で失業は増えるか** - [AIの分析結果](注１)
10. **人工知能の金融業界への応用** - [AIの分析結果](注１)
11. **人工知能の教育業界への応用** - [AIの分析結果](注１)
12. **人工知能の工業デザインへの応用** - [AIの分析結果](注１)
13. **人工知能の農業への応用** - [AIの分析結果](注１)
14. **人工知能の文化への影響** - [AIの分析結果](注１)
15. **人工知能の社会への影響** - [AIの分析結果](注１)

注１:[AIの分析結果]は、400文字以上600文字以内とし、リスト化しないでください。

日本語で回答してください。"""

# --- プロンプト生成関数 ---
full_prompt = f"""以下は関連する検索結果です：
{context}

この情報を基に、以下の質問に答えてください。
{USER_PROMPT}

また、以下のフォーマットに基づいて、分析に当たって、AIが最も参考にしたコンテンツのタイトルのリストを生成してください。
参考にしたコンテンツ用リスト化フォーマット:
1. **最近のLLMの進歩に関する重要なトピック** - {reference_titles[0]}
2. **日本のLLM開発における固有の特徴** - {reference_titles[1]}
3. **米国のLLM開発における固有の特徴** - {reference_titles[2]}
4. **中国のLLM開発における固有の特徴** - {reference_titles[3]}
5. **米国と中国のLLM開発のレベル差** - {reference_titles[4]}
6. **シンギュラリティの定義と、その実現時期** - {reference_titles[5]}
7. **AGIの実現時期と、現在の課題** - {reference_titles[6]}
8. **人工知能で人々は豊かになるか** - {reference_titles[7]}
9. **人工知能で失業は増えるか** - {reference_titles[8]}
10. **人工知能の金融業界への応用** - {reference_titles[9]}
11. **人工知能の教育業界への応用** - {reference_titles[10]}
12. **人工知能の工業デザインへの応用** - {reference_titles[11]}
13. **人工知能の農業への応用** - {reference_titles[12]}
14. **人工知能の文化への影響** - {reference_titles[13]}
15. **人工知能の社会への影響** - {reference_titles[14]}
"""

full_prompt = full_prompt.replace("\n\uf09f","\n\n")

# LLMの初期化
llm = ChatOpenAI(
    model="qwen-max",
    temperature=0.3,
    openai_api_key=os.getenv("DASHSCOPE_API_KEY"),
    openai_api_base="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
)

# LLM呼び出し
print("LLM推論を実行中...")
response = llm.invoke([HumanMessage(content=full_prompt)])
llm_output = response.content
#print("==== 推論の実行 ====")
#print(llm_output)

# ---
from IPython.display import Markdown

# ---
print("\n======== AIレポート(QWEN-MAX) ========\n")
Markdown(llm_output)

```

D先生： “今回は、MARKDOWNから出してきました・・・（笑）。“

![imageEHS1-5-5](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-5.jpg) 

QEU:FOUNDER  ： “今回は、前回よりもマシにはなってきています。情報量がかなり大きくなってきていますからね。それには、多言語化がそれなりに効いていると思います。敢ていうと、もうちょっとチャンク量を大きくしたほうがよかった。2000位はあるべきだったですね。”

D先生： “なんというかな・・・。「ローカルなテーマ」では、このようなやり方は不要です。しかし、**「ユニバーサルなテーマ」**では、これは有効な方法です。いよいよ、BONSAIに行きますか？“

QEU:FOUNDER ： “**TRIZのことも考えている**ので、ちょっと休みです。まあ、BONSAIを進めることにはなるとは思うがね。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “しかし、もうシステム自体がヤバくなってくると、なりふり構わないよね。最近、ああ、もうシステムがやばいんだって・・・。”

![imageEHS1-5-6](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-6.jpg) 

C部長 : “ビックリしましたよねえ。**「なぜ、アナタ（立〇）が賛成ですか？」**って・・・。”

![imageEHS1-5-7](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-7.jpg) 

QEU:FOUNDER ： “システムの受益者だから、アレ（システム）が倒れたら困るんですよ。だから、本来は自分の仲間をも、敢て目をつむって受難させるんです。もう、なにふりかまっていない・・・。何気に、ヤバイことが、近々起きると思います。”

![imageEHS1-5-8](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-8.jpg) 

QEU:FOUNDER ： “国内の事であれば、**受難させる人をこっそり増やす**ことでうまく行くが、これは国外がトリガーだから、結構大事になるんじゃない？”

![imageEHS1-5-9](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-9.jpg) 

QEU:FOUNDER  ： “財政改善をやめるんだとよ・・・（笑）。”

D先生： “ひどいなあ・・・。一体、だれがやったんですか！？”

![imageEHS1-5-10](/2025-06-05-QEUR23_EOHS4/imageEHS1-5-10.jpg) 

QEU:FOUNDER ： “この人（↑）です。”
