---
title: QEUR23_BS4RVW5 - Qwen-14Bを使って強化学習Finetuningを試す(Unsloth-ORPO)
date: 2025-02-08
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_BS4RVW5 - Qwen-14Bを使って強化学習Finetuningを試す(Unsloth-ORPO)

## ～ SFTとORPOを同時に試します ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER （設定年齢65歳）： “率直に言ってUnsloth様が苦労して作ってくれた**DeepSeekの蒸留版(↓)**は我々の用途としては使えないよね。理由・・・？J語の弱点は、どうでもいいです。いままでもE語で議論していたので・・・。このモデルの大きな問題は、DSモデルのフォーマットの影響が強く出過ぎています。これをフォーマット修正して、BONSAIシステムに合わせるのは、結構、大変になると思いますよ。”

![imageBSR1-6-1](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-1.jpg) 

D先生（設定年齢65歳）： “残念ですが、それが妥当でしょうね。でも、せっかくだから、ここで得た情報をFINETUNEのために使いたいですね。 “

![imageBSR1-6-2](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-2.jpg)

QEU:FOUNDER（設定年齢65歳）  ： “それでは今回のノウハウをデータベースに追加し、Qwen-32bのファインチューニングに反映してみたいと思います。さらに学習用のデータセット（↑）に、DeepSeekの出力結果（REASONING PLUS RESPONSE）をいくつか追加しています。これで、SFTによるファインチューニングがどうなるか・・・。推論の結果をドン！！”

![imageBSR1-6-3](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-3.jpg)

D先生（設定年齢65歳） ： “J語の推論結果って、例によって、何度学習をトライしても簡単な出力になってしまいます。さて、ここでE語とC語の推論結果がどうなるか・・・。。”

**（E語の場合）**

**----  RESPONSE  ----**

Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request. To improve quality of response, you should perform validation before writing. You should create REASONING for validation in advance, and improve the response if necessary.
Instruction:

I live in Manila. I will be going to Japan in early April to see the cherry blossoms. Please provide me with a recommended itinerary for 4 nights and 5 days, making full use of the Shinkansen. Please also provide a list of places to visit and activities to do each day.

Input:

Reasoning:
You should first identify the key points of the instruction, such as the location (Manila), destination (Japan), season (early April), duration (4 nights and 5 days), and transportation method (Shinkansen). Next, you should research popular destinations in Japan during cherry blossom season, taking into account factors such as proximity to Manila, availability of Shinkansen connections, and the beauty and accessibility of the cherry blossoms. You should then create an itinerary that balances travel time with sightseeing opportuni-ties, and includes a mix of cultural experiences, natural attractions, and local cuisine. Additionally, you should consider any potential language barriers or cultural differences that may affect the traveler's expe-rience, and provide recommendations for accommodations and transportation options that are convenient and affordable. By following these steps, you can ensure that the itinerary is well-organized, informative, and tailored to the traveler's interests and preferences.

Response:

Day 1: Arrive in Tokyo
- Take a taxi from Narita Airport to your hotel
- Visit Ueno Park to see cherry blossoms
- Try some Japanese street food in Shibuya

Day 2: Tokyo
- Take the Shinkansen to Hakone
- Visit Owakudani, a volcanic valley with hot springs and sulfur vents
- Go on a cruise on Lake Ashi to see Mt. Fuji

Day 3: Hakone to Kyoto
- Take the Shinkansen to Kyoto
- Visit Fushimi Inari Taisha, famous for its thousands of torii gates
- Explore the Gion district and try some traditional Japanese sweets

Day 4: Kyoto
- Visit Kiyomizu-dera temple, which has beautiful cherry blossoms
- Walk through the Philosopher's Path, lined with cherry trees
- Visit the Arashiyama Bamboo Grove and take a boat ride on the Hozu River

Day 5: Kyoto to Osaka
- Take the Shinkansen to Osaka
- Visit Osaka Castle and explore the surrounding park
- Try some Osaka-style takoyaki and other street food
- Take a taxi back to Kansai International Airport<|im_end|>


**（C語の場合）**

**----  RESPONSE  ----**

Below is an instruction that describes a task, paired with an input that provides further context. You should write a response that appropriately completes the request. To improve quality of response, you should perform validation before writing. You should create REASONING for validation in advance, and improve the response if necessary.

Instruction:
我住在馬尼拉。我將在四月初去日本看櫻花。為您推薦充分利用新幹線的5天4夜旅行計畫。列出您將要參觀的地方和每天要做的活動。

Input:

Reasoning:
首先，助理應考慮從馬尼拉到日本的交通方式，並確定最佳的出發日期。接下來，助理應研究日本櫻花的最佳賞花時間，並選擇最佳的賞花地點。然後，助理應考慮新幹線的路線和班次，並根據這些信息制定行程。助理還應考慮每天的活動，包括賞櫻、文化體驗和休閒活動。最後，助理應確保行程安排合理，並考慮到旅客的需求和喜好。

Response:
以下是一個充分利用新幹線的5天4夜旅行計畫：

第一天：
- 早上：搭乘飛機從馬尼拉前往東京。
- 下午：抵達東京後，乘坐新幹線前往京都。
- 晚上：在京都的酒店入住。

第二天：
- 上午：在京都賞櫻，包括參觀清水寺和金閣寺。
- 下午：在京都的花園散步，欣賞櫻花。
- 晚上：在京都的溫泉酒店住宿。

第三天：
- 上午：搭乘新幹線前往大阪。
- 下午：參觀大阪城公園，欣賞櫻花。
- 晚上：在大阪的居酒屋品嚐當地美食。

第四天：
- 上午：搭乘新幹線前往奈良。
- 下午：參觀奈良公園，欣賞櫻花和鹿群。
- 晚上：返回大阪，在大阪的酒店住宿。

第五天：
- 上午：搭乘新幹線返回東京。
- 下午：在東京的淺草寺欣賞櫻花。
- 晚上：搭乘飛機返回馬尼拉。<|im_end|>



D先生： “なっ・・・、なんと！！**J語の出力の質がまたもやEC語に大負け**じゃないですか？今回の学習データは、J語の情報を圧倒的に充実させ、さらにDeepseekの出力を大量にコピペしたのに・・・。 “

QEU:FOUNDER ： “「SFT（教師あり学習）によるFinetuningって、その程度のものである」ということ・・・。SFTって、モデルへの効用はせいぜい2つ（↓）しかないと思うよ。”

- **「出力の調子（文体、口調、書式を含む）」を変更する**
- **ハルシネーションを減らす**

D先生： “逆にいうと、**SFTでは出力する文章の情報量を充実させることができない**。もしも強化学習でファインチューニングしたら、もっと質が良くなるんですか？ “

![imageBSR1-6-4](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-4.jpg)

QEU:FOUNDER ： “まあ、ここは「論より証拠」で、強化学習によるファインチューニングを試してみるしかなんじゃないですかね。でも、ここでは直感的な説明はできます。SFTによる強化学習では、**学習データの「最大公約数みたいな出力しか出てこない」**と思うんです。だから、いくら優秀なデータを学習に一部だけ紛れ込ませてもだめだと思います。**「外挿が可能な強化学習によるファインチューニング」**でなければ・・・。”

![imageBSR1-6-5](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-5.jpg)

D先生： “確かに・・・。私は強化学習のプロジェクトを長くやっているのでわかります。強化学習は、与えられたゲーム環境のもとで、未来に得られるスコア最大化していきます。をまあ、ここらへんはFOUNDERの仮説ですからね。実際にやってみないと・・・。それでは、例題としてのDPOのプログラムをドン！！ただし、モデルは32bではなく、14bを使います。今回は、あくまで「やってみた」ですからね・・・。 “

強化学習におけるDPO（Discounted Policy Optimization）とORPO（Off-Policy Reinforcement Learning with Importance Sampling）の主な違いは、どのようにデータを使用するかにあります。

- **DPO（Discounted Policy Optimization）**: このアプローチは、現在のポリシーに基づいて行動を評価し、将来の報酬を割引価格で評価します。これにより、長期的な報酬を重視するポリシーが学習されます。

- **ORPO（Off-Policy Reinforcement Learning with Importance Sampling）**: このアプローチは、現在のポリシーと異なるポリシー（オフポリシー）に基づいて行動を評価します。重要サンプリングを使用して、オフポリシーのデータをオンポリシーの学習に適用します。

要するに、DPOは現在のポリシーに基づいて学習を行い、ORPOは異なるポリシーに基づいて学習を行う点が異なります。どちらも強化学習において重要な役割を果たしますが、使用するデータの源とその適用方法が異なります。

QEU:FOUNDER ： “D先生・・・。今回使うのは、**ORPO**ですよ。念のため・・・。”

```python
# ---
from unsloth import FastLanguageModel
import torch

max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# ---
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen2.5-14B-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

# ---
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

# ---
# Data Prep
# Instruction: "What is 2+2?"
# Accepted: "The answer is 4"
# Rejected: "The answer is 5"
# ORPO の目標は、「拒否（Accepted）」サンプルにペナルティを課し、「承認（Rejected）」サンプルの可能性を高めることです。recipe-research は基本的に、Mistral を使用して「拒否」応答を生成し、GPT-4 を使用して「承認」応答を生成しました。

# ---
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN

def format_prompt(sample):
    instruction = sample["instruction"]  # Extract the instruction from the sample
    input = sample["input"]              # Extract the input from the sample
    accepted = sample["accepted"]        # Extract the accepted response from the sample
    rejected = sample["rejected"]        # Extract the rejected response from the sample

    # ORPOTrainer expects prompt/chosen/rejected keys
    sample["prompt"] = alpaca_prompt.format(instruction, input, "")  # Format the prompt using the template
    sample["chosen"] = accepted + EOS_TOKEN  # Append EOS token to the accepted response
    sample["rejected"] = rejected + EOS_TOKEN  # Append EOS token to the rejected response

    return sample  # Return the modified sample

pass

from datasets import load_dataset

dataset = load_dataset("reciperesearch/dolphin-sft-v0.1-preference")["train"]

# ---
#この関数は、dataset.map を使用してデータセット内の各サンプルに適用されます。dataset.map は、format_prompt 関数を通じて各サンプルを処理し、フォーマットされたデータセットを返します。
dataset = dataset.map(format_prompt,)

```

D先生： “一体、今回のデータセットは、どんな感じなのですか？”

![imageBSR1-6-6](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-6.jpg)


D先生： “えっ？こんな感じなの！？Siamese Learningでやってきた、Tripletみたいですね。”

QEU:FOUNDER ： “さっきのORPOの定義通りだネ。このデータ群において、**REJECTとアゲ、ACCEPTをサゲるようにモデルを調整します**。ある意味、着地点が分かりやすい方法です。”

D先生： “これを使うと、REASONINGの情報量がいやでも増えていくでしょう。それにしても・・・。この手法は、**REASONINGが入らない**でしょう？”

![imageBSR1-6-7](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-7.jpg)

QEU:FOUNDER ： “そうなんですよ・・・。思い出して？UnslothのDeepseek蒸留版モデルを・・・。**REASONINGとREAPONSEの区切りがなかった**でしょ？現状は作れないんですよ。ひょっとして、最先端は変わっているだろうけど・・・。それでは、つづきます。”

```python
# ---
# Enable reward modelling stats
from unsloth import PatchDPOTrainer
PatchDPOTrainer()
# ---
from trl import ORPOConfig, ORPOTrainer

orpo_trainer = ORPOTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = ORPOConfig(
        max_length = max_seq_length,
        max_prompt_length = max_seq_length//2,
        max_completion_length = max_seq_length//2,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        beta = 0.1,
        logging_steps = 1,
        optim = "adamw_8bit",
        lr_scheduler_type = "linear",
        max_steps = 30, # Change to num_train_epochs = 1 for full training runs
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        output_dir = "outputs",
    ),
)

# ---
trainer_stats = orpo_trainer.train()

```

D先生： “学習の様子も変わりました。すごい情報群だ・・・。”

![imageBSR1-6-8](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-8.jpg)

QEU:FOUNDER ： “お遊びだけど、W&B(Weight and biases)のサービスで可視化しましょう。”

![imageBSR1-6-9](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-9.jpg)


D先生： “**「REWARD」**って項目があるんですね。そのREWARDがCHOSENとREJECTの２つがあると・・・。・・・ということは、これから長いSTEPをかけて、CHOSENのスコアが上がり、REJECTのスコアが下がってくるんでしょう。今の学習では、まだ足らないようです。”

QEU:FOUNDER ： “今回は、ここまででいいでしょう。あくまで、「やってみた」なんだし・・・。結果は以下の通りです。”

Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
我住在馬尼拉。我將在四月初去日本看櫻花。為您推薦充分利用新幹線的5天4夜旅行計畫。列出您將要參觀的地方和每天要做的活動。

### Input:

### Response:
以下是一個5天4夜的日本賞櫻之旅建議行程，主要集中在東京和周邊地區，您可以利用新幹線快速移動：

**第一天：**

- 早上抵達東京羽田機場，乘坐新幹線前往東京站。
- 下午前往淺草寺，感受傳統日式風情。
- 晚上漫步於晴空塔附近，享受東京的夜景。

**第二天：**

- 上午前往上野公園，欣賞櫻花並參觀上野動物園。
- 下午乘坐新幹線前往京都，探索金閣寺和清水寺。
- 晚上在京都享受日式料理。

**第三天：**

- 上午遊覽伏見稻荷大社和嵐山竹林。
- 下午乘坐新幹線返回東京，參觀明治神宮。
- 晚上在涉谷逛街購物。

**第四天：**

- 上午前往箱根，搭乘登山電車欣賞壯麗景色。
- 下午返回東京，參觀皇居和東京鐵塔。
- 晚上在六本木享用美食。

**第五天：**

- 上午前往台場，參觀自由之堡和夢之島公園。
- 下午前往迪士尼樂園或迪士尼海洋樂園。
- 晚上乘坐新幹線前往成田機場，準備返程。

這個行程涵蓋了東京、京都和箱根的主要景點，讓您有機會欣賞櫻花美景，體驗日本的文化和自然風光。請根據您的個人偏好和時間安排進行調整。祝您旅途愉快！


D先生： “まあ、当たり前ですが、すばらしい出来栄えです。ここら辺の評価は、自分で設計したデータを学習しないとわからないんですよね。それにしても・・・。あのデータセットって、**とても作りづらい**ですよね・・・（笑）。”

QEU:FOUNDER ： “このつづきは、「まとめ」でやりましょう。”



## ～ まとめ ～

### ・・・ いままでの「中間まとめ」です ・・・

D先生：“最新のAIとは言わない・・・。**かなり新しいAI（笑）モデルを使った各種のトライ**が完了しました。ちょっと、ここでまとめておきたい。”

[![MOVIE1](http://img.youtube.com/vi/M1OvmjV0nc8/0.jpg)](http://www.youtube.com/watch?v=M1OvmjV0nc8 "米テックを抜く？AI専門家「超すごい」開発コストの安さとド根性に驚愕…スターゲート計画は？")

QEU:FOUNDER ： “まず、この動画を見てください。まずは、ここからだ・・・。”

D先生：“あの・・・。ORPOのデータセットの作り方から話したいのですが・・・。”

QEU:FOUNDER ： “人海戦術を認めるんでしたら、同じ質問に対して、現状のモデルの出力とdeepseekの出力を収集すればいいんです。そして、現状モデルの出力を「reject」、そしてdeepseekの出力を「chosen」と定義すればいいです。”

C部長： “えぇ～っ、面倒くさい・・・。もっと、楽にやれませんか？”

D先生： “モデル内の**「平準化をめざす！」**という戦略というのはどうです？”

QEU:FOUNDER ： “今のQwenモデルを見ると、J語とE語の出力の質には大きな差があります。だったら、**同じ質問とJ語とE語でやってみて、J語の結果を「reject」とし、J語に翻訳したE語での出力を「chosen」にすればいいです**。この方法は、ほぼ完ぺきに自動化できますよ。”

D先生： “ただし、Pre-train段階でモデルが十分な「J語の力量があれば」という前提の話ですが・・・。”

![imageBSR1-6-10](/2025-02-08-QEUR23_BS4RVW5/imageBSR1-6-10.jpg)

QEU:FOUNDER ： “Qwen-32bであれば問題はないでしょう・・・。C部長も自分でLLMモデルを作ってみない？できれば、REASONINGを使って・・・。”

C部長 : “強化学習を使うのであれば、パワフルなREASONINGモデルが使えるのかなあ・・・。でも、さっきのデータセット生成で問題があるじゃないですか・”

QEU:FOUNDER ： “それなんだよね。今の状況では、**モデルの学習は2-STEPにするしかない**です。第一段階は強化学習(ORPO)で、第二段階は教師あり学習（SFT）になります。”

C部長： “わあ、面倒くさい。”

QEU:FOUNDER ： “REASONING付きのALPACAフォーマットを使うためには、この2ステップはしようがないです。ただし、第二段階はLLMモデルにフォーマットを教えるのだけが目的です。”

C部長： “これから、FOUNDERも、この続きをやらないんですか？いよいよ、BONSAIプロジェクトが再スタートでしょ？”

QEU:FOUNDER ： “外観検査自動機で、少しやりたいことがあるんです。小生は、外観検査自動機で新しいアイデアがあれば、それを常に優先します。なにしろ、外観検査自動機は圧倒的に実用的ですからね・・・。それをやってみて、もしもダメだったら、すぐに戻ってきますよ。”
