---
title: QEUR23_BS4RVW11 - 閑話休題～Solve it with code準備編(Qwen-max)
date: 2025-03-10
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "Qwen-max"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_BS4RVW11 - 閑話休題～Solve it with code準備編(Qwen-max)

## ～ Qwen-maxは、大変な万能ツールだ・・・ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “いずれにしろ、現時点で「Solve it with codeという大テーマ」に飛び込むのは、少しだけ早いのかもしれませんね。”

D先生： “じゃあ、(このプロジェクトを)やめときます？ “

QEU:FOUNDER ： “いや・・・。ついでに、もう一つ違う話題をやってみましょう。・・・というか、本件の続きになりますかね。前回作った2048ゲームを環境として強化学習に使いましょう。”

![imageBSR1-11-1](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-1.jpg) 

D先生： “なつかしいなあ・・・。昔、RT法を使って強化学習の問題を解きましたよね。そのコードの開発のために、我々は大変な時間を消耗しました。その開発時間が、Qwen-maxモデルを使えば本当に短縮されるんですかねえ？”

![imageBSR1-11-2](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-2.jpg) 

QEU:FOUNDER ： “前回、Qwen-maxを使ってみて、このコードの質に手ごたえがあったでしょ？今回は、プロンプトに、その「2048ゲームの全コード」を入力してみましょう。”

![imageBSR1-11-3](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-3.jpg) 

QEU:FOUNDER ： “それでは、プロンプトをドン！！Qwen-maxは、どのように反応するかな？”

```python
###########################
# Qwen-maxプロンプト
###########################
以下に示す2048gameを環境として、DEEP強化学習ができるpythonプログラムを生成してください。
学習ステップ数は１００回とし、各ステップ処理後にゲームのスコアとディープラーニングのモデルの学習損失を表示してください。ディープラーニングはPytorchを使ってください。
そのゲームスコアと学習損失は、学習完了後にmatplotlibで推移グラフを書いてください。
pygameによるゲームの自動進行は不要です。ゲーム盤の状態のテキスト表示は、各ステップが完了した後で表示し、出力は５０回ごとに画面をクリア（clear_output）してください。
'''
import pygame
import random

pygame.init()

WINDOW_SIZE = 400
GRID_SIZE = 4
TILE_SIZE = WINDOW_SIZE // GRID_SIZE
MARGIN = 5

window = pygame.display.set_mode((WINDOW_SIZE, WINDOW_SIZE))
pygame.display.set_caption("2048")

WHITE = (255, 255, 255)
GRAY = (200, 200, 200)
BLACK = (0, 0, 0)
COLORS = {
    0: (204, 192, 180),
    2: (238, 228, 218),
    4: (237, 224, 200),
    8: (242, 177, 121),
    16: (245, 149, 99),
    32: (246, 124, 95),
    64: (246, 94, 59),
    128: (237, 207, 114),
    256: (237, 204, 97),
    512: (237, 200, 80),
    1024: (237, 197, 63),
    2048: (237, 194, 46),
}

font = pygame.font.SysFont("Arial", 30)

def initialize_grid():
    grid = [[0] * GRID_SIZE for _ in range(GRID_SIZE)]
    add_new_tile(grid)
    add_new_tile(grid)
    return grid

def add_new_tile(grid):
    empty_cells = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE) if grid[i][j] == 0]
    if empty_cells:
        i, j = random.choice(empty_cells)
        grid[i][j] = 2 if random.random() < 0.9 else 4

def compress_and_merge(row):
    compressed = [num for num in row if num != 0]
    compressed += [0] * (len(row) - len(compressed))
    merged = []
    has_merged = False
    i = 0
    while i < len(compressed):
        if i < len(compressed) - 1 and compressed[i] == compressed[i+1]:
            merged.append(compressed[i] * 2)
            i += 2
            has_merged = True
        else:
            merged.append(compressed[i])
            i += 1
    new_compressed = [num for num in merged if num != 0]
    new_row = new_compressed + [0] * (len(row) - len(new_compressed))
    has_moved = (new_row != list(row)) or has_merged
    return new_row, has_moved

def move_left(grid):
    new_grid = []
    moved = False
    for row in grid:
        new_row, has_moved = compress_and_merge(row)
        new_grid.append(new_row)
        if has_moved:
            moved = True
    return new_grid, moved

def move_right(grid):
    new_grid = []
    moved = False
    for row in grid:
        reversed_row = row[::-1]
        new_reversed_row, has_moved = compress_and_merge(reversed_row)
        new_row = new_reversed_row[::-1]
        new_grid.append(new_row)
        if has_moved:
            moved = True
    return new_grid, moved

def move_up(grid):
    transposed = list(zip(*grid))
    new_transposed = []
    moved = False
    for row in transposed:
        new_row, has_moved = compress_and_merge(row)
        new_transposed.append(new_row)
        if has_moved:
            moved = True
    new_grid = list(zip(*new_transposed))
    return [list(row) for row in new_grid], moved

def move_down(grid):
    transposed = list(zip(*grid))
    new_transposed = []
    moved = False
    for row in transposed:
        reversed_row = row[::-1]
        new_reversed_row, has_moved = compress_and_merge(reversed_row)
        new_row = new_reversed_row[::-1]
        new_transposed.append(new_row)
        if has_moved:
            moved = True
    new_grid = list(zip(*new_transposed))
    return [list(row) for row in new_grid], moved

def is_game_over(grid):
    for row in grid:
        if 0 in row:
            return False
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE - 1):
            if grid[i][j] == grid[i][j+1]:
                return False
    for j in range(GRID_SIZE):
        for i in range(GRID_SIZE - 1):
            if grid[i][j] == grid[i+1][j]:
                return False
    return True

def draw_grid(window, grid):
    window.fill(WHITE)
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE):
            x = j * TILE_SIZE + MARGIN
            y = i * TILE_SIZE + MARGIN
            rect = pygame.Rect(x, y, TILE_SIZE - 2*MARGIN, TILE_SIZE - 2*MARGIN)
            value = grid[i][j]
            color = COLORS.get(value, (204, 192, 180))
            pygame.draw.rect(window, color, rect)
            if value != 0:
                text = font.render(str(value), True, BLACK)
                text_rect = text.get_rect(center=rect.center)
                window.blit(text, text_rect)
    pygame.display.update()

def main():
    grid = initialize_grid()
    running = True
    game_over = False
    while running:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_q:
                    running = False
                if not game_over:
                    if event.key in (pygame.K_LEFT, pygame.K_RIGHT, pygame.K_UP, pygame.K_DOWN):
                        old_grid = [row.copy() for row in grid]
                        if event.key == pygame.K_LEFT:
                            new_grid, moved = move_left(grid)
                        elif event.key == pygame.K_RIGHT:
                            new_grid, moved = move_right(grid)
                        elif event.key == pygame.K_UP:
                            new_grid, moved = move_up(grid)
                        elif event.key == pygame.K_DOWN:
                            new_grid, moved = move_down(grid)
                        if moved:
                            grid[:] = new_grid
                            add_new_tile(grid)
                        game_over = is_game_over(grid)
        draw_grid(window, grid)
        if game_over:
            text = font.render("Game Over! Press 'q' to quit", True, BLACK)
            text_rect = text.get_rect(center=(WINDOW_SIZE//2, WINDOW_SIZE//2))
            window.blit(text, text_rect)
            pygame.display.update()
    pygame.quit()

if __name__ == "__main__":
    main()
'''

```

D先生：“ゲームのプログラムは、「全部コピペ」にしたのか・・・。その一方で追加の要求事項は、いろいろな工夫をしていますね。”

QEU:FOUNDER ： “我々はいままでの「（苦い）経験」があるので、それらの経験を使って追加要求事項を定義したわけです。それでは、モデルの出力のうち、推論(REASON)の部分を見て行きましょう。”

**(Qwen-max: Reasoning)**

好的，我现在需要帮用户生成一个使用PyTorch进行深度强化学习的2048游戏程序。用户的要求包括：学習ステップ数100回，每步处理后显示游戏得分和模型损失，训练完成后用matplotlib画出得分和损失的变化图。另外，不需要pygame自动进行游戏，但每步结束后要显示游戏盘面，并且每100步清屏。

首先，我需要理解用户提供的2048游戏代码结构。现有的代码是使用pygame实现的，用户希望将其作为环境，用强化学习来训练模型。但用户指出不需要pygame的自动进行，所以可能需要将游戏逻辑与显示部分分离，让强化学习代理在后台运行，只在必要时输出状态。

接下来，我需要确定如何将2048游戏环境适配到强化学习框架中。通常，强化学习需要环境的状态、动作、奖励、是否结束等信息。这里的状态是4x4的网格，动作是上下左右四个方向。奖励应该是每次移动后的得分变化，或者游戏结束时的总分。需要修改游戏代码，使其能够返回当前状态、执行动作后的奖励，以及是否结束。

然后，考虑深度学习模型的结构。使用PyTorch的话，可能需要一个神经网络来预测每个动作的Q值。输入是4x4的网格，输出是四个动作的Q值。需要将网格数据转换为张量，并进行归一化处理，比如除以最大值（例如2048或更大的数）以避免数值过大。

接下来是强化学习算法的选择。常用的有DQN（深度Q网络），可能还需要经验回放和目标网络。但用户可能希望简化，所以可能采用基本的DQN，或者更简单的Q-learning，但结合神经网络。由于训练步数只有100步，可能效果有限，但用户可能希望看到框架的实现。

然后，处理训练循环。每个步骤中，代理选择动作（比如使用ε-greedy策略），执行动作，获得奖励，存储经验，然后更新模型。每一步后需要记录得分和损失，并在最后绘制图表。

关于显示部分，用户要求每步结束后显示游戏盘面，但不需要pygame自动进行。所以可能需要将游戏的文本表示输出到控制台，而不是使用pygame的图形界面。因此，需要修改draw_grid函数，或者创建一个文本显示的函数，替代原来的pygame绘图。

另外，用户提到每100步清屏，可能是指在训练过程中每100步清除之前的输出，避免控制台信息过多。可以使用IPython的clear_output，但需要注意是否在notebook环境中使用。或者使用print('\033c')来清屏，但可能平台相关。

现在，具体步骤：

将游戏逻辑从pygame中分离出来，创建一个GameEnv类，提供reset和step方法，返回状态、奖励、done等信息。这需要修改现有的initialize_grid、move等函数，使其不依赖pygame，而是返回状态。
实现文本显示游戏盘面的方法，替代pygame的draw_grid，比如用print输出4x4的矩阵。
构建PyTorch模型，输入4x4的网格，输出四个动作的Q值。可能需要将网格展平成16个元素的向量，或者使用卷积层。
实现DQN算法，包括经验回放、目标网络、ε-greedy策略。但考虑到训练步数较少，可能需要简化，比如不使用经验回放，直接在线学习，但效果可能不佳。
训练循环：进行100个episode，每个episode中执行动作直到游戏结束，记录每个步骤的得分和损失，最后绘制图表。
可能的挑战：

状态表示：如何将4x4的网格转换为神经网络的输入。可能需要归一化，例如每个单元格的值取log2，然后除以最大可能值（如11，因为2^11=2048），或者直接除以2048。
奖励设计：每次移动的得分变化作为即时奖励，或者游戏结束时的总分作为最终奖励。可能需要结合两者，但需要测试。
训练稳定性：由于2048的稀疏奖励，可能需要更复杂的奖励结构或算法，但用户可能希望保持简单。
现在，具体代码实现的大致结构：

定义GameEnv类，包含reset、step、check_game_over等方法。
定义DQN模型，继承自nn.Module。
定义经验回放缓冲区（如果使用DQN的话）。
训练循环，使用ε-greedy选择动作，执行step，计算损失并更新模型。
记录每个episode的得分和损失，最后绘图。
需要注意用户要求的是学習ステップ数100回，可能是指训练100个episode，每个episode是一个完整的游戏，直到结束。或者每个步骤是一个移动。需要确认用户的意思。根据用户的描述，“学習ステップ数は１００回”，可能是指训练100个episode，每个episode可能包含多个步骤。

另外，用户要求各ステップ処理后显示游戏得分和损失。这里的ステップ可能指每个移动步骤，还是每个训练步骤？可能需要进一步明确。但根据用户提供的代码示例中的main函数，每个步骤是用户按键移动，所以可能用户希望每个移动步骤后显示信息。但强化学习中，每个训练步骤可能对应一个移动步骤。因此，可能需要在每个移动步骤后记录得分和损失，并在训练结束后绘制。

QEU:FOUNDER  ： “こういうコメントが延々続きます。”

D先生： “あの・・・。こんなREASONINGって、無駄じゃないですか？ “

QEU:FOUNDER  ： “まあ、君のような「ど素人」はそう思うかもね・・・。しかし、**このREASONINGはとても役に立つ**んです。この推論の結果を適切に分析すると「システムの要件定義書」となるんです。小生は、実はこのREASONINGを読んで、FEEDBACKしているんです。Qwen-maxが思い違いをしていることを修正しながら・・・。”

D先生： “まあ・・・。素人ユーザーとしては、漠然と「2048ゲームの強化学習を作ってください」としか書かないわけですからね。 その出力を「たたき台」にしてFEEDBACKするのは良いアイデアです。“

QEU:FOUNDER  ： “そうそう・・・。もし、初めのトライでQwen-maxがKerasによる強化学習プログラムのコードを吐き出してきたならば、次に、「すんません。Pytorchでやってください。」と指示すればいいんですよ。じゃあ、次に成果物（コード）の部分を見てみましょう。”

```python
###########################
# Qwen-max: Response（コード）
###########################
# ---
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
#import matplotlib.pyplot as plt
from IPython.display import clear_output

# 2048游戏环境封装
class Game2048Env:
    def __init__(self):
        self.grid = None
        self.score = 0
        self.reset()

    def reset(self):
        self.grid = [[0] * 4 for _ in range(4)]
        self.add_new_tile()
        self.add_new_tile()
        self.score = 0
        return self.get_state()

    def add_new_tile(self):
        empty_cells = [(i, j) for i in range(4) for j in range(4) if self.grid[i][j] == 0]
        if empty_cells:
            i, j = random.choice(empty_cells)
            self.grid[i][j] = 2 if random.random() < 0.9 else 4

    def step(self, action):
        old_score = self.score
        moved = False
        
        if action == 0:  # Left
            new_grid, moved = self.move_left(self.grid)
        elif action == 1:  # Right
            new_grid, moved = self.move_right(self.grid)
        elif action == 2:  # Up
            new_grid, moved = self.move_up(self.grid)
        elif action == 3:  # Down
            new_grid, moved = self.move_down(self.grid)
        else:
            raise ValueError("Invalid action")

        if moved:
            self.grid = new_grid
            self.add_new_tile()

        reward = self.score - old_score
        done = self.is_game_over()
        return self.get_state(), reward, done, {}

    def move_left(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row)
            new_grid.append(new_row)
            if has_moved:
                moved = True
        return new_grid, moved

    def compress_and_merge(self, row):
        compressed = [num for num in row if num != 0]
        compressed += [0] * (4 - len(compressed))
        merged = []
        has_merged = False
        i = 0
        while i < len(compressed):
            if i < len(compressed)-1 and compressed[i] == compressed[i+1]:
                merged_val = compressed[i] * 2
                merged.append(merged_val)
                self.score += merged_val
                i += 2
                has_merged = True
            else:
                merged.append(compressed[i])
                i += 1
        new_compressed = merged + [0] * (4 - len(merged))
        has_moved = (new_compressed != row) or has_merged
        return new_compressed, has_moved

    def move_right(self, grid):
        new_grid = []
        moved = False
        for row in grid:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_grid.append(new_row[::-1])
            if has_moved:
                moved = True
        return new_grid, moved

    def move_up(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row)
            new_transposed.append(new_row)
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def move_down(self, grid):
        transposed = list(zip(*grid))
        new_transposed = []
        moved = False
        for row in transposed:
            new_row, has_moved = self.compress_and_merge(row[::-1])
            new_transposed.append(new_row[::-1])
            if has_moved:
                moved = True
        return [list(row) for row in zip(*new_transposed)], moved

    def is_game_over(self):
        for row in self.grid:
            if 0 in row:
                return False
        for i in range(4):
            for j in range(3):
                if self.grid[i][j] == self.grid[i][j+1]:
                    return False
        for j in range(4):
            for i in range(3):
                if self.grid[i][j] == self.grid[i+1][j]:
                    return False
        return True

    def get_state(self):
        state = []
        for row in self.grid:
            for num in row:
                state.append(0 if num == 0 else (num / 2048.0))
        return torch.FloatTensor(state).unsqueeze(0)

    def render(self):
        print(f"Score: {self.score}")
        for row in self.grid:
            print([num for num in row])
        print()

# DQN模型定义
class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(16, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 4)
        )

    def forward(self, x):
        return self.fc(x)

# 经验回放缓冲区
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return torch.cat(states), torch.tensor(actions), torch.tensor(rewards, dtype=torch.float32), torch.cat(next_states), torch.tensor(dones)

    def __len__(self):
        return len(self.buffer)

# 训练参数
num_episodes = 5000
batch_size = 16
gamma = 0.95
epsilon_start = 0.95
epsilon_end = 0.01
epsilon_decay = 0.995
target_update = 100
learning_rate = 0.001

# 初始化组件
env = Game2048Env()
policy_net = DQN()
target_net = DQN()
target_net.load_state_dict(policy_net.state_dict())
optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)
memory = ReplayBuffer(50)
criterion = nn.MSELoss()

epsilon = epsilon_start
scores = []
losses = []

# 训练循环
for episode in range(num_episodes):
    state = env.reset()
    done = False
    total_reward = 0
    episode_losses = []
    
    while not done:
        # ε-greedy动作选择
        if random.random() > epsilon:
            with torch.no_grad():
                q_values = policy_net(state)
                action = q_values.argmax().item()
        else:
            action = random.randint(0, 3)
        
        # 执行动作
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
        
        # 存储经验
        memory.push(state, action, reward, next_state, done)
        state = next_state
        
        # 经验回放
        if len(memory) >= batch_size:
            states, actions, rewards, next_states, dones = memory.sample(batch_size)
            
            current_q = policy_net(states).gather(1, actions.unsqueeze(1))
            next_q = target_net(next_states).max(1)[0].detach()
            target_q = rewards + gamma * next_q * (1 - dones.float())
            
            loss = criterion(current_q.squeeze(), target_q)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            episode_losses.append(loss.item())
        
    # 更新目标网络
    if episode % target_update == 0:
        target_net.load_state_dict(policy_net.state_dict())
        
    # 显示信息
    if episode % 100 == 0:
        clear_output(wait=True)
        env.render()
    current_loss = loss.item() if episode_losses else 0
    print(f"Episode: {episode+1}/{num_episodes}, Score: {total_reward}, Loss: {current_loss:.4f}")
    
    # 衰减探索率
    epsilon = max(epsilon_end, epsilon * epsilon_decay)
    
    # 记录统计信息
    scores.append(total_reward)
    avg_loss = sum(episode_losses)/len(episode_losses) if episode_losses else 0
    losses.append(avg_loss)

# ---
# スコアの出力（リスト）
print(scores)
# 損失の出力（リスト）
print(losses)

```

QEU:FOUNDER ： “ここまでは、プログラムの**「前半」**です。つまり、強化学習そのものが完了するまでですね。ここで、スクリーンへの出力結果を見てみましょう。”

![imageBSR1-11-4](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-4.jpg) 

D先生： “おっ！うまくできています。でも、FOUNDER・・・。かなり、いじった（修正した）でしょう？“

```python
# ---
import matplotlib.pyplot as plt

# 绘制结果
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(scores)
plt.title('Score Progress')
plt.xlabel('Episode')
plt.ylabel('Score')

plt.subplot(1, 2, 2)
plt.plot(losses)
plt.title('Training Loss')
plt.xlabel('Episode')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

```

QEU:FOUNDER ： “もちろん、Qwen-maxから出力されたコードは、ちょっといじりました。ただし、修正そのものは、とても少ないです。主な修正は、ハイパーパラメタとインデントの崩れぐらいです。全体的に見て、驚くほど精度がいいですよ。あと、このように前半と後半に分けた理由は、コードスニペットの長さが過度に冗長になるのを避けるためです。次は、プログラム後半の結果を見てみましょう。”

![imageBSR1-11-5](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-5.jpg) 

D先生： “今回RLシステムは、うまく動いています。とくにepoch初期に出てくる「学習損失の丘」には味わいがありますね。それにしても、左側のグラフは見にくいです。移動平均できませんか？”

QEU:FOUNDER： “もし、そうしたいんだったら、後半のコードをプロンプトに貼り付けて移動平均を追加するように、Qwen-maxモデルにお願いしてください。。”

D先生： “なるほど。そうすると、コートを生成するときには、できるだけコードを適切にモジュール分割したほうが便利ですね。もう8年以上も前になるが・・・。昔、我々はこのプログラムを作るために数日を掛けたわけです。今回のFOUNDERの所要時間は？“

QEU:FOUNDER ： “小生のトータル作業時間は20分は要らないですね。”

D先生： “ガーン・・・。”

QEU:FOUNDER ： “いままでで得られたノウハウをまとめてみましょう。UnslothのGRPOモデルでコードを生成することは失敗しました。現在のところ、Deepseekで有名になったGRPOによるREASONING生成はコード生成のタスクには向いていないようです。”

D先生： “QwQモデルでは、コードの生成は可能であるだけでなく、**このモデルの活用による「Solve it with code」は素晴らしく有用です。**ただし、QwQのReasoningには、まだ課題がないわけではない。単に、GPUのパワーでコードを吐き出しているにすぎません。“

QEU:FOUNDER ： “もしも、QwQモデルのREASONINGで「システム要件定義書」が出てくるのであれば、コーディングの世界に革命が起きるでしょう。ユーザーがREASONINGを読みながら、気に入らない部分を変更していけば、ユーザーが望むコードが、極めて短期間で得られます。”

![imageBSR1-11-6](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-6.jpg) 

D先生： “もう一つ付け加えると、出力されるコードは**「モジュール単位」であるべき**。ここまでくると、少なくとも2年後には**「（創造性を持たない）プログラマは要らなくなる」**なあ・・・。“

![imageBSR1-11-7](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-7.jpg) 

QEU:FOUNDER ： “同意、少なくとも2年以内・・・。それでもね、ここまででも**「EVERYBODY IS CREATIVE CLASS」の実現に十分に近づいた**と言えます。ただし、これには注釈が付きます。誰もが、CREATIVE CLASSになれるわけではないです。CREATIVITYを持っている誰もが、CREATIVE CLASSになるわけです。”

D先生：“FOUNDER・・・。本件をもう一歩進めませんか？我々が昔やってきたRT法（厳密に言うと新RT法）での強化学習をやっていませんか？”

QEU:FOUNDER ： “AIモデルが持っていないノウハウ（RT法）を注入して、ちゃんとしたプログラムができるのか？まあ・・・、ちょっとやってみて、うまくいけば、このまま続けましょう。そうでなければ、ここで終わりです”



## ～ まとめ ～

### ・・・ 前回の続きです ・・・

D先生：“この調子で、**「（A国が）J国との地位協定を見直す」ところまで行って欲しい**ですね。”

![imageBSR1-11-8](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-8.jpg) 

QEU:FOUNDER ： “なんか、そういう話も少しづつ流れていますよね。本当のところは、知らんけど・・・。最近、Xあたりでは「受け取る情報の分散」が大きくなってきました。”

![imageBSR1-11-9](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-9.jpg) 

D先生：“中間のメディアがいなくなり、情報をスクリーニングする人がいなくなったからでしょう。何はともあれ、我々としては、T大統領を絶対支持でしょう？”

![imageBSR1-11-10](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-10.jpg) 

QEU:FOUNDER ： “もちろん！！”

![imageBSR1-11-11](/2025-03-12-QEUR23_BS4RVW11/imageBSR1-11-11.jpg) 

QEU:FOUNDER ： “彼は、**ディープステートと戦う「光の戦士」**なのだから・・・。”
