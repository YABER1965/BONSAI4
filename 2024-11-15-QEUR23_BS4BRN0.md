---
title: QEUR23_BS4BRN0: BONSAI4 – ADVANCED_DEBATE(その4)
date: 2024-09-02
tags: ["QEUシステム", "メトリックス", "Python言語", "LLM", "データセット", "Fine-tuning", "イノベーション","Embedding"]
excerpt: LLMのファインチューニングを最適化する
---

## QEUR23_BS4BRN0: BONSAI4 – ADVANCED_DEBATE(その4)

## ～ よりユニークになりたいの・・・ ～

D先生（設定年齢65歳） ： “久々にLLMのプロジェクトに戻ってきました。前回のBONSAI3では、TW-CN(-JP?)間の紛争勃発の可能性（有事）について論じていました。そして、いきなり外観検査自動機（VLM）に行っちゃいましたが、プロジェクト成功後にもどってきました。さて、FOUNDER・・・。前回のつづき（有事）のつづきをやりますか？”

![imageJRF4-1-1](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-1.jpg)

QEU:FOUNDER（設定年齢65歳） ： “もう、（有事を）やらないですよ。このところ、そのアジェンダの重要性が落ちてきました。少しだけだが、世の中の雰囲気が変わったきたし・・・。**意味のない「What if」って、やっていて疲れるんです**。”

![imageJRF4-1-2](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-2.jpg)

D先生（設定年齢65歳） ： “冬眠状態ですかね。C国もぜんぜんやる気はないでしょうし、しばらくは平和ですね。ただし、あと8か月ぐらいすれば、紛争推進のお化けが再復活もありえますね。そもそも、今回の技術テーマは何ですか？「ディーベートのお題」は、さておき・・・。”

![imageJRF4-1-3](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-3.jpg)

QEU:FOUNDER ： “ちょっと議題を設定する前に、この話（↑）をネタにして遊んでみたいです。”

![imageJRF4-1-4](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-4.jpg)

C部長： “D先生から聞きました。**「今回のBONSAI4ではEmbeddingにこだわりたい」**って・・・。”

D先生 ： “それにしても、参考テキストのようにCohereのEmbeddingの生成サービスを使うんですか？贅沢ですね。”

QEU:FOUNDER ： “まあ、今回は内容をかなり変えています。今回の我々のテーマに合うようにね・・・。それではプログラムをドン！”

```python
# ---
import numpy as np
import pandas as pd
import time
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer

# ---
# Download and load the embedding model
# 文の埋め込みを開始するには、SentenceTransformer ライブラリを使用して BAAI/bge-base-en モデルをダウンロードして読み込む必要があります。
# このモデルを初めて使用する場合は、次のコードを実行すると、さまざまなファイルのダウンロードがトリガーされます。
# ---
emb_model = SentenceTransformer('BAAI/bge-base-en')

# ---
# load ratings data(1 of 3)
ratings = pd.read_csv("drive/MyDrive/ratings.csv")
#ratings.head()

# ---
# load ratings data(2 of 3)
users = pd.read_csv("drive/MyDrive/users.csv")
#users.head()

# ---
# load ratings data(3 of 3)
movies_all = pd.read_csv("drive/MyDrive/movies.csv")
print(movies_all.shape)
print(movies_all.columns)
movies = movies_all.loc[:200,:]
movies.head()

```

C部長： “あれ？3つも読み込みをするデータフレームがあるんですね。複雑なデータベースの構造ですね。”

![imageJRF4-1-5](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-5.jpg)

QEU:FOUNDER ： “これは、昔、機械学習でも使った映画の口コミのレーティングを予測するためのデータセットです。このうち、Embeddingを使うのは、Movieファイルの２つの列（コラム）、「title」と「overview」に対してだけです。これをembeddingに変換して、多変量回帰分析をやってみたいということです。つづきにいきます。”

```python
# ---
def embed_docs_in_chunks(docs, chunk_size=256):
    embeddings = []
    for i in tqdm(range(0, len(docs)+1, chunk_size)):
        chunk = docs[i:min(i+chunk_size, len(docs))]
        embeddings.extend(emb_model.encode(chunk, normalize_embeddings=True))
    return embeddings
# ---
titles = movies["title"].tolist()
titles_embeddings = embed_docs_in_chunks(titles)

# ---
overviews = movies["overview"].tolist()
overviews_embeddings = embed_docs_in_chunks(overviews)
print(len(overviews_embeddings))
print(overviews_embeddings[0][0:50])

# ---
# save embeddings back into dataframe
movies["title_embedding"] = titles_embeddings
movies["overview_embedding"] = overviews_embeddings
# ---
# Preprocessing
# データに対して機械学習を実行するには、データを適切な入力形式に変換できるように、必要な前処理をいくつか実行する必要があります。
user_feature_columns = ["age", "occupation", "gender"]
movie_feature_columns = ["title_embedding", "overview_embedding"]
# we only keep the user and movie ids along with the features
users = users[["user_id"] + user_feature_columns]
movies = movies[["movie_id"] + movie_feature_columns]

# ---
# join ratings with users and movies
ratings = ratings.merge(users, on="user_id")
ratings = ratings.merge(movies, on="movie_id")
# コラム'title_embedding'に欠値があるレコードを削除
ratings_cleaned = ratings.dropna(subset=['title_embedding'])
print("DF結合・欠値を削除した後のデータフレーム:\n")
ratings_cleaned

```

QEU:FOUNDER ： “さきほどC部長が挙げた3つのデータフレームが、キーIDで結合されて一つのデータフレームになりました。複数の原因が組み合わされて、レーディング（星の数）が結果として現れる構造になっています。”

![imageJRF4-1-6](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-6.jpg)

C部長： “あれ？右側のRANDOMは、今までなかった・・・？”

QEU:FOUNDER ： “その値の意味は後で説明します。それでは、コードを進めましょう。”

```python
# ---
# データベースをもとに戻す(ratings)
ratings = ratings_cleaned.copy()
# ---
# split into features and target
feature_cols = user_feature_columns + movie_feature_columns
print(feature_cols)
target_col = "rating"
X = ratings[feature_cols]
y = ratings[target_col]
#len(X.title_embedding[0])

# ---
# 各次元がデータフレーム内の 1 つの列を構成するように、埋め込みをフラット化する必要があります。これは、推奨システムが機能するために必要です。
categorical_cols = ["gender", "occupation"]
embedding_cols = ["title_embedding", "overview_embedding"]
# ---
# one-hot encode categorical columns
X = pd.get_dummies(X, columns=categorical_cols)
# ---
for embedding_col in embedding_cols:
    # 各埋め込みは768個の浮動小数点数のリストなので、それらを平坦化する必要がある。
    # リスト内の各要素を個別の列として、「埋め込み」列のリストから DataFrame を作成します。
    embedding_df = pd.DataFrame(X[embedding_col].to_list())
    embedding_df.columns = [f'{embedding_col}_{i}' for i in embedding_df.columns]

    # Concatenate the new DataFrame with the original DataFrame
    X = pd.concat([X, embedding_df], axis=1)

    # Drop the original 'embedding' column
    X = X.drop(embedding_col, axis=1)
# ---
X.head()

# ---
# データベースの分離
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ---
# Training the model
# 次に、生成した埋め込みに基づいて映画を推奨するモデルをトレーニングする必要があります。
# ---
def train_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)

    # compute accuracy on train and test
    y_pred = model.predict(X_train)
    print(f"training dataに対しての精度: {model.score(X_train, y_train):.2}")

    y_pred = model.predict(X_test)
    print(f"test set scoreに対しての精度: {model.score(X_test, y_test):.2f}")

# L2正則化を用いた線形回帰モデルであるリッジ回帰モデルを試してみましょう
ridge_model = Ridge(alpha=5.0)
train_model(ridge_model, X_train, y_train, X_test, y_test)

```

D先生 ： “ちょっと高級に、T(2)法ではなく・・・。解析ツールに**Ridge回帰**を使用しましたね。”

![imageJRF4-1-7](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-7.jpg)

C部長： “悪いですが、予測精度がわるいなあ・・・。”

QEU:FOUNDER ： “それはそうでしょ？たかが、これは**「線形回帰」**なんだから・・・。映画の評論予測がリニアな構造になるわけがないです。線形予測でやれることは、影響力が高い項目を検出するぐらいです。”

```python
# ---
# first get the absolute value of the coefficients
coefficients = np.abs(ridge_model.coef_)
# get the feature names
feature_names = X.columns
# sort the coefficients in descending order
sorted_indices = np.argsort(coefficients)[::-1]
# print the top 10 features
for i in range(20):
    print(f"{feature_names[sorted_indices[i]]}: {coefficients[sorted_indices[i]]}")

```

D先生 ： “重要な要因がバラバラと出てきました。”

```python
# ---
# 回帰係数の分析結果
occupation_lawyer: 0.5418708615891569
occupation_tradesman/craftsman: 0.48331201858366213
overview_embedding_192: 0.42711222357928263
title_embedding_52: 0.4042839316110261
overview_embedding_727: 0.39196712932051775
title_embedding_735: 0.38356313424138166
overview_embedding_753: 0.381540914050532
title_embedding_162: 0.38019093765658213
title_embedding_538: 0.37404145780595766
overview_embedding_369: 0.3720279627275197
overview_embedding_447: 0.3644984191496451
title_embedding_77: 0.36027067109585975
overview_embedding_405: 0.3391560987900534
title_embedding_601: 0.3355712813077783
title_embedding_435: 0.3348001741628898
title_embedding_173: 0.33378261345805593
overview_embedding_323: 0.33349310090677975
title_embedding_293: 0.33246245218946696
title_embedding_545: 0.30785394335874394
overview_embedding_757: 0.3040394419624819

```

D先生 ： “なるほど、結論はこうなったのか・・・。せっかく、ここまで来ているのであれば、各項目の影響力を可視化したいですね。

```python
# ---
# 小さなデータフレームを生成する
df_out = pd.concat([X_test, y_test], axis=1)
# ---
arr_column = ['overview_embedding_192', 'title_embedding_52', 'overview_embedding_727', 'ti-tle_embedding_735', 'overview_embedding_753', 'title_embedding_162', 'rating']
df_out = df_out.loc[:, arr_column]

# ---
import random

arr_random = []
arr_y_test = y_test.values
for i in range(len(arr_y_test)):
    y_random = arr_y_test[i]+0.6*random.random()-0.3
    arr_random.append(y_random)
# ---
df_out['random'] = arr_random

# ---
# 散布図の描画
plt.figure(figsize=(12, 8))

# 各要因(X1〜X4)とYとの散布図を作成
for i in range(6):
    # ---
    arr_Xs = df_out[arr_column[i]].values
    srr_Ys = df_out['random'].values
    val_cor = np.corrcoef(arr_Xs, srr_Ys)
    # ---
    plt.subplot(2, 3, i+1)
    plt.scatter(df_out[arr_column[i]], df_out['random'], color='blue', label=f'X{i} vs Y')
    str_title = f'{arr_column[i]} by {round(val_cor[0,1],4)}'
    plt.title(str_title.replace("embedding_",""))
    plt.xlabel(f'X{i}')
    plt.ylabel('Y')
    plt.grid()
    plt.legend()

plt.tight_layout()
plt.show()
```

QEU:FOUNDER ： “それでは、複数の相関図を取ってみましょう。データの分布を見やすくするために、本来は整数のY値に微小なバラツキを加えました。それが「random」というコラムの意味です。いよいよ相関グラフをみてみましょう。”

![imageJRF4-1-8](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-8.jpg)

C部長： “良くて相関係数が0.1・・・。（相関値が）小さいですね。まあ、これは当たり前でしょうね。”

QEU:FOUNDER ： “なにはともあれ、結論からいうと、BAAIモデルを使って言語をEmbedding分解をして、それなりの意味を持つメトリックスを抽出できるのだから、大したものですね。便利な世の中だ。”

C部長： “この相関を改善できるのでしょうか？”

D先生 ： “さあ・・・。相関を強くするには、どうすればいいのか・・。う～ん・・・。”

C部長： “提案があります。CohereのEmbedding生成サービスを使いましょう。”

QEU:FOUNDER ： “それを敢て使わないのが、今回のポイントです。”


## ～ まとめ ～


QEU:FOUNDER ： “ホント・・・。おもしろい話題(↓)ですねえ。しばらくは、この話題だけで晩御飯が食べられます。”

[![MOVIE1](http://img.youtube.com/vi/QWIf4O-K_Mk/0.jpg)](http://www.youtube.com/watch?v=QWIf4O-K_Mk "町山智浩氏生出演！『USA大統領選総括と選挙後の米社会を深掘りする』")

D先生： “個人的には、**「生産性の話（↓）」**にとても興味があります。”

![imageJRF4-1-9](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-9.jpg)

QEU:FOUNDER ： “Department of ・・・。うまく**DOGEという名前になった**よね。ある種、これは**「天命」か**・・・？でも、いいのかなあ・・・？”

![imageJRF4-1-10](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-10.jpg)

D先生： “バンバン、イーロン様に政府をリストラしてもらって人員をカットしてもらいましょう！噂によると、あの会社(↑)では80％ぐらい減らしたんでしょう？”

QEU:FOUNDER ： “いいのかなあ・・・。それで**効率があがるとは思えない**のだが。”

![imageJRF4-1-11](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-11.jpg)

C部長 : “すいません。眼が悪いのかなあ・・・。グラフ（↑）ではJ国の生産性の順位が上がっています。**MJGA?**”

D先生： “ひどいなあ・・・。グラフの**「上下をさかさまにしている」**じゃないですか・・・。私も、おもわず「J国スゴイ！」と叫びそうになりました。**効率って「OUTPUTとINPUTの比」なんです**。この場合、「INPUTを深慮なく小さくして、本当にいいのか？」という疑問があります。”

![imageJRF4-1-12](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-12.jpg)

QEU:FOUNDER ： “この定義の分子である、**OUTPUTの本質が左側のワンちゃん（↑）**にあります。この犬は、21世紀の今日まで、もっとも成功したインターネット・ミームになりました。ただし、この犬は？”

![imageJRF4-1-13](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-13.jpg)

C部長 : “この犬はブリーダから廃棄されたのを保護されたのですよね。それが、ご主人のブログで紹介されているうちに、とんでもない成功を収めました。”

QEU:FOUNDER ： “INPUTをコントロールするのは、ちょっと計算すればできます。でも、OUTPUTの増加は、管理者やオーナーが計算してもできません。たとえ、そいつらが大天才でもね。ただ一ついえるのは、**INPUTを削るのはOUTPUTを大きく損なう可能性があります。**我々には、多くの痛い記憶があります。”

![imageJRF4-1-14](/2024-11-15-QEUR23_BS4BRN0/imageJRF4-1-14.jpg)

C部長 : “昔、新自由主義の人たちは、**「少数の優秀な人だけが会社にいれば、その会社は成功するんだ！！」**と述べていました。”

### 自分や自国が永遠に凄い、あるいは永遠に世界の上位だと思い込んでいる人は、早く気づくべきことがあります。

- 世界は常に変わり続けています。
- 永遠に強い国として存在し続ける国はありません。
- 上下の地位はいつでも逆転する可能性があります。
- あなたが凄いと思っていることも、やがて凄くなくなるでしょう。
- 例として挙げますと、かつて「軽蔑」されていた中国も今は変わりました。世界の警察と呼ばれたアメリカも、変わりつつあります。
- あなたが「軽蔑」している国や人々も、日々変化しています。
- その「軽蔑」している国や人々の下で、あなたが働く日が来る可能性も充分にあります。
- 歴史を振り返れば、**傲慢や過信はいつも破滅の前触れ**です。謙虚さを持ち、他国や他者を尊重することが、未来への備えとなるでしょう。

QEU:FOUNDER ： “そういう会社って、近年になってずいぶん黄昏ちゃったのです。**価値を生むって、ほとんどが運だから**・・・。価値を生むには、必要最低限の条件として**会社の中の人が「自由に、幸せに働く」しかない**のです。V国の人は、世の中の複雑さをよくわかっていますね。”

